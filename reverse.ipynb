{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import requests\n",
    "from functools import reduce\n",
    "from numpy.random import default_rng\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### activation function: sigmoid\n",
    "##### cost function: - correct * log (predicted)\n",
    "## for delta calculation check: https://en.wikipedia.org/wiki/Delta_rule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_HIDDEN_LAYERS = 8\n",
    "def nonlin(x, deriv=False):\n",
    "    if (deriv==True):\n",
    "        return (x*(1-x))\n",
    "\n",
    "    return 1/(1+np.exp(-x))\n",
    "\n",
    "with open('words.txt', 'r') as f:\n",
    "    lines = f.readlines()\n",
    "    \n",
    "f.close()\n",
    "\n",
    "lines = [line[:len(line)-1] for line in lines]\n",
    "revLines = [line[::-1] for line in lines]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# don't touch above this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len = 45 # pneumonoultramicroscopicsilicovolcanoconiosis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# make x and y columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def makeXY():\n",
    "\n",
    "    l = len(lines)\n",
    "    X = np.empty([l, max_len])\n",
    "    Y = np.empty([l, max_len])\n",
    "    for ind, word in enumerate(lines):\n",
    "        \n",
    "        inArr = np.random.randint(1, 58, (1,max_len)) # ord('z') - 64  = 58\n",
    "        inArrRev = np.random.randint(1, 58, (1,max_len)) \n",
    "        wordLen = len(word)\n",
    "        for letIndx, let in enumerate(word):\n",
    "            inArr[0, letIndx] = ord(let) - 64\n",
    "            inArrRev[0, wordLen-1-letIndx] = ord(let) - 64\n",
    "        # inArr[letIndx+1:] = \n",
    "        X[ind] = inArr\n",
    "        Y[ind] = inArrRev\n",
    "    return X, Y\n",
    "\n",
    "X, Y = makeXY()\n",
    "X_train = X[0:20000]\n",
    "Y_train = Y[0:20000]\n",
    "X_validation = X[20001:20020]\n",
    "Y_validation = Y[20001:20020]\n",
    "X_test = X[20020:]\n",
    "Y_test = X[20020:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([33., 35., 52., 41., 46., 47., 44., 41., 52., 37., 22., 37.,  4.,\n",
       "        45.,  4., 55., 25., 35., 13., 16., 15., 29.,  4.,  7., 18., 25.,\n",
       "        13., 14., 24., 25., 16., 43., 48.,  9., 32., 42., 49., 15., 53.,\n",
       "        55., 30., 23., 24., 38., 39.]),\n",
       " array([37., 52., 41., 44., 47., 46., 41., 52., 35., 33., 33., 56., 56.,\n",
       "        28., 47., 56., 21., 48., 25., 49., 16., 44., 48., 32., 19., 14.,\n",
       "        12., 15., 15., 14.,  7., 20., 52., 10., 10., 16.,  1., 53., 42.,\n",
       "        54., 13., 36.,  9., 34., 19.]))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r = np.random.randint(1, 20000)\n",
    "X[r], Y[r]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# make connections between layers (synapses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     43,
     72
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> \u001b[0;32m<ipython-input-13-c0b10a340e06>\u001b[0m(40)\u001b[0;36mforwardBackwardPropagation\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m     38 \u001b[0;31m        \u001b[0;32mreturn\u001b[0m \u001b[0marr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     39 \u001b[0;31m    \u001b[0mpdb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_trace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m---> 40 \u001b[0;31m    \u001b[0ml3_error\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ml3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m   \u001b[0;31m# l3_error: 1 x normal\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     41 \u001b[0;31m    \u001b[0ml3_delta\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ml3_error\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnonlin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ml3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mderiv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m   \u001b[0;31m# l3_delta: 1 x normal\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     42 \u001b[0;31m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n",
      "ipdb> b 49\n",
      "Breakpoint 3 at <ipython-input-13-c0b10a340e06>:49\n",
      "ipdb> r\n",
      "> \u001b[0;32m<ipython-input-13-c0b10a340e06>\u001b[0m(49)\u001b[0;36mforwardBackwardPropagation\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m     47 \u001b[0;31m    \u001b[0ml1_delta\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ml1_error\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnonlin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ml1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mderiv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m   \u001b[0;31m# l1_delta:  1 x small\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     48 \u001b[0;31m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[1;31m3\u001b[0;32m--> 49 \u001b[0;31m    \u001b[0ml3_delta_reshaped\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ml3_delta\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_len\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     50 \u001b[0;31m    \u001b[0msyn2\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0ml2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ml3_delta_reshaped\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# syn2: large x normal, l2.T: large x 1, l3_delta: 1 x normal\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     51 \u001b[0;31m    \u001b[0msyn1\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0ml1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ml2_delta\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n",
      "ipdb> s\n",
      "--Call--\n",
      "> \u001b[0;32m<__array_function__ internals>\u001b[0m(2)\u001b[0;36mreshape\u001b[0;34m()\u001b[0m\n",
      "\n",
      "ipdb> return\n",
      "--Return--\n",
      "array([[0., 0... 0., 0., 0.]])\n",
      "> \u001b[0;32m<__array_function__ internals>\u001b[0m(6)\u001b[0;36mreshape\u001b[0;34m()\u001b[0m\n",
      "\n",
      "ipdb> return\n",
      "> \u001b[0;32m<ipython-input-13-c0b10a340e06>\u001b[0m(50)\u001b[0;36mforwardBackwardPropagation\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m     48 \u001b[0;31m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[1;31m3\u001b[0;32m    49 \u001b[0;31m    \u001b[0ml3_delta_reshaped\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ml3_delta\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_len\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m---> 50 \u001b[0;31m    \u001b[0msyn2\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0ml2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ml3_delta_reshaped\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# syn2: large x normal, l2.T: large x 1, l3_delta: 1 x normal\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     51 \u001b[0;31m    \u001b[0msyn1\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0ml1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ml2_delta\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     52 \u001b[0;31m    \u001b[0msyn0\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0ml0\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ml1_delta\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n",
      "ipdb> s\n",
      "ValueError: shapes (64,) and (1,45) not aligned: 64 (dim 0) != 1 (dim 0)\n",
      "> \u001b[0;32m<ipython-input-13-c0b10a340e06>\u001b[0m(50)\u001b[0;36mforwardBackwardPropagation\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m     48 \u001b[0;31m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[1;31m3\u001b[0;32m    49 \u001b[0;31m    \u001b[0ml3_delta_reshaped\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ml3_delta\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_len\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m---> 50 \u001b[0;31m    \u001b[0msyn2\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0ml2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ml3_delta_reshaped\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# syn2: large x normal, l2.T: large x 1, l3_delta: 1 x normal\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     51 \u001b[0;31m    \u001b[0msyn1\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0ml1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ml2_delta\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     52 \u001b[0;31m    \u001b[0msyn0\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0ml0\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ml1_delta\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "l = len(lines)\n",
    "# seed is good for debugging, makes sure we start at\n",
    "# the same time\n",
    "np.random.seed(1)\n",
    "sig = 0.7 # ratio between layers\n",
    "\n",
    "# synapses (or connections)\n",
    "# np.random.uniform(low=0.0, high=1.0, size=None)\n",
    "rng = default_rng()\n",
    "\n",
    "\n",
    "\n",
    "# the lower limit is set to 1.0 and upper is set to 26.1 because of the best and worst\n",
    "# cases. In the best case the letter in position i might be the same letter as mid+i.\n",
    "# In the worst case, the letter might an 'a' whereas it was supposed to be a 'z'\n",
    "syn0 = rng.uniform(1.0, 26.1, (max_len, int(max_len*sig)))\n",
    "syn1 = rng.uniform(1.0, 26.1, (int(max_len*sig), int(max_len/sig)))\n",
    "syn2 = rng.uniform(1.0, 26.1, (int(max_len/sig), int(max_len)))  # large x normal\n",
    "# synN = np.random.random((int(max_len*sig),max_len))     # this is going to be\n",
    "import pdb\n",
    "\n",
    "def forwardBackwardPropagation(x, y, predict=False):\n",
    "    global syn0, syn1, syn2\n",
    "    # forward propagation\n",
    "    l0 = x # 1 x max_len\n",
    "    l1 = nonlin(np.dot(l0, syn0)) # 1 x normal . normal x small   \n",
    "    l2 = nonlin(np.dot(l1, syn1)) # 1 x small . small x large\n",
    "    l3 = nonlin(np.dot(l2, syn2)) # 1 x large . large x normal,   normal = max_len\n",
    "    \n",
    "    if predict:\n",
    "        arr = np.empty([1, max_len])\n",
    "        for i, el in enumerate(l3):\n",
    "            if np.abs(el - int(el)) < 0.5:\n",
    "                v = int(el)\n",
    "            else:\n",
    "                v = int(el) + 1\n",
    "            arr[i] = v + 64\n",
    "        return arr\n",
    "    pdb.set_trace()\n",
    "    l3_error = y * (np.log(l3))   # l3_error: 1 x normal\n",
    "    l3_delta = l3_error * nonlin(l3, deriv=True)   # l3_delta: 1 x normal\n",
    "    \n",
    "    l2_error = l3_delta.dot(syn2.T) # l2_erro: 1 x normal . normal x large = 1 x large\n",
    "    l2_delta = l2_error * nonlin(l2, deriv=True)  #  l2_delta: 1 x large \n",
    "    \n",
    "    l1_error = l2_delta.dot(syn1.T)  # l1_error: 1 x large . large x small = 1 x small\n",
    "    l1_delta = l1_error * nonlin(l1, deriv=True)   # l1_delta:  1 x small\n",
    "    \n",
    "    large = int(max_len / sig)\n",
    "    small = int (max_len * sig)\n",
    "    normal = max_len\n",
    "    syn2 += np.reshape(l2.T, (large, 1)).dot(np.reshape(l3_delta, (1, normal))  # syn2: large x normal, l2.T: large x 1, l3_delta: 1 x normal\n",
    "    syn1 += l1.T.dot(l2_delta.T)\n",
    "    syn0 += l0.T.dot(l1_delta.T)\n",
    "    \n",
    "    return\n",
    "\n",
    "l = len(Y)\n",
    "for i in np.arange(l):\n",
    "    forwardBackwardPropagation(X_train[i], Y_train[i])\n",
    "    \n",
    "def validate():\n",
    "    for x, y in zip(X_validation, Y_validation):\n",
    "        predicted = forwardBackwardPropagation(x, y, True)\n",
    "        predictedWord = ''.join([chr(el) % 0x110000 for el in predicted])\n",
    "        actualWord = ''.join([chr(el) % 0x110000 for el in X])\n",
    "        print(f\"actual word: {actualWord}, predicted word: {predictedWord}\")\n",
    "validate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on built-in function uniform:\n",
      "\n",
      "uniform(...) method of numpy.random.mtrand.RandomState instance\n",
      "    uniform(low=0.0, high=1.0, size=None)\n",
      "    \n",
      "    Draw samples from a uniform distribution.\n",
      "    \n",
      "    Samples are uniformly distributed over the half-open interval\n",
      "    ``[low, high)`` (includes low, but excludes high).  In other words,\n",
      "    any value within the given interval is equally likely to be drawn\n",
      "    by `uniform`.\n",
      "    \n",
      "    .. note::\n",
      "        New code should use the ``uniform`` method of a ``default_rng()``\n",
      "        instance instead; see `random-quick-start`.\n",
      "    \n",
      "    Parameters\n",
      "    ----------\n",
      "    low : float or array_like of floats, optional\n",
      "        Lower boundary of the output interval.  All values generated will be\n",
      "        greater than or equal to low.  The default value is 0.\n",
      "    high : float or array_like of floats\n",
      "        Upper boundary of the output interval.  All values generated will be\n",
      "        less than or equal to high.  The default value is 1.0.\n",
      "    size : int or tuple of ints, optional\n",
      "        Output shape.  If the given shape is, e.g., ``(m, n, k)``, then\n",
      "        ``m * n * k`` samples are drawn.  If size is ``None`` (default),\n",
      "        a single value is returned if ``low`` and ``high`` are both scalars.\n",
      "        Otherwise, ``np.broadcast(low, high).size`` samples are drawn.\n",
      "    \n",
      "    Returns\n",
      "    -------\n",
      "    out : ndarray or scalar\n",
      "        Drawn samples from the parameterized uniform distribution.\n",
      "    \n",
      "    See Also\n",
      "    --------\n",
      "    randint : Discrete uniform distribution, yielding integers.\n",
      "    random_integers : Discrete uniform distribution over the closed\n",
      "                      interval ``[low, high]``.\n",
      "    random_sample : Floats uniformly distributed over ``[0, 1)``.\n",
      "    random : Alias for `random_sample`.\n",
      "    rand : Convenience function that accepts dimensions as input, e.g.,\n",
      "           ``rand(2,2)`` would generate a 2-by-2 array of floats,\n",
      "           uniformly distributed over ``[0, 1)``.\n",
      "    Generator.uniform: which should be used for new code.\n",
      "    \n",
      "    Notes\n",
      "    -----\n",
      "    The probability density function of the uniform distribution is\n",
      "    \n",
      "    .. math:: p(x) = \\frac{1}{b - a}\n",
      "    \n",
      "    anywhere within the interval ``[a, b)``, and zero elsewhere.\n",
      "    \n",
      "    When ``high`` == ``low``, values of ``low`` will be returned.\n",
      "    If ``high`` < ``low``, the results are officially undefined\n",
      "    and may eventually raise an error, i.e. do not rely on this\n",
      "    function to behave when passed arguments satisfying that\n",
      "    inequality condition. The ``high`` limit may be included in the\n",
      "    returned array of floats due to floating-point rounding in the\n",
      "    equation ``low + (high-low) * random_sample()``. For example:\n",
      "    \n",
      "    >>> x = np.float32(5*0.99999999)\n",
      "    >>> x\n",
      "    5.0\n",
      "    \n",
      "    \n",
      "    Examples\n",
      "    --------\n",
      "    Draw samples from the distribution:\n",
      "    \n",
      "    >>> s = np.random.uniform(-1,0,1000)\n",
      "    \n",
      "    All values are within the given interval:\n",
      "    \n",
      "    >>> np.all(s >= -1)\n",
      "    True\n",
      "    >>> np.all(s < 0)\n",
      "    True\n",
      "    \n",
      "    Display the histogram of the samples, along with the\n",
      "    probability density function:\n",
      "    \n",
      "    >>> import matplotlib.pyplot as plt\n",
      "    >>> count, bins, ignored = plt.hist(s, 15, density=True)\n",
      "    >>> plt.plot(bins, np.ones_like(bins), linewidth=2, color='r')\n",
      "    >>> plt.show()\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(np.random.uniform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25487 25487\n"
     ]
    }
   ],
   "source": [
    "print(len(X), len(Y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-05-04 17:22:43,302 : INFO : loading projection weights from /Users/prajjwaldangal/Downloads/GoogleNews-vectors-negative300.bin\n",
      "2021-05-04 17:23:29,733 : INFO : loaded (3000000, 300) matrix from /Users/prajjwaldangal/Downloads/GoogleNews-vectors-negative300.bin\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading the input file\n"
     ]
    }
   ],
   "source": [
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "model_vec = gensim.models.KeyedVectors.load_word2vec_format('/Users/prajjwaldangal/Downloads/GoogleNews-vectors-negative300.bin', binary=True)\n",
    "_VECTOR_SIZE = int(os.environ.get('VECTOR_SIZE', '300'))\n",
    "_HIDDEN_DIM = int(os.environ.get('HIDDEN_DIM', '500'))\n",
    "_LEARNING_RATE = float(os.environ.get('LEARNING_RATE', '0.0025'))\n",
    "_NEPOCH = int(os.environ.get('NEPOCH', '10'))\n",
    "evaluate_loss_after = 2\n",
    "vocabulary_size = 25487\n",
    "\n",
    "_MODEL_FILE = os.environ.get('MODEL_FILE')\n",
    "vector_size = _VECTOR_SIZE\n",
    "unknown_token = \"UNKNOWN_TOKEN\"\n",
    "sentence_start_token = \"SENTENCE_START\"\n",
    "sentence_end_token = \"SENTENCE_END\"\n",
    "hidden_dim = _HIDDEN_DIM\n",
    "learning_rate = _LEARNING_RATE\n",
    "nepoch = _NEPOCH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNNumpy:\n",
    "     \n",
    "    def __init__(self, vector_dim , hidden_dim, label_dim = 6 , bptt_truncate = 6):\n",
    "        # Assign instance variables\n",
    "        self.label_dim = label_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.bptt_truncate = bptt_truncate\n",
    "        self.vector_dim = vector_dim\n",
    "        # Randomly initialize the network parameters\n",
    "        self.U = np.random.uniform(-np.sqrt(1./vector_dim), np.sqrt(1./vector_dim), (hidden_dim, vector_dim))\n",
    "        self.V = np.random.uniform(-np.sqrt(1./hidden_dim), np.sqrt(1./hidden_dim), (label_dim, hidden_dim))\n",
    "        self.W = np.random.uniform(-np.sqrt(1./hidden_dim), np.sqrt(1./hidden_dim), (hidden_dim, hidden_dim))\n",
    "\n",
    "def forward_propagation(self, x):\n",
    "    # import pdb; pdb.set_trace()\n",
    "\n",
    "    # The total number of time steps\n",
    "    T = len(x)\n",
    "    # During forward propagation we save all hidden states in s because need them later.\n",
    "    # We add one additional element for the initial hidden, which we set to 0\n",
    "    s = np.zeros((T + 1, self.hidden_dim))\n",
    "    s[-1] = np.zeros(self.hidden_dim)\n",
    "    # The outputs at each time step. Again, we save them for later.\n",
    "    o = np.zeros((T+1, self.label_dim))\n",
    "    # For each time step...\n",
    "    for t in np.arange(T):\n",
    "        X_i = x[t]\n",
    "        s[t] = np.tanh(self.U.dot(X_i) + self.W.dot(s[t-1]))\n",
    "    o = softmax(self.V.dot(s[-2]))\n",
    "    return [o, s]\n",
    "RNNNumpy.forward_propagation = forward_propagation\n",
    "\n",
    "def predict(self, x):\n",
    "    # Perform forward propagation and return index of the highest score\n",
    "    o, s = self.forward_propagation(x)\n",
    "    return np.argmax(o, axis=0)\n",
    " \n",
    "RNNNumpy.predict = predict\n",
    "\n",
    "\n",
    "def calculate_total_loss(self, x, y):\n",
    "    L = 0.0\n",
    "    # For each sentence...\n",
    "    for i in np.arange(len(y)):\n",
    "        o, s = self.forward_propagation(x[i])\n",
    "        # We only care about our prediction of the \"correct\" words\n",
    "        correct_word_predictions = o[y[i]]\n",
    "        # Add to the loss based on how off we were\n",
    "        L += -1.0 * (np.log(correct_word_predictions))\n",
    "    return L\n",
    " \n",
    "def calculate_loss(self, x, y):\n",
    "    # Divide the total loss by the number of training examples\n",
    "    # y -> all y_train[i]\n",
    "    N = len(y)\n",
    "    return self.calculate_total_loss(x,y)/N\n",
    " \n",
    "RNNNumpy.calculate_total_loss = calculate_total_loss\n",
    "RNNNumpy.calculate_loss = calculate_loss\n",
    "\n",
    "def bptt(self, x, y):\n",
    "    T = len(x)\n",
    "    # Perform forward propagation\n",
    "    o, s = self.forward_propagation(x)\n",
    "    # We accumulate the gradients in these variables\n",
    "    dLdU = np.zeros(self.U.shape)\n",
    "    dLdV = np.zeros(self.V.shape)\n",
    "    dLdW = np.zeros(self.W.shape)\n",
    "    delta_o = o\n",
    "    delta_o[y] -= 1.\n",
    "    # For each output backwards...\n",
    "    dLdV += np.outer(delta_o, s[T-1].T)\n",
    "    # Initial delta calculation\n",
    "    delta_t = self.V.T.dot(delta_o) * (1 - (s[-1] ** 2))\n",
    "    # Backpropagation through time (for at most self.bptt_truncate steps)\n",
    "    for bptt_step in np.arange(max(0, (T-1)-self.bptt_truncate), (T-1)+1)[::-1]:\n",
    "        #print \"Backpropagation step t=%d bptt step=%d \" % (t, bptt_step)\n",
    "        dLdW += np.outer(delta_t, s[bptt_step-1])     \n",
    "        X_j = x[bptt_step]\n",
    "        dLdU += np.outer(delta_t, X_j)   \n",
    "        # Update delta for next step\n",
    "        delta_t = self.W.T.dot(delta_t) * (1 - s[bptt_step-1] ** 2)\n",
    "    return [dLdU, dLdV, dLdW]\n",
    " \n",
    "RNNNumpy.bptt = bptt\n",
    "\n",
    "\n",
    "# Performs one step of SGD.\n",
    "def numpy_sgd_step(self, x, y, learning_rate):\n",
    "    # Calculate the gradients\n",
    "    dLdU, dLdV, dLdW = self.bptt(x, y)\n",
    "    # Change parameters according to gradients and learning rate\n",
    "    self.U -= learning_rate * dLdU\n",
    "    self.V -= learning_rate * dLdV\n",
    "    self.W -= learning_rate * dLdW\n",
    " \n",
    "RNNNumpy.sgd_step = numpy_sgd_step\n",
    "\n",
    "\n",
    "# Outer SGD Loop\n",
    "# - model: The RNN model instance\n",
    "# - X_train: The training data set\n",
    "# - y_train: The training data labels\n",
    "# - learning_rate: Initial learning rate for SGD\n",
    "# - nepoch: Number of times to iterate through the complete dataset\n",
    "# - evaluate_loss_after: Evaluate the loss after this many epochs\n",
    "def train_with_sgd(model, X_train, y_train, learning_rate, nepoch, evaluate_loss_after):\n",
    "    # We keep track of the losses so we can plot them later\n",
    "    losses = []\n",
    "    num_examples_seen = 0\n",
    "    print (\"There is going to be \", nepoch, \" epoches\")\n",
    "    for epoch in range(nepoch):\n",
    "        if (epoch % evaluate_loss_after == 0):\n",
    "            loss = model.calculate_loss(X_train, y_train)\n",
    "            losses.append((num_examples_seen, loss))\n",
    "            time = datetime.now().strftime('%Y-%m-%d-%H-%M-%S')\n",
    "            print (\"%s: Loss after num_examples_seen=%d epoch=%d: %f\" % (time, num_examples_seen, epoch, loss))\n",
    "            # Adjust the learning rate if loss increases\n",
    "            if (len(losses) > 1 and losses[-1][1] > losses[-2][1]):\n",
    "                learning_rate = learning_rate * 0.5  \n",
    "                print (\"Setting learning rate to %f\" % learning_rate)\n",
    "            sys.stdout.flush()\n",
    "        # For each training example...\n",
    "        \n",
    "        for i in range(len(y_train)):\n",
    "            # One SGD step\n",
    "            model.sgd_step(X_train[i], y_train[i], learning_rate)\n",
    "            num_examples_seen += 1\n",
    "    filename11 = 'numpy_model.sav'\n",
    "    pickle.dump(model, open(filename11, 'wb'))\n",
    "model = RNNNumpy(vector_size,hidden_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_with_sgd(model, X_train, y_train, learning_rate, nepoch, evaluate_loss_after)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "linkedin_course",
   "language": "python",
   "name": "linkedin_course"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
