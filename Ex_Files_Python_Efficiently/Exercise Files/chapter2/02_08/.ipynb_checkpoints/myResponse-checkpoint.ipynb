{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "f=open('goldmedals.txt', 'r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections as cls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Paavo Nurmi', 9),\n",
       " ('Carl Lewis', 9),\n",
       " ('Usain Bolt', 9),\n",
       " ('Ray Ewry', 8),\n",
       " ('Allyson Felix', 6)]"
      ]
     },
     "execution_count": 217,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "medal = cls.namedtuple('medal', ['year', 'athelete', 'nation', 'event'])\n",
    "medals = [medal(*line.split(\"\\t\")) for line in f]\n",
    "c = cls.Counter(medal.athelete for medal in medals)\n",
    "c.most_common(5)\n",
    "\n",
    "# atheletes with most number of gold medals in different events\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# bruteforce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Paavo Nurmi', {'cross country individual', 'cross country team', '5000m', '1500m', '10000m', '3000m team'})\n",
      "('Ville Ritola', {'cross country team', '5000m', '10000m', '3000m steeplechase', '3000m team'})\n",
      "('Alvin Kraenzlein', {'60m', '200m hurdles', 'long jump', '110m hurdles'})\n",
      "('Hannes Kolehmainen', {'cross country individual', '5000m', '10000m', 'marathon'})\n",
      "('Jesse Owens', {'200m', 'long jump', '100m', '4x100m relay'})\n"
     ]
    }
   ],
   "source": [
    "dct = cls.defaultdict(list)\n",
    "for medal in medals:\n",
    "    dct[medal.athelete]\n",
    "    dct[medal.athelete].append(medal.event)\n",
    "\n",
    "def clean(event):\n",
    "    return ' '.join(word for word in event.split() if word not in ('men','women'))\n",
    "\n",
    "dct2 = cls.defaultdict(set)\n",
    "for athelete, events in dct.items():\n",
    "    for event in events:\n",
    "        dct2[athelete].add(clean(event))\n",
    "import operator\n",
    "sorted(dct2.items(), key=lambda x: len(x[1]))\n",
    "# for i in range(5):\n",
    "#     athelete=max(dct2.items(), key=lambda x: len(x[1]))\n",
    "#     print(athelete)#, dct2[athelete])\n",
    "#     dct2.pop(athelete[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "k = cls.Counter('abc') - cls.Counter('bc')\n",
    "print(sum(k.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class dict in module builtins:\n",
      "\n",
      "class dict(object)\n",
      " |  dict() -> new empty dictionary\n",
      " |  dict(mapping) -> new dictionary initialized from a mapping object's\n",
      " |      (key, value) pairs\n",
      " |  dict(iterable) -> new dictionary initialized as if via:\n",
      " |      d = {}\n",
      " |      for k, v in iterable:\n",
      " |          d[k] = v\n",
      " |  dict(**kwargs) -> new dictionary initialized with the name=value pairs\n",
      " |      in the keyword argument list.  For example:  dict(one=1, two=2)\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __contains__(self, key, /)\n",
      " |      True if the dictionary has the specified key, else False.\n",
      " |  \n",
      " |  __delitem__(self, key, /)\n",
      " |      Delete self[key].\n",
      " |  \n",
      " |  __eq__(self, value, /)\n",
      " |      Return self==value.\n",
      " |  \n",
      " |  __ge__(self, value, /)\n",
      " |      Return self>=value.\n",
      " |  \n",
      " |  __getattribute__(self, name, /)\n",
      " |      Return getattr(self, name).\n",
      " |  \n",
      " |  __getitem__(...)\n",
      " |      x.__getitem__(y) <==> x[y]\n",
      " |  \n",
      " |  __gt__(self, value, /)\n",
      " |      Return self>value.\n",
      " |  \n",
      " |  __init__(self, /, *args, **kwargs)\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |  \n",
      " |  __iter__(self, /)\n",
      " |      Implement iter(self).\n",
      " |  \n",
      " |  __le__(self, value, /)\n",
      " |      Return self<=value.\n",
      " |  \n",
      " |  __len__(self, /)\n",
      " |      Return len(self).\n",
      " |  \n",
      " |  __lt__(self, value, /)\n",
      " |      Return self<value.\n",
      " |  \n",
      " |  __ne__(self, value, /)\n",
      " |      Return self!=value.\n",
      " |  \n",
      " |  __repr__(self, /)\n",
      " |      Return repr(self).\n",
      " |  \n",
      " |  __setitem__(self, key, value, /)\n",
      " |      Set self[key] to value.\n",
      " |  \n",
      " |  __sizeof__(...)\n",
      " |      D.__sizeof__() -> size of D in memory, in bytes\n",
      " |  \n",
      " |  clear(...)\n",
      " |      D.clear() -> None.  Remove all items from D.\n",
      " |  \n",
      " |  copy(...)\n",
      " |      D.copy() -> a shallow copy of D\n",
      " |  \n",
      " |  get(self, key, default=None, /)\n",
      " |      Return the value for key if key is in the dictionary, else default.\n",
      " |  \n",
      " |  items(...)\n",
      " |      D.items() -> a set-like object providing a view on D's items\n",
      " |  \n",
      " |  keys(...)\n",
      " |      D.keys() -> a set-like object providing a view on D's keys\n",
      " |  \n",
      " |  pop(...)\n",
      " |      D.pop(k[,d]) -> v, remove specified key and return the corresponding value.\n",
      " |      If key is not found, d is returned if given, otherwise KeyError is raised\n",
      " |  \n",
      " |  popitem(...)\n",
      " |      D.popitem() -> (k, v), remove and return some (key, value) pair as a\n",
      " |      2-tuple; but raise KeyError if D is empty.\n",
      " |  \n",
      " |  setdefault(self, key, default=None, /)\n",
      " |      Insert key with a value of default if key is not in the dictionary.\n",
      " |      \n",
      " |      Return the value for key if key is in the dictionary, else default.\n",
      " |  \n",
      " |  update(...)\n",
      " |      D.update([E, ]**F) -> None.  Update D from dict/iterable E and F.\n",
      " |      If E is present and has a .keys() method, then does:  for k in E: D[k] = E[k]\n",
      " |      If E is present and lacks a .keys() method, then does:  for k, v in E: D[k] = v\n",
      " |      In either case, this is followed by: for k in F:  D[k] = F[k]\n",
      " |  \n",
      " |  values(...)\n",
      " |      D.values() -> an object providing a view on D's values\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods defined here:\n",
      " |  \n",
      " |  fromkeys(iterable, value=None, /) from builtins.type\n",
      " |      Create a new dictionary with keys from iterable and values set to value.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Static methods defined here:\n",
      " |  \n",
      " |  __new__(*args, **kwargs) from builtins.type\n",
      " |      Create and return a new object.  See help(type) for accurate signature.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes defined here:\n",
      " |  \n",
      " |  __hash__ = None\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Paavo Nurmi', 9),\n",
       " ('Carl Lewis', 9),\n",
       " ('Usain Bolt', 9),\n",
       " ('Ray Ewry', 8),\n",
       " ('Allyson Felix', 6)]"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "medals = [medal(*line.split(\"\\t\")) for line in open('goldmedals.txt', 'r')]\n",
    "c = cls.Counter(medal.athelete for medal in medals)\n",
    "c.most_common(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Challenge # 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class set in module builtins:\n",
      "\n",
      "class set(object)\n",
      " |  set() -> new empty set object\n",
      " |  set(iterable) -> new set object\n",
      " |  \n",
      " |  Build an unordered collection of unique elements.\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __and__(self, value, /)\n",
      " |      Return self&value.\n",
      " |  \n",
      " |  __contains__(...)\n",
      " |      x.__contains__(y) <==> y in x.\n",
      " |  \n",
      " |  __eq__(self, value, /)\n",
      " |      Return self==value.\n",
      " |  \n",
      " |  __ge__(self, value, /)\n",
      " |      Return self>=value.\n",
      " |  \n",
      " |  __getattribute__(self, name, /)\n",
      " |      Return getattr(self, name).\n",
      " |  \n",
      " |  __gt__(self, value, /)\n",
      " |      Return self>value.\n",
      " |  \n",
      " |  __iand__(self, value, /)\n",
      " |      Return self&=value.\n",
      " |  \n",
      " |  __init__(self, /, *args, **kwargs)\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |  \n",
      " |  __ior__(self, value, /)\n",
      " |      Return self|=value.\n",
      " |  \n",
      " |  __isub__(self, value, /)\n",
      " |      Return self-=value.\n",
      " |  \n",
      " |  __iter__(self, /)\n",
      " |      Implement iter(self).\n",
      " |  \n",
      " |  __ixor__(self, value, /)\n",
      " |      Return self^=value.\n",
      " |  \n",
      " |  __le__(self, value, /)\n",
      " |      Return self<=value.\n",
      " |  \n",
      " |  __len__(self, /)\n",
      " |      Return len(self).\n",
      " |  \n",
      " |  __lt__(self, value, /)\n",
      " |      Return self<value.\n",
      " |  \n",
      " |  __ne__(self, value, /)\n",
      " |      Return self!=value.\n",
      " |  \n",
      " |  __or__(self, value, /)\n",
      " |      Return self|value.\n",
      " |  \n",
      " |  __rand__(self, value, /)\n",
      " |      Return value&self.\n",
      " |  \n",
      " |  __reduce__(...)\n",
      " |      Return state information for pickling.\n",
      " |  \n",
      " |  __repr__(self, /)\n",
      " |      Return repr(self).\n",
      " |  \n",
      " |  __ror__(self, value, /)\n",
      " |      Return value|self.\n",
      " |  \n",
      " |  __rsub__(self, value, /)\n",
      " |      Return value-self.\n",
      " |  \n",
      " |  __rxor__(self, value, /)\n",
      " |      Return value^self.\n",
      " |  \n",
      " |  __sizeof__(...)\n",
      " |      S.__sizeof__() -> size of S in memory, in bytes\n",
      " |  \n",
      " |  __sub__(self, value, /)\n",
      " |      Return self-value.\n",
      " |  \n",
      " |  __xor__(self, value, /)\n",
      " |      Return self^value.\n",
      " |  \n",
      " |  add(...)\n",
      " |      Add an element to a set.\n",
      " |      \n",
      " |      This has no effect if the element is already present.\n",
      " |  \n",
      " |  clear(...)\n",
      " |      Remove all elements from this set.\n",
      " |  \n",
      " |  copy(...)\n",
      " |      Return a shallow copy of a set.\n",
      " |  \n",
      " |  difference(...)\n",
      " |      Return the difference of two or more sets as a new set.\n",
      " |      \n",
      " |      (i.e. all elements that are in this set but not the others.)\n",
      " |  \n",
      " |  difference_update(...)\n",
      " |      Remove all elements of another set from this set.\n",
      " |  \n",
      " |  discard(...)\n",
      " |      Remove an element from a set if it is a member.\n",
      " |      \n",
      " |      If the element is not a member, do nothing.\n",
      " |  \n",
      " |  intersection(...)\n",
      " |      Return the intersection of two sets as a new set.\n",
      " |      \n",
      " |      (i.e. all elements that are in both sets.)\n",
      " |  \n",
      " |  intersection_update(...)\n",
      " |      Update a set with the intersection of itself and another.\n",
      " |  \n",
      " |  isdisjoint(...)\n",
      " |      Return True if two sets have a null intersection.\n",
      " |  \n",
      " |  issubset(...)\n",
      " |      Report whether another set contains this set.\n",
      " |  \n",
      " |  issuperset(...)\n",
      " |      Report whether this set contains another set.\n",
      " |  \n",
      " |  pop(...)\n",
      " |      Remove and return an arbitrary set element.\n",
      " |      Raises KeyError if the set is empty.\n",
      " |  \n",
      " |  remove(...)\n",
      " |      Remove an element from a set; it must be a member.\n",
      " |      \n",
      " |      If the element is not a member, raise a KeyError.\n",
      " |  \n",
      " |  symmetric_difference(...)\n",
      " |      Return the symmetric difference of two sets as a new set.\n",
      " |      \n",
      " |      (i.e. all elements that are in exactly one of the sets.)\n",
      " |  \n",
      " |  symmetric_difference_update(...)\n",
      " |      Update a set with the symmetric difference of itself and another.\n",
      " |  \n",
      " |  union(...)\n",
      " |      Return the union of sets as a new set.\n",
      " |      \n",
      " |      (i.e. all elements that are in either set.)\n",
      " |  \n",
      " |  update(...)\n",
      " |      Update a set with the union of itself and others.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Static methods defined here:\n",
      " |  \n",
      " |  __new__(*args, **kwargs) from builtins.type\n",
      " |      Create and return a new object.  See help(type) for accurate signature.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes defined here:\n",
      " |  \n",
      " |  __hash__ = None\n",
      "\n"
     ]
    }
   ],
   "source": [
    "s = set()\n",
    "help(set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = set([medal.event for medal in medals])\n",
    "dct = cls.defaultdict(int)\n",
    "for event in k:\n",
    "    for medal in medals:\n",
    "        dct[medal.athelete] += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting gensim\n",
      "  Using cached gensim-3.8.3-cp37-cp37m-macosx_10_9_x86_64.whl (24.2 MB)\n",
      "Requirement already satisfied: six>=1.5.0 in /opt/anaconda3/lib/python3.7/site-packages (from gensim) (1.14.0)\n",
      "Requirement already satisfied: numpy>=1.11.3 in /opt/anaconda3/lib/python3.7/site-packages (from gensim) (1.18.1)\n",
      "Requirement already satisfied: scipy>=0.18.1 in /opt/anaconda3/lib/python3.7/site-packages (from gensim) (1.4.1)\n",
      "Processing /Users/prajjwaldangal/Library/Caches/pip/wheels/56/b5/6d/86dbe4f29d4688e5163a8b8c6b740494310040286fca4dc648/smart_open-2.1.0-py3-none-any.whl\n",
      "Collecting boto3\n",
      "  Using cached boto3-1.14.48-py2.py3-none-any.whl (129 kB)\n",
      "Requirement already satisfied: boto in /opt/anaconda3/lib/python3.7/site-packages (from smart-open>=1.8.1->gensim) (2.49.0)\n",
      "Requirement already satisfied: requests in /opt/anaconda3/lib/python3.7/site-packages (from smart-open>=1.8.1->gensim) (2.22.0)\n",
      "Collecting s3transfer<0.4.0,>=0.3.0\n",
      "  Using cached s3transfer-0.3.3-py2.py3-none-any.whl (69 kB)\n",
      "Collecting jmespath<1.0.0,>=0.7.1\n",
      "  Using cached jmespath-0.10.0-py2.py3-none-any.whl (24 kB)\n",
      "Collecting botocore<1.18.0,>=1.17.48\n",
      "  Using cached botocore-1.17.48-py2.py3-none-any.whl (6.5 MB)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /opt/anaconda3/lib/python3.7/site-packages (from requests->smart-open>=1.8.1->gensim) (1.25.8)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /opt/anaconda3/lib/python3.7/site-packages (from requests->smart-open>=1.8.1->gensim) (3.0.4)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in /opt/anaconda3/lib/python3.7/site-packages (from requests->smart-open>=1.8.1->gensim) (2.8)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/lib/python3.7/site-packages (from requests->smart-open>=1.8.1->gensim) (2019.11.28)\n",
      "Collecting docutils<0.16,>=0.10\n",
      "  Using cached docutils-0.15.2-py3-none-any.whl (547 kB)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /opt/anaconda3/lib/python3.7/site-packages (from botocore<1.18.0,>=1.17.48->boto3->smart-open>=1.8.1->gensim) (2.8.1)\n",
      "Installing collected packages: docutils, jmespath, botocore, s3transfer, boto3, smart-open, gensim\n",
      "  Attempting uninstall: docutils\n",
      "    Found existing installation: docutils 0.16\n",
      "    Uninstalling docutils-0.16:\n",
      "      Successfully uninstalled docutils-0.16\n",
      "Successfully installed boto3-1.14.48 botocore-1.17.48 docutils-0.15.2 gensim-3.8.3 jmespath-0.10.0 s3transfer-0.3.3 smart-open-2.1.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on module gensim.models.word2vec in gensim.models:\n",
      "\n",
      "NAME\n",
      "    gensim.models.word2vec\n",
      "\n",
      "DESCRIPTION\n",
      "    This module implements the word2vec family of algorithms, using highly optimized C routines,\n",
      "    data streaming and Pythonic interfaces.\n",
      "    \n",
      "    The word2vec algorithms include skip-gram and CBOW models, using either\n",
      "    hierarchical softmax or negative sampling: `Tomas Mikolov et al: Efficient Estimation of Word Representations\n",
      "    in Vector Space <https://arxiv.org/pdf/1301.3781.pdf>`_, `Tomas Mikolov et al: Distributed Representations of Words\n",
      "    and Phrases and their Compositionality <https://arxiv.org/abs/1310.4546>`_.\n",
      "    \n",
      "    Other embeddings\n",
      "    ================\n",
      "    \n",
      "    There are more ways to train word vectors in Gensim than just Word2Vec.\n",
      "    See also :class:`~gensim.models.doc2vec.Doc2Vec`, :class:`~gensim.models.fasttext.FastText` and\n",
      "    wrappers for :class:`~gensim.models.wrappers.VarEmbed` and :class:`~gensim.models.wrappers.WordRank`.\n",
      "    \n",
      "    The training algorithms were originally ported from the C package https://code.google.com/p/word2vec/\n",
      "    and extended with additional functionality and optimizations over the years.\n",
      "    \n",
      "    For a tutorial on Gensim word2vec, with an interactive web app trained on GoogleNews,\n",
      "    visit https://rare-technologies.com/word2vec-tutorial/.\n",
      "    \n",
      "    **Make sure you have a C compiler before installing Gensim, to use the optimized word2vec routines**\n",
      "    (70x speedup compared to plain NumPy implementation, https://rare-technologies.com/parallelizing-word2vec-in-python/).\n",
      "    \n",
      "    Usage examples\n",
      "    ==============\n",
      "    \n",
      "    Initialize a model with e.g.:\n",
      "    \n",
      "    .. sourcecode:: pycon\n",
      "    \n",
      "        >>> from gensim.test.utils import common_texts, get_tmpfile\n",
      "        >>> from gensim.models import Word2Vec\n",
      "        >>>\n",
      "        >>> path = get_tmpfile(\"word2vec.model\")\n",
      "        >>>\n",
      "        >>> model = Word2Vec(common_texts, size=100, window=5, min_count=1, workers=4)\n",
      "        >>> model.save(\"word2vec.model\")\n",
      "    \n",
      "    The training is streamed, meaning `sentences` can be a generator, reading input data\n",
      "    from disk on-the-fly, without loading the entire corpus into RAM.\n",
      "    \n",
      "    It also means you can continue training the model later:\n",
      "    \n",
      "    .. sourcecode:: pycon\n",
      "    \n",
      "        >>> model = Word2Vec.load(\"word2vec.model\")\n",
      "        >>> model.train([[\"hello\", \"world\"]], total_examples=1, epochs=1)\n",
      "        (0, 2)\n",
      "    \n",
      "    The trained word vectors are stored in a :class:`~gensim.models.keyedvectors.KeyedVectors` instance in `model.wv`:\n",
      "    \n",
      "    .. sourcecode:: pycon\n",
      "    \n",
      "        >>> vector = model.wv['computer']  # numpy vector of a word\n",
      "    \n",
      "    The reason for separating the trained vectors into `KeyedVectors` is that if you don't\n",
      "    need the full model state any more (don't need to continue training), the state can discarded,\n",
      "    resulting in a much smaller and faster object that can be mmapped for lightning\n",
      "    fast loading and sharing the vectors in RAM between processes:\n",
      "    \n",
      "    .. sourcecode:: pycon\n",
      "    \n",
      "        >>> from gensim.models import KeyedVectors\n",
      "        >>>\n",
      "        >>> path = get_tmpfile(\"wordvectors.kv\")\n",
      "        >>>\n",
      "        >>> model.wv.save(path)\n",
      "        >>> wv = KeyedVectors.load(\"model.wv\", mmap='r')\n",
      "        >>> vector = wv['computer']  # numpy vector of a word\n",
      "    \n",
      "    Gensim can also load word vectors in the \"word2vec C format\", as a\n",
      "    :class:`~gensim.models.keyedvectors.KeyedVectors` instance:\n",
      "    \n",
      "    .. sourcecode:: pycon\n",
      "    \n",
      "        >>> from gensim.test.utils import datapath\n",
      "        >>>\n",
      "        >>> wv_from_text = KeyedVectors.load_word2vec_format(datapath('word2vec_pre_kv_c'), binary=False)  # C text format\n",
      "        >>> wv_from_bin = KeyedVectors.load_word2vec_format(datapath(\"euclidean_vectors.bin\"), binary=True)  # C bin format\n",
      "    \n",
      "    It is impossible to continue training the vectors loaded from the C format because the hidden weights,\n",
      "    vocabulary frequencies and the binary tree are missing. To continue training, you'll need the\n",
      "    full :class:`~gensim.models.word2vec.Word2Vec` object state, as stored by :meth:`~gensim.models.word2vec.Word2Vec.save`,\n",
      "    not just the :class:`~gensim.models.keyedvectors.KeyedVectors`.\n",
      "    \n",
      "    You can perform various NLP word tasks with a trained model. Some of them\n",
      "    are already built-in - you can see it in :mod:`gensim.models.keyedvectors`.\n",
      "    \n",
      "    If you're finished training a model (i.e. no more updates, only querying),\n",
      "    you can switch to the :class:`~gensim.models.keyedvectors.KeyedVectors` instance:\n",
      "    \n",
      "    .. sourcecode:: pycon\n",
      "    \n",
      "        >>> word_vectors = model.wv\n",
      "        >>> del model\n",
      "    \n",
      "    to trim unneeded model state = use much less RAM and allow fast loading and memory sharing (mmap).\n",
      "    \n",
      "    Note that there is a :mod:`gensim.models.phrases` module which lets you automatically\n",
      "    detect phrases longer than one word. Using phrases, you can learn a word2vec model\n",
      "    where \"words\" are actually multiword expressions, such as `new_york_times` or `financial_crisis`:\n",
      "    \n",
      "    .. sourcecode:: pycon\n",
      "    \n",
      "        >>> from gensim.test.utils import common_texts\n",
      "        >>> from gensim.models import Phrases\n",
      "        >>>\n",
      "        >>> bigram_transformer = Phrases(common_texts)\n",
      "        >>> model = Word2Vec(bigram_transformer[common_texts], min_count=1)\n",
      "\n",
      "CLASSES\n",
      "    builtins.object\n",
      "        BrownCorpus\n",
      "        LineSentence\n",
      "        PathLineSentences\n",
      "        Text8Corpus\n",
      "    gensim.models.base_any2vec.BaseWordEmbeddingsModel(gensim.models.base_any2vec.BaseAny2VecModel)\n",
      "        Word2Vec\n",
      "    gensim.utils.SaveLoad(builtins.object)\n",
      "        Word2VecTrainables\n",
      "        Word2VecVocab\n",
      "    \n",
      "    class BrownCorpus(builtins.object)\n",
      "     |  BrownCorpus(dirname)\n",
      "     |  \n",
      "     |  Iterate over sentences from the `Brown corpus <https://en.wikipedia.org/wiki/Brown_Corpus>`_\n",
      "     |  (part of `NLTK data <https://www.nltk.org/data.html>`_).\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, dirname)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  __iter__(self)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors defined here:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "    \n",
      "    class LineSentence(builtins.object)\n",
      "     |  LineSentence(source, max_sentence_length=10000, limit=None)\n",
      "     |  \n",
      "     |  Iterate over a file that contains sentences: one line = one sentence.\n",
      "     |  Words must be already preprocessed and separated by whitespace.\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, source, max_sentence_length=10000, limit=None)\n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      source : string or a file-like object\n",
      "     |          Path to the file on disk, or an already-open file object (must support `seek(0)`).\n",
      "     |      limit : int or None\n",
      "     |          Clip the file to the first `limit` lines. Do no clipping if `limit is None` (the default).\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      .. sourcecode:: pycon\n",
      "     |      \n",
      "     |          >>> from gensim.test.utils import datapath\n",
      "     |          >>> sentences = LineSentence(datapath('lee_background.cor'))\n",
      "     |          >>> for sentence in sentences:\n",
      "     |          ...     pass\n",
      "     |  \n",
      "     |  __iter__(self)\n",
      "     |      Iterate through the lines in the source.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors defined here:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "    \n",
      "    class PathLineSentences(builtins.object)\n",
      "     |  PathLineSentences(source, max_sentence_length=10000, limit=None)\n",
      "     |  \n",
      "     |  Like :class:`~gensim.models.word2vec.LineSentence`, but process all files in a directory\n",
      "     |  in alphabetical order by filename.\n",
      "     |  \n",
      "     |  The directory must only contain files that can be read by :class:`gensim.models.word2vec.LineSentence`:\n",
      "     |  .bz2, .gz, and text files. Any file not ending with .bz2 or .gz is assumed to be a text file.\n",
      "     |  \n",
      "     |  The format of files (either text, or compressed text files) in the path is one sentence = one line,\n",
      "     |  with words already preprocessed and separated by whitespace.\n",
      "     |  \n",
      "     |  Warnings\n",
      "     |  --------\n",
      "     |  Does **not recurse** into subdirectories.\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, source, max_sentence_length=10000, limit=None)\n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      source : str\n",
      "     |          Path to the directory.\n",
      "     |      limit : int or None\n",
      "     |          Read only the first `limit` lines from each file. Read all if limit is None (the default).\n",
      "     |  \n",
      "     |  __iter__(self)\n",
      "     |      iterate through the files\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors defined here:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "    \n",
      "    class Text8Corpus(builtins.object)\n",
      "     |  Text8Corpus(fname, max_sentence_length=10000)\n",
      "     |  \n",
      "     |  Iterate over sentences from the \"text8\" corpus, unzipped from http://mattmahoney.net/dc/text8.zip.\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, fname, max_sentence_length=10000)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  __iter__(self)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors defined here:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "    \n",
      "    class Word2Vec(gensim.models.base_any2vec.BaseWordEmbeddingsModel)\n",
      "     |  Word2Vec(sentences=None, corpus_file=None, size=100, alpha=0.025, window=5, min_count=5, max_vocab_size=None, sample=0.001, seed=1, workers=3, min_alpha=0.0001, sg=0, hs=0, negative=5, ns_exponent=0.75, cbow_mean=1, hashfxn=<built-in function hash>, iter=5, null_word=0, trim_rule=None, sorted_vocab=1, batch_words=10000, compute_loss=False, callbacks=(), max_final_vocab=None)\n",
      "     |  \n",
      "     |  Train, use and evaluate neural networks described in https://code.google.com/p/word2vec/.\n",
      "     |  \n",
      "     |  Once you're finished training a model (=no more updates, only querying)\n",
      "     |  store and use only the :class:`~gensim.models.keyedvectors.KeyedVectors` instance in `self.wv` to reduce memory.\n",
      "     |  \n",
      "     |  The model can be stored/loaded via its :meth:`~gensim.models.word2vec.Word2Vec.save` and\n",
      "     |  :meth:`~gensim.models.word2vec.Word2Vec.load` methods.\n",
      "     |  \n",
      "     |  The trained word vectors can also be stored/loaded from a format compatible with the\n",
      "     |  original word2vec implementation via `self.wv.save_word2vec_format`\n",
      "     |  and :meth:`gensim.models.keyedvectors.KeyedVectors.load_word2vec_format`.\n",
      "     |  \n",
      "     |  Some important attributes are the following:\n",
      "     |  \n",
      "     |  Attributes\n",
      "     |  ----------\n",
      "     |  wv : :class:`~gensim.models.keyedvectors.Word2VecKeyedVectors`\n",
      "     |      This object essentially contains the mapping between words and embeddings. After training, it can be used\n",
      "     |      directly to query those embeddings in various ways. See the module level docstring for examples.\n",
      "     |  \n",
      "     |  vocabulary : :class:`~gensim.models.word2vec.Word2VecVocab`\n",
      "     |      This object represents the vocabulary (sometimes called Dictionary in gensim) of the model.\n",
      "     |      Besides keeping track of all unique words, this object provides extra functionality, such as\n",
      "     |      constructing a huffman tree (frequent words are closer to the root), or discarding extremely rare words.\n",
      "     |  \n",
      "     |  trainables : :class:`~gensim.models.word2vec.Word2VecTrainables`\n",
      "     |      This object represents the inner shallow neural network used to train the embeddings. The semantics of the\n",
      "     |      network differ slightly in the two available training modes (CBOW or SG) but you can think of it as a NN with\n",
      "     |      a single projection and hidden layer which we train on the corpus. The weights are then used as our embeddings\n",
      "     |      (which means that the size of the hidden layer is equal to the number of features `self.size`).\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      Word2Vec\n",
      "     |      gensim.models.base_any2vec.BaseWordEmbeddingsModel\n",
      "     |      gensim.models.base_any2vec.BaseAny2VecModel\n",
      "     |      gensim.utils.SaveLoad\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __contains__(self, word)\n",
      "     |      Deprecated. Use `self.wv.__contains__` instead.\n",
      "     |      Refer to the documentation for :meth:`~gensim.models.keyedvectors.Word2VecKeyedVectors.__contains__`.\n",
      "     |  \n",
      "     |  __getitem__(self, words)\n",
      "     |      Deprecated. Use `self.wv.__getitem__` instead.\n",
      "     |      Refer to the documentation for :meth:`~gensim.models.keyedvectors.Word2VecKeyedVectors.__getitem__`.\n",
      "     |  \n",
      "     |  __init__(self, sentences=None, corpus_file=None, size=100, alpha=0.025, window=5, min_count=5, max_vocab_size=None, sample=0.001, seed=1, workers=3, min_alpha=0.0001, sg=0, hs=0, negative=5, ns_exponent=0.75, cbow_mean=1, hashfxn=<built-in function hash>, iter=5, null_word=0, trim_rule=None, sorted_vocab=1, batch_words=10000, compute_loss=False, callbacks=(), max_final_vocab=None)\n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      sentences : iterable of iterables, optional\n",
      "     |          The `sentences` iterable can be simply a list of lists of tokens, but for larger corpora,\n",
      "     |          consider an iterable that streams the sentences directly from disk/network.\n",
      "     |          See :class:`~gensim.models.word2vec.BrownCorpus`, :class:`~gensim.models.word2vec.Text8Corpus`\n",
      "     |          or :class:`~gensim.models.word2vec.LineSentence` in :mod:`~gensim.models.word2vec` module for such examples.\n",
      "     |          See also the `tutorial on data streaming in Python\n",
      "     |          <https://rare-technologies.com/data-streaming-in-python-generators-iterators-iterables/>`_.\n",
      "     |          If you don't supply `sentences`, the model is left uninitialized -- use if you plan to initialize it\n",
      "     |          in some other way.\n",
      "     |      corpus_file : str, optional\n",
      "     |          Path to a corpus file in :class:`~gensim.models.word2vec.LineSentence` format.\n",
      "     |          You may use this argument instead of `sentences` to get performance boost. Only one of `sentences` or\n",
      "     |          `corpus_file` arguments need to be passed (or none of them, in that case, the model is left uninitialized).\n",
      "     |      size : int, optional\n",
      "     |          Dimensionality of the word vectors.\n",
      "     |      window : int, optional\n",
      "     |          Maximum distance between the current and predicted word within a sentence.\n",
      "     |      min_count : int, optional\n",
      "     |          Ignores all words with total frequency lower than this.\n",
      "     |      workers : int, optional\n",
      "     |          Use these many worker threads to train the model (=faster training with multicore machines).\n",
      "     |      sg : {0, 1}, optional\n",
      "     |          Training algorithm: 1 for skip-gram; otherwise CBOW.\n",
      "     |      hs : {0, 1}, optional\n",
      "     |          If 1, hierarchical softmax will be used for model training.\n",
      "     |          If 0, and `negative` is non-zero, negative sampling will be used.\n",
      "     |      negative : int, optional\n",
      "     |          If > 0, negative sampling will be used, the int for negative specifies how many \"noise words\"\n",
      "     |          should be drawn (usually between 5-20).\n",
      "     |          If set to 0, no negative sampling is used.\n",
      "     |      ns_exponent : float, optional\n",
      "     |          The exponent used to shape the negative sampling distribution. A value of 1.0 samples exactly in proportion\n",
      "     |          to the frequencies, 0.0 samples all words equally, while a negative value samples low-frequency words more\n",
      "     |          than high-frequency words. The popular default value of 0.75 was chosen by the original Word2Vec paper.\n",
      "     |          More recently, in https://arxiv.org/abs/1804.04212, Caselles-Dupr√©, Lesaint, & Royo-Letelier suggest that\n",
      "     |          other values may perform better for recommendation applications.\n",
      "     |      cbow_mean : {0, 1}, optional\n",
      "     |          If 0, use the sum of the context word vectors. If 1, use the mean, only applies when cbow is used.\n",
      "     |      alpha : float, optional\n",
      "     |          The initial learning rate.\n",
      "     |      min_alpha : float, optional\n",
      "     |          Learning rate will linearly drop to `min_alpha` as training progresses.\n",
      "     |      seed : int, optional\n",
      "     |          Seed for the random number generator. Initial vectors for each word are seeded with a hash of\n",
      "     |          the concatenation of word + `str(seed)`. Note that for a fully deterministically-reproducible run,\n",
      "     |          you must also limit the model to a single worker thread (`workers=1`), to eliminate ordering jitter\n",
      "     |          from OS thread scheduling. (In Python 3, reproducibility between interpreter launches also requires\n",
      "     |          use of the `PYTHONHASHSEED` environment variable to control hash randomization).\n",
      "     |      max_vocab_size : int, optional\n",
      "     |          Limits the RAM during vocabulary building; if there are more unique\n",
      "     |          words than this, then prune the infrequent ones. Every 10 million word types need about 1GB of RAM.\n",
      "     |          Set to `None` for no limit.\n",
      "     |      max_final_vocab : int, optional\n",
      "     |          Limits the vocab to a target vocab size by automatically picking a matching min_count. If the specified\n",
      "     |          min_count is more than the calculated min_count, the specified min_count will be used.\n",
      "     |          Set to `None` if not required.\n",
      "     |      sample : float, optional\n",
      "     |          The threshold for configuring which higher-frequency words are randomly downsampled,\n",
      "     |          useful range is (0, 1e-5).\n",
      "     |      hashfxn : function, optional\n",
      "     |          Hash function to use to randomly initialize weights, for increased training reproducibility.\n",
      "     |      iter : int, optional\n",
      "     |          Number of iterations (epochs) over the corpus.\n",
      "     |      trim_rule : function, optional\n",
      "     |          Vocabulary trimming rule, specifies whether certain words should remain in the vocabulary,\n",
      "     |          be trimmed away, or handled using the default (discard if word count < min_count).\n",
      "     |          Can be None (min_count will be used, look to :func:`~gensim.utils.keep_vocab_item`),\n",
      "     |          or a callable that accepts parameters (word, count, min_count) and returns either\n",
      "     |          :attr:`gensim.utils.RULE_DISCARD`, :attr:`gensim.utils.RULE_KEEP` or :attr:`gensim.utils.RULE_DEFAULT`.\n",
      "     |          The rule, if given, is only used to prune vocabulary during build_vocab() and is not stored as part of the\n",
      "     |          model.\n",
      "     |      \n",
      "     |          The input parameters are of the following types:\n",
      "     |              * `word` (str) - the word we are examining\n",
      "     |              * `count` (int) - the word's frequency count in the corpus\n",
      "     |              * `min_count` (int) - the minimum count threshold.\n",
      "     |      sorted_vocab : {0, 1}, optional\n",
      "     |          If 1, sort the vocabulary by descending frequency before assigning word indexes.\n",
      "     |          See :meth:`~gensim.models.word2vec.Word2VecVocab.sort_vocab()`.\n",
      "     |      batch_words : int, optional\n",
      "     |          Target size (in words) for batches of examples passed to worker threads (and\n",
      "     |          thus cython routines).(Larger batches will be passed if individual\n",
      "     |          texts are longer than 10000 words, but the standard cython code truncates to that maximum.)\n",
      "     |      compute_loss: bool, optional\n",
      "     |          If True, computes and stores loss value which can be retrieved using\n",
      "     |          :meth:`~gensim.models.word2vec.Word2Vec.get_latest_training_loss`.\n",
      "     |      callbacks : iterable of :class:`~gensim.models.callbacks.CallbackAny2Vec`, optional\n",
      "     |          Sequence of callbacks to be executed at specific stages during training.\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      Initialize and train a :class:`~gensim.models.word2vec.Word2Vec` model\n",
      "     |      \n",
      "     |      .. sourcecode:: pycon\n",
      "     |      \n",
      "     |          >>> from gensim.models import Word2Vec\n",
      "     |          >>> sentences = [[\"cat\", \"say\", \"meow\"], [\"dog\", \"say\", \"woof\"]]\n",
      "     |          >>> model = Word2Vec(sentences, min_count=1)\n",
      "     |  \n",
      "     |  __str__(self)\n",
      "     |      Human readable representation of the model's state.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      str\n",
      "     |          Human readable representation of the model's state, including the vocabulary size, vector size\n",
      "     |          and learning rate.\n",
      "     |  \n",
      "     |  accuracy(self, questions, restrict_vocab=30000, most_similar=None, case_insensitive=True)\n",
      "     |      Deprecated. Use `self.wv.accuracy` instead.\n",
      "     |      See :meth:`~gensim.models.word2vec.Word2VecKeyedVectors.accuracy`.\n",
      "     |  \n",
      "     |  clear_sims(self)\n",
      "     |      Remove all L2-normalized word vectors from the model, to free up memory.\n",
      "     |      \n",
      "     |      You can recompute them later again using the :meth:`~gensim.models.word2vec.Word2Vec.init_sims` method.\n",
      "     |  \n",
      "     |  delete_temporary_training_data(self, replace_word_vectors_with_normalized=False)\n",
      "     |      Discard parameters that are used in training and scoring, to save memory.\n",
      "     |      \n",
      "     |      Warnings\n",
      "     |      --------\n",
      "     |      Use only if you're sure you're done training a model.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      replace_word_vectors_with_normalized : bool, optional\n",
      "     |          If True, forget the original (not normalized) word vectors and only keep\n",
      "     |          the L2-normalized word vectors, to save even more memory.\n",
      "     |  \n",
      "     |  get_latest_training_loss(self)\n",
      "     |      Get current value of the training loss.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      float\n",
      "     |          Current training loss.\n",
      "     |  \n",
      "     |  init_sims(self, replace=False)\n",
      "     |      Deprecated. Use `self.wv.init_sims` instead.\n",
      "     |      See :meth:`~gensim.models.keyedvectors.Word2VecKeyedVectors.init_sims`.\n",
      "     |  \n",
      "     |  intersect_word2vec_format(self, fname, lockf=0.0, binary=False, encoding='utf8', unicode_errors='strict')\n",
      "     |      Merge in an input-hidden weight matrix loaded from the original C word2vec-tool format,\n",
      "     |      where it intersects with the current vocabulary.\n",
      "     |      \n",
      "     |      No words are added to the existing vocabulary, but intersecting words adopt the file's weights, and\n",
      "     |      non-intersecting words are left alone.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      fname : str\n",
      "     |          The file path to load the vectors from.\n",
      "     |      lockf : float, optional\n",
      "     |          Lock-factor value to be set for any imported word-vectors; the\n",
      "     |          default value of 0.0 prevents further updating of the vector during subsequent\n",
      "     |          training. Use 1.0 to allow further training updates of merged vectors.\n",
      "     |      binary : bool, optional\n",
      "     |          If True, `fname` is in the binary word2vec C format.\n",
      "     |      encoding : str, optional\n",
      "     |          Encoding of `text` for `unicode` function (python2 only).\n",
      "     |      unicode_errors : str, optional\n",
      "     |          Error handling behaviour, used as parameter for `unicode` function (python2 only).\n",
      "     |  \n",
      "     |  predict_output_word(self, context_words_list, topn=10)\n",
      "     |      Get the probability distribution of the center word given context words.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      context_words_list : list of str\n",
      "     |          List of context words.\n",
      "     |      topn : int, optional\n",
      "     |          Return `topn` words and their probabilities.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      list of (str, float)\n",
      "     |          `topn` length list of tuples of (word, probability).\n",
      "     |  \n",
      "     |  reset_from(self, other_model)\n",
      "     |      Borrow shareable pre-built structures from `other_model` and reset hidden layer weights.\n",
      "     |      \n",
      "     |      Structures copied are:\n",
      "     |          * Vocabulary\n",
      "     |          * Index to word mapping\n",
      "     |          * Cumulative frequency table (used for negative sampling)\n",
      "     |          * Cached corpus length\n",
      "     |      \n",
      "     |      Useful when testing multiple models on the same corpus in parallel.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      other_model : :class:`~gensim.models.word2vec.Word2Vec`\n",
      "     |          Another model to copy the internal structures from.\n",
      "     |  \n",
      "     |  save(self, *args, **kwargs)\n",
      "     |      Save the model.\n",
      "     |      This saved model can be loaded again using :func:`~gensim.models.word2vec.Word2Vec.load`, which supports\n",
      "     |      online training and getting vectors for vocabulary words.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      fname : str\n",
      "     |          Path to the file.\n",
      "     |  \n",
      "     |  save_word2vec_format(self, fname, fvocab=None, binary=False)\n",
      "     |      Deprecated. Use `model.wv.save_word2vec_format` instead.\n",
      "     |      See :meth:`gensim.models.KeyedVectors.save_word2vec_format`.\n",
      "     |  \n",
      "     |  score(self, sentences, total_sentences=1000000, chunksize=100, queue_factor=2, report_delay=1)\n",
      "     |      Score the log probability for a sequence of sentences.\n",
      "     |      This does not change the fitted model in any way (see :meth:`~gensim.models.word2vec.Word2Vec.train` for that).\n",
      "     |      \n",
      "     |      Gensim has currently only implemented score for the hierarchical softmax scheme,\n",
      "     |      so you need to have run word2vec with `hs=1` and `negative=0` for this to work.\n",
      "     |      \n",
      "     |      Note that you should specify `total_sentences`; you'll run into problems if you ask to\n",
      "     |      score more than this number of sentences but it is inefficient to set the value too high.\n",
      "     |      \n",
      "     |      See the `article by Matt Taddy: \"Document Classification by Inversion of Distributed Language Representations\"\n",
      "     |      <https://arxiv.org/pdf/1504.07295.pdf>`_ and the\n",
      "     |      `gensim demo <https://github.com/piskvorky/gensim/blob/develop/docs/notebooks/deepir.ipynb>`_ for examples of\n",
      "     |      how to use such scores in document classification.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      sentences : iterable of list of str\n",
      "     |          The `sentences` iterable can be simply a list of lists of tokens, but for larger corpora,\n",
      "     |          consider an iterable that streams the sentences directly from disk/network.\n",
      "     |          See :class:`~gensim.models.word2vec.BrownCorpus`, :class:`~gensim.models.word2vec.Text8Corpus`\n",
      "     |          or :class:`~gensim.models.word2vec.LineSentence` in :mod:`~gensim.models.word2vec` module for such examples.\n",
      "     |      total_sentences : int, optional\n",
      "     |          Count of sentences.\n",
      "     |      chunksize : int, optional\n",
      "     |          Chunksize of jobs\n",
      "     |      queue_factor : int, optional\n",
      "     |          Multiplier for size of queue (number of workers * queue_factor).\n",
      "     |      report_delay : float, optional\n",
      "     |          Seconds to wait before reporting progress.\n",
      "     |  \n",
      "     |  train(self, sentences=None, corpus_file=None, total_examples=None, total_words=None, epochs=None, start_alpha=None, end_alpha=None, word_count=0, queue_factor=2, report_delay=1.0, compute_loss=False, callbacks=())\n",
      "     |      Update the model's neural weights from a sequence of sentences.\n",
      "     |      \n",
      "     |      Notes\n",
      "     |      -----\n",
      "     |      To support linear learning-rate decay from (initial) `alpha` to `min_alpha`, and accurate\n",
      "     |      progress-percentage logging, either `total_examples` (count of sentences) or `total_words` (count of\n",
      "     |      raw words in sentences) **MUST** be provided. If `sentences` is the same corpus\n",
      "     |      that was provided to :meth:`~gensim.models.word2vec.Word2Vec.build_vocab` earlier,\n",
      "     |      you can simply use `total_examples=self.corpus_count`.\n",
      "     |      \n",
      "     |      Warnings\n",
      "     |      --------\n",
      "     |      To avoid common mistakes around the model's ability to do multiple training passes itself, an\n",
      "     |      explicit `epochs` argument **MUST** be provided. In the common and recommended case\n",
      "     |      where :meth:`~gensim.models.word2vec.Word2Vec.train` is only called once, you can set `epochs=self.iter`.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      sentences : iterable of list of str\n",
      "     |          The `sentences` iterable can be simply a list of lists of tokens, but for larger corpora,\n",
      "     |          consider an iterable that streams the sentences directly from disk/network.\n",
      "     |          See :class:`~gensim.models.word2vec.BrownCorpus`, :class:`~gensim.models.word2vec.Text8Corpus`\n",
      "     |          or :class:`~gensim.models.word2vec.LineSentence` in :mod:`~gensim.models.word2vec` module for such examples.\n",
      "     |          See also the `tutorial on data streaming in Python\n",
      "     |          <https://rare-technologies.com/data-streaming-in-python-generators-iterators-iterables/>`_.\n",
      "     |      corpus_file : str, optional\n",
      "     |          Path to a corpus file in :class:`~gensim.models.word2vec.LineSentence` format.\n",
      "     |          You may use this argument instead of `sentences` to get performance boost. Only one of `sentences` or\n",
      "     |          `corpus_file` arguments need to be passed (not both of them).\n",
      "     |      total_examples : int\n",
      "     |          Count of sentences.\n",
      "     |      total_words : int\n",
      "     |          Count of raw words in sentences.\n",
      "     |      epochs : int\n",
      "     |          Number of iterations (epochs) over the corpus.\n",
      "     |      start_alpha : float, optional\n",
      "     |          Initial learning rate. If supplied, replaces the starting `alpha` from the constructor,\n",
      "     |          for this one call to`train()`.\n",
      "     |          Use only if making multiple calls to `train()`, when you want to manage the alpha learning-rate yourself\n",
      "     |          (not recommended).\n",
      "     |      end_alpha : float, optional\n",
      "     |          Final learning rate. Drops linearly from `start_alpha`.\n",
      "     |          If supplied, this replaces the final `min_alpha` from the constructor, for this one call to `train()`.\n",
      "     |          Use only if making multiple calls to `train()`, when you want to manage the alpha learning-rate yourself\n",
      "     |          (not recommended).\n",
      "     |      word_count : int, optional\n",
      "     |          Count of words already trained. Set this to 0 for the usual\n",
      "     |          case of training on all words in sentences.\n",
      "     |      queue_factor : int, optional\n",
      "     |          Multiplier for size of queue (number of workers * queue_factor).\n",
      "     |      report_delay : float, optional\n",
      "     |          Seconds to wait before reporting progress.\n",
      "     |      compute_loss: bool, optional\n",
      "     |          If True, computes and stores loss value which can be retrieved using\n",
      "     |          :meth:`~gensim.models.word2vec.Word2Vec.get_latest_training_loss`.\n",
      "     |      callbacks : iterable of :class:`~gensim.models.callbacks.CallbackAny2Vec`, optional\n",
      "     |          Sequence of callbacks to be executed at specific stages during training.\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      .. sourcecode:: pycon\n",
      "     |      \n",
      "     |          >>> from gensim.models import Word2Vec\n",
      "     |          >>> sentences = [[\"cat\", \"say\", \"meow\"], [\"dog\", \"say\", \"woof\"]]\n",
      "     |          >>>\n",
      "     |          >>> model = Word2Vec(min_count=1)\n",
      "     |          >>> model.build_vocab(sentences)  # prepare the model vocabulary\n",
      "     |          >>> model.train(sentences, total_examples=model.corpus_count, epochs=model.iter)  # train word vectors\n",
      "     |          (1, 30)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods defined here:\n",
      "     |  \n",
      "     |  load(*args, **kwargs) from builtins.type\n",
      "     |      Load a previously saved :class:`~gensim.models.word2vec.Word2Vec` model.\n",
      "     |      \n",
      "     |      See Also\n",
      "     |      --------\n",
      "     |      :meth:`~gensim.models.word2vec.Word2Vec.save`\n",
      "     |          Save model.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      fname : str\n",
      "     |          Path to the saved file.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      :class:`~gensim.models.word2vec.Word2Vec`\n",
      "     |          Loaded model.\n",
      "     |  \n",
      "     |  load_word2vec_format(fname, fvocab=None, binary=False, encoding='utf8', unicode_errors='strict', limit=None, datatype=<class 'numpy.float32'>) from builtins.type\n",
      "     |      Deprecated. Use :meth:`gensim.models.KeyedVectors.load_word2vec_format` instead.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Static methods defined here:\n",
      "     |  \n",
      "     |  log_accuracy(section)\n",
      "     |      Deprecated. Use `self.wv.log_accuracy` instead.\n",
      "     |      See :meth:`~gensim.models.word2vec.Word2VecKeyedVectors.log_accuracy`.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from gensim.models.base_any2vec.BaseWordEmbeddingsModel:\n",
      "     |  \n",
      "     |  build_vocab(self, sentences=None, corpus_file=None, update=False, progress_per=10000, keep_raw_vocab=False, trim_rule=None, **kwargs)\n",
      "     |      Build vocabulary from a sequence of sentences (can be a once-only generator stream).\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      sentences : iterable of list of str\n",
      "     |          Can be simply a list of lists of tokens, but for larger corpora,\n",
      "     |          consider an iterable that streams the sentences directly from disk/network.\n",
      "     |          See :class:`~gensim.models.word2vec.BrownCorpus`, :class:`~gensim.models.word2vec.Text8Corpus`\n",
      "     |          or :class:`~gensim.models.word2vec.LineSentence` module for such examples.\n",
      "     |      corpus_file : str, optional\n",
      "     |          Path to a corpus file in :class:`~gensim.models.word2vec.LineSentence` format.\n",
      "     |          You may use this argument instead of `sentences` to get performance boost. Only one of `sentences` or\n",
      "     |          `corpus_file` arguments need to be passed (not both of them).\n",
      "     |      update : bool\n",
      "     |          If true, the new words in `sentences` will be added to model's vocab.\n",
      "     |      progress_per : int, optional\n",
      "     |          Indicates how many words to process before showing/updating the progress.\n",
      "     |      keep_raw_vocab : bool, optional\n",
      "     |          If False, the raw vocabulary will be deleted after the scaling is done to free up RAM.\n",
      "     |      trim_rule : function, optional\n",
      "     |          Vocabulary trimming rule, specifies whether certain words should remain in the vocabulary,\n",
      "     |          be trimmed away, or handled using the default (discard if word count < min_count).\n",
      "     |          Can be None (min_count will be used, look to :func:`~gensim.utils.keep_vocab_item`),\n",
      "     |          or a callable that accepts parameters (word, count, min_count) and returns either\n",
      "     |          :attr:`gensim.utils.RULE_DISCARD`, :attr:`gensim.utils.RULE_KEEP` or :attr:`gensim.utils.RULE_DEFAULT`.\n",
      "     |          The rule, if given, is only used to prune vocabulary during current method call and is not stored as part\n",
      "     |          of the model.\n",
      "     |      \n",
      "     |          The input parameters are of the following types:\n",
      "     |              * `word` (str) - the word we are examining\n",
      "     |              * `count` (int) - the word's frequency count in the corpus\n",
      "     |              * `min_count` (int) - the minimum count threshold.\n",
      "     |      \n",
      "     |      **kwargs : object\n",
      "     |          Key word arguments propagated to `self.vocabulary.prepare_vocab`\n",
      "     |  \n",
      "     |  build_vocab_from_freq(self, word_freq, keep_raw_vocab=False, corpus_count=None, trim_rule=None, update=False)\n",
      "     |      Build vocabulary from a dictionary of word frequencies.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      word_freq : dict of (str, int)\n",
      "     |          A mapping from a word in the vocabulary to its frequency count.\n",
      "     |      keep_raw_vocab : bool, optional\n",
      "     |          If False, delete the raw vocabulary after the scaling is done to free up RAM.\n",
      "     |      corpus_count : int, optional\n",
      "     |          Even if no corpus is provided, this argument can set corpus_count explicitly.\n",
      "     |      trim_rule : function, optional\n",
      "     |          Vocabulary trimming rule, specifies whether certain words should remain in the vocabulary,\n",
      "     |          be trimmed away, or handled using the default (discard if word count < min_count).\n",
      "     |          Can be None (min_count will be used, look to :func:`~gensim.utils.keep_vocab_item`),\n",
      "     |          or a callable that accepts parameters (word, count, min_count) and returns either\n",
      "     |          :attr:`gensim.utils.RULE_DISCARD`, :attr:`gensim.utils.RULE_KEEP` or :attr:`gensim.utils.RULE_DEFAULT`.\n",
      "     |          The rule, if given, is only used to prune vocabulary during current method call and is not stored as part\n",
      "     |          of the model.\n",
      "     |      \n",
      "     |          The input parameters are of the following types:\n",
      "     |              * `word` (str) - the word we are examining\n",
      "     |              * `count` (int) - the word's frequency count in the corpus\n",
      "     |              * `min_count` (int) - the minimum count threshold.\n",
      "     |      \n",
      "     |      update : bool, optional\n",
      "     |          If true, the new provided words in `word_freq` dict will be added to model's vocab.\n",
      "     |  \n",
      "     |  doesnt_match(self, words)\n",
      "     |      Deprecated, use self.wv.doesnt_match() instead.\n",
      "     |      \n",
      "     |      Refer to the documentation for :meth:`~gensim.models.keyedvectors.WordEmbeddingsKeyedVectors.doesnt_match`.\n",
      "     |  \n",
      "     |  estimate_memory(self, vocab_size=None, report=None)\n",
      "     |      Estimate required memory for a model using current settings and provided vocabulary size.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      vocab_size : int, optional\n",
      "     |          Number of unique tokens in the vocabulary\n",
      "     |      report : dict of (str, int), optional\n",
      "     |          A dictionary from string representations of the model's memory consuming members to their size in bytes.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      dict of (str, int)\n",
      "     |          A dictionary from string representations of the model's memory consuming members to their size in bytes.\n",
      "     |  \n",
      "     |  evaluate_word_pairs(self, pairs, delimiter='\\t', restrict_vocab=300000, case_insensitive=True, dummy4unknown=False)\n",
      "     |      Deprecated, use self.wv.evaluate_word_pairs() instead.\n",
      "     |      \n",
      "     |      Refer to the documentation for\n",
      "     |      :meth:`~gensim.models.keyedvectors.WordEmbeddingsKeyedVectors.evaluate_word_pairs`.\n",
      "     |  \n",
      "     |  most_similar(self, positive=None, negative=None, topn=10, restrict_vocab=None, indexer=None)\n",
      "     |      Deprecated, use self.wv.most_similar() instead.\n",
      "     |      \n",
      "     |      Refer to the documentation for :meth:`~gensim.models.keyedvectors.WordEmbeddingsKeyedVectors.most_similar`.\n",
      "     |  \n",
      "     |  most_similar_cosmul(self, positive=None, negative=None, topn=10)\n",
      "     |      Deprecated, use self.wv.most_similar_cosmul() instead.\n",
      "     |      \n",
      "     |      Refer to the documentation for\n",
      "     |      :meth:`~gensim.models.keyedvectors.WordEmbeddingsKeyedVectors.most_similar_cosmul`.\n",
      "     |  \n",
      "     |  n_similarity(self, ws1, ws2)\n",
      "     |      Deprecated, use self.wv.n_similarity() instead.\n",
      "     |      \n",
      "     |      Refer to the documentation for :meth:`~gensim.models.keyedvectors.WordEmbeddingsKeyedVectors.n_similarity`.\n",
      "     |  \n",
      "     |  similar_by_vector(self, vector, topn=10, restrict_vocab=None)\n",
      "     |      Deprecated, use self.wv.similar_by_vector() instead.\n",
      "     |      \n",
      "     |      Refer to the documentation for :meth:`~gensim.models.keyedvectors.WordEmbeddingsKeyedVectors.similar_by_vector`.\n",
      "     |  \n",
      "     |  similar_by_word(self, word, topn=10, restrict_vocab=None)\n",
      "     |      Deprecated, use self.wv.similar_by_word() instead.\n",
      "     |      \n",
      "     |      Refer to the documentation for :meth:`~gensim.models.keyedvectors.WordEmbeddingsKeyedVectors.similar_by_word`.\n",
      "     |  \n",
      "     |  similarity(self, w1, w2)\n",
      "     |      Deprecated, use self.wv.similarity() instead.\n",
      "     |      \n",
      "     |      Refer to the documentation for :meth:`~gensim.models.keyedvectors.WordEmbeddingsKeyedVectors.similarity`.\n",
      "     |  \n",
      "     |  wmdistance(self, document1, document2)\n",
      "     |      Deprecated, use self.wv.wmdistance() instead.\n",
      "     |      \n",
      "     |      Refer to the documentation for :meth:`~gensim.models.keyedvectors.WordEmbeddingsKeyedVectors.wmdistance`.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from gensim.models.base_any2vec.BaseWordEmbeddingsModel:\n",
      "     |  \n",
      "     |  cum_table\n",
      "     |  \n",
      "     |  hashfxn\n",
      "     |  \n",
      "     |  iter\n",
      "     |  \n",
      "     |  layer1_size\n",
      "     |  \n",
      "     |  min_count\n",
      "     |  \n",
      "     |  sample\n",
      "     |  \n",
      "     |  syn0_lockf\n",
      "     |  \n",
      "     |  syn1\n",
      "     |  \n",
      "     |  syn1neg\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from gensim.utils.SaveLoad:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "    \n",
      "    class Word2VecTrainables(gensim.utils.SaveLoad)\n",
      "     |  Word2VecTrainables(vector_size=100, seed=1, hashfxn=<built-in function hash>)\n",
      "     |  \n",
      "     |  Represents the inner shallow neural network used to train :class:`~gensim.models.word2vec.Word2Vec`.\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      Word2VecTrainables\n",
      "     |      gensim.utils.SaveLoad\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, vector_size=100, seed=1, hashfxn=<built-in function hash>)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  prepare_weights(self, hs, negative, wv, update=False, vocabulary=None)\n",
      "     |      Build tables and model weights based on final vocabulary settings.\n",
      "     |  \n",
      "     |  reset_weights(self, hs, negative, wv)\n",
      "     |      Reset all projection weights to an initial (untrained) state, but keep the existing vocabulary.\n",
      "     |  \n",
      "     |  seeded_vector(self, seed_string, vector_size)\n",
      "     |      Get a random vector (but deterministic by seed_string).\n",
      "     |  \n",
      "     |  update_weights(self, hs, negative, wv)\n",
      "     |      Copy all the existing weights, and reset the weights for the newly added vocabulary.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from gensim.utils.SaveLoad:\n",
      "     |  \n",
      "     |  save(self, fname_or_handle, separately=None, sep_limit=10485760, ignore=frozenset(), pickle_protocol=2)\n",
      "     |      Save the object to a file.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      fname_or_handle : str or file-like\n",
      "     |          Path to output file or already opened file-like object. If the object is a file handle,\n",
      "     |          no special array handling will be performed, all attributes will be saved to the same file.\n",
      "     |      separately : list of str or None, optional\n",
      "     |          If None, automatically detect large numpy/scipy.sparse arrays in the object being stored, and store\n",
      "     |          them into separate files. This prevent memory errors for large objects, and also allows\n",
      "     |          `memory-mapping <https://en.wikipedia.org/wiki/Mmap>`_ the large arrays for efficient\n",
      "     |          loading and sharing the large arrays in RAM between multiple processes.\n",
      "     |      \n",
      "     |          If list of str: store these attributes into separate files. The automated size check\n",
      "     |          is not performed in this case.\n",
      "     |      sep_limit : int, optional\n",
      "     |          Don't store arrays smaller than this separately. In bytes.\n",
      "     |      ignore : frozenset of str, optional\n",
      "     |          Attributes that shouldn't be stored at all.\n",
      "     |      pickle_protocol : int, optional\n",
      "     |          Protocol number for pickle.\n",
      "     |      \n",
      "     |      See Also\n",
      "     |      --------\n",
      "     |      :meth:`~gensim.utils.SaveLoad.load`\n",
      "     |          Load object from file.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods inherited from gensim.utils.SaveLoad:\n",
      "     |  \n",
      "     |  load(fname, mmap=None) from builtins.type\n",
      "     |      Load an object previously saved using :meth:`~gensim.utils.SaveLoad.save` from a file.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      fname : str\n",
      "     |          Path to file that contains needed object.\n",
      "     |      mmap : str, optional\n",
      "     |          Memory-map option.  If the object was saved with large arrays stored separately, you can load these arrays\n",
      "     |          via mmap (shared memory) using `mmap='r'.\n",
      "     |          If the file being loaded is compressed (either '.gz' or '.bz2'), then `mmap=None` **must be** set.\n",
      "     |      \n",
      "     |      See Also\n",
      "     |      --------\n",
      "     |      :meth:`~gensim.utils.SaveLoad.save`\n",
      "     |          Save object to file.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      object\n",
      "     |          Object loaded from `fname`.\n",
      "     |      \n",
      "     |      Raises\n",
      "     |      ------\n",
      "     |      AttributeError\n",
      "     |          When called on an object instance instead of class (this is a class method).\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from gensim.utils.SaveLoad:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "    \n",
      "    class Word2VecVocab(gensim.utils.SaveLoad)\n",
      "     |  Word2VecVocab(max_vocab_size=None, min_count=5, sample=0.001, sorted_vocab=True, null_word=0, max_final_vocab=None, ns_exponent=0.75)\n",
      "     |  \n",
      "     |  Vocabulary used by :class:`~gensim.models.word2vec.Word2Vec`.\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      Word2VecVocab\n",
      "     |      gensim.utils.SaveLoad\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, max_vocab_size=None, min_count=5, sample=0.001, sorted_vocab=True, null_word=0, max_final_vocab=None, ns_exponent=0.75)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  add_null_word(self, wv)\n",
      "     |  \n",
      "     |  create_binary_tree(self, wv)\n",
      "     |      Create a `binary Huffman tree <https://en.wikipedia.org/wiki/Huffman_coding>`_ using stored vocabulary\n",
      "     |      word counts. Frequent words will have shorter binary codes.\n",
      "     |      Called internally from :meth:`~gensim.models.word2vec.Word2VecVocab.build_vocab`.\n",
      "     |  \n",
      "     |  make_cum_table(self, wv, domain=2147483647)\n",
      "     |      Create a cumulative-distribution table using stored vocabulary word counts for\n",
      "     |      drawing random words in the negative-sampling training routines.\n",
      "     |      \n",
      "     |      To draw a word index, choose a random integer up to the maximum value in the table (cum_table[-1]),\n",
      "     |      then finding that integer's sorted insertion point (as if by `bisect_left` or `ndarray.searchsorted()`).\n",
      "     |      That insertion point is the drawn index, coming up in proportion equal to the increment at that slot.\n",
      "     |      \n",
      "     |      Called internally from :meth:`~gensim.models.word2vec.Word2VecVocab.build_vocab`.\n",
      "     |  \n",
      "     |  prepare_vocab(self, hs, negative, wv, update=False, keep_raw_vocab=False, trim_rule=None, min_count=None, sample=None, dry_run=False)\n",
      "     |      Apply vocabulary settings for `min_count` (discarding less-frequent words)\n",
      "     |      and `sample` (controlling the downsampling of more-frequent words).\n",
      "     |      \n",
      "     |      Calling with `dry_run=True` will only simulate the provided settings and\n",
      "     |      report the size of the retained vocabulary, effective corpus length, and\n",
      "     |      estimated memory requirements. Results are both printed via logging and\n",
      "     |      returned as a dict.\n",
      "     |      \n",
      "     |      Delete the raw vocabulary after the scaling is done to free up RAM,\n",
      "     |      unless `keep_raw_vocab` is set.\n",
      "     |  \n",
      "     |  scan_vocab(self, sentences=None, corpus_file=None, progress_per=10000, workers=None, trim_rule=None)\n",
      "     |  \n",
      "     |  sort_vocab(self, wv)\n",
      "     |      Sort the vocabulary so the most frequent words have the lowest indexes.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from gensim.utils.SaveLoad:\n",
      "     |  \n",
      "     |  save(self, fname_or_handle, separately=None, sep_limit=10485760, ignore=frozenset(), pickle_protocol=2)\n",
      "     |      Save the object to a file.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      fname_or_handle : str or file-like\n",
      "     |          Path to output file or already opened file-like object. If the object is a file handle,\n",
      "     |          no special array handling will be performed, all attributes will be saved to the same file.\n",
      "     |      separately : list of str or None, optional\n",
      "     |          If None, automatically detect large numpy/scipy.sparse arrays in the object being stored, and store\n",
      "     |          them into separate files. This prevent memory errors for large objects, and also allows\n",
      "     |          `memory-mapping <https://en.wikipedia.org/wiki/Mmap>`_ the large arrays for efficient\n",
      "     |          loading and sharing the large arrays in RAM between multiple processes.\n",
      "     |      \n",
      "     |          If list of str: store these attributes into separate files. The automated size check\n",
      "     |          is not performed in this case.\n",
      "     |      sep_limit : int, optional\n",
      "     |          Don't store arrays smaller than this separately. In bytes.\n",
      "     |      ignore : frozenset of str, optional\n",
      "     |          Attributes that shouldn't be stored at all.\n",
      "     |      pickle_protocol : int, optional\n",
      "     |          Protocol number for pickle.\n",
      "     |      \n",
      "     |      See Also\n",
      "     |      --------\n",
      "     |      :meth:`~gensim.utils.SaveLoad.load`\n",
      "     |          Load object from file.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods inherited from gensim.utils.SaveLoad:\n",
      "     |  \n",
      "     |  load(fname, mmap=None) from builtins.type\n",
      "     |      Load an object previously saved using :meth:`~gensim.utils.SaveLoad.save` from a file.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      fname : str\n",
      "     |          Path to file that contains needed object.\n",
      "     |      mmap : str, optional\n",
      "     |          Memory-map option.  If the object was saved with large arrays stored separately, you can load these arrays\n",
      "     |          via mmap (shared memory) using `mmap='r'.\n",
      "     |          If the file being loaded is compressed (either '.gz' or '.bz2'), then `mmap=None` **must be** set.\n",
      "     |      \n",
      "     |      See Also\n",
      "     |      --------\n",
      "     |      :meth:`~gensim.utils.SaveLoad.save`\n",
      "     |          Save object to file.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      object\n",
      "     |          Object loaded from `fname`.\n",
      "     |      \n",
      "     |      Raises\n",
      "     |      ------\n",
      "     |      AttributeError\n",
      "     |          When called on an object instance instead of class (this is a class method).\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from gensim.utils.SaveLoad:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "\n",
      "FUNCTIONS\n",
      "    array(...)\n",
      "        array(object, dtype=None, copy=True, order='K', subok=False, ndmin=0)\n",
      "        \n",
      "        Create an array.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        object : array_like\n",
      "            An array, any object exposing the array interface, an object whose\n",
      "            __array__ method returns an array, or any (nested) sequence.\n",
      "        dtype : data-type, optional\n",
      "            The desired data-type for the array.  If not given, then the type will\n",
      "            be determined as the minimum type required to hold the objects in the\n",
      "            sequence.\n",
      "        copy : bool, optional\n",
      "            If true (default), then the object is copied.  Otherwise, a copy will\n",
      "            only be made if __array__ returns a copy, if obj is a nested sequence,\n",
      "            or if a copy is needed to satisfy any of the other requirements\n",
      "            (`dtype`, `order`, etc.).\n",
      "        order : {'K', 'A', 'C', 'F'}, optional\n",
      "            Specify the memory layout of the array. If object is not an array, the\n",
      "            newly created array will be in C order (row major) unless 'F' is\n",
      "            specified, in which case it will be in Fortran order (column major).\n",
      "            If object is an array the following holds.\n",
      "        \n",
      "            ===== ========= ===================================================\n",
      "            order  no copy                     copy=True\n",
      "            ===== ========= ===================================================\n",
      "            'K'   unchanged F & C order preserved, otherwise most similar order\n",
      "            'A'   unchanged F order if input is F and not C, otherwise C order\n",
      "            'C'   C order   C order\n",
      "            'F'   F order   F order\n",
      "            ===== ========= ===================================================\n",
      "        \n",
      "            When ``copy=False`` and a copy is made for other reasons, the result is\n",
      "            the same as if ``copy=True``, with some exceptions for `A`, see the\n",
      "            Notes section. The default order is 'K'.\n",
      "        subok : bool, optional\n",
      "            If True, then sub-classes will be passed-through, otherwise\n",
      "            the returned array will be forced to be a base-class array (default).\n",
      "        ndmin : int, optional\n",
      "            Specifies the minimum number of dimensions that the resulting\n",
      "            array should have.  Ones will be pre-pended to the shape as\n",
      "            needed to meet this requirement.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        out : ndarray\n",
      "            An array object satisfying the specified requirements.\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        empty_like : Return an empty array with shape and type of input.\n",
      "        ones_like : Return an array of ones with shape and type of input.\n",
      "        zeros_like : Return an array of zeros with shape and type of input.\n",
      "        full_like : Return a new array with shape of input filled with value.\n",
      "        empty : Return a new uninitialized array.\n",
      "        ones : Return a new array setting values to one.\n",
      "        zeros : Return a new array setting values to zero.\n",
      "        full : Return a new array of given shape filled with value.\n",
      "        \n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        When order is 'A' and `object` is an array in neither 'C' nor 'F' order,\n",
      "        and a copy is forced by a change in dtype, then the order of the result is\n",
      "        not necessarily 'C' as expected. This is likely a bug.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> np.array([1, 2, 3])\n",
      "        array([1, 2, 3])\n",
      "        \n",
      "        Upcasting:\n",
      "        \n",
      "        >>> np.array([1, 2, 3.0])\n",
      "        array([ 1.,  2.,  3.])\n",
      "        \n",
      "        More than one dimension:\n",
      "        \n",
      "        >>> np.array([[1, 2], [3, 4]])\n",
      "        array([[1, 2],\n",
      "               [3, 4]])\n",
      "        \n",
      "        Minimum dimensions 2:\n",
      "        \n",
      "        >>> np.array([1, 2, 3], ndmin=2)\n",
      "        array([[1, 2, 3]])\n",
      "        \n",
      "        Type provided:\n",
      "        \n",
      "        >>> np.array([1, 2, 3], dtype=complex)\n",
      "        array([ 1.+0.j,  2.+0.j,  3.+0.j])\n",
      "        \n",
      "        Data-type consisting of more than one element:\n",
      "        \n",
      "        >>> x = np.array([(1,2),(3,4)],dtype=[('a','<i4'),('b','<i4')])\n",
      "        >>> x['a']\n",
      "        array([1, 3])\n",
      "        \n",
      "        Creating an array from sub-classes:\n",
      "        \n",
      "        >>> np.array(np.mat('1 2; 3 4'))\n",
      "        array([[1, 2],\n",
      "               [3, 4]])\n",
      "        \n",
      "        >>> np.array(np.mat('1 2; 3 4'), subok=True)\n",
      "        matrix([[1, 2],\n",
      "                [3, 4]])\n",
      "    \n",
      "    default_timer = perf_counter(...)\n",
      "        perf_counter() -> float\n",
      "        \n",
      "        Performance counter for benchmarking.\n",
      "    \n",
      "    empty(...)\n",
      "        empty(shape, dtype=float, order='C')\n",
      "        \n",
      "        Return a new array of given shape and type, without initializing entries.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        shape : int or tuple of int\n",
      "            Shape of the empty array, e.g., ``(2, 3)`` or ``2``.\n",
      "        dtype : data-type, optional\n",
      "            Desired output data-type for the array, e.g, `numpy.int8`. Default is\n",
      "            `numpy.float64`.\n",
      "        order : {'C', 'F'}, optional, default: 'C'\n",
      "            Whether to store multi-dimensional data in row-major\n",
      "            (C-style) or column-major (Fortran-style) order in\n",
      "            memory.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        out : ndarray\n",
      "            Array of uninitialized (arbitrary) data of the given shape, dtype, and\n",
      "            order.  Object arrays will be initialized to None.\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        empty_like : Return an empty array with shape and type of input.\n",
      "        ones : Return a new array setting values to one.\n",
      "        zeros : Return a new array setting values to zero.\n",
      "        full : Return a new array of given shape filled with value.\n",
      "        \n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        `empty`, unlike `zeros`, does not set the array values to zero,\n",
      "        and may therefore be marginally faster.  On the other hand, it requires\n",
      "        the user to manually set all the values in the array, and should be\n",
      "        used with caution.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> np.empty([2, 2])\n",
      "        array([[ -9.74499359e+001,   6.69583040e-309],\n",
      "               [  2.13182611e-314,   3.06959433e-309]])         #uninitialized\n",
      "        \n",
      "        >>> np.empty([2, 2], dtype=int)\n",
      "        array([[-1073741821, -1067949133],\n",
      "               [  496041986,    19249760]])                     #uninitialized\n",
      "    \n",
      "    fromstring(...)\n",
      "        fromstring(string, dtype=float, count=-1, sep='')\n",
      "        \n",
      "        A new 1-D array initialized from text data in a string.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        string : str\n",
      "            A string containing the data.\n",
      "        dtype : data-type, optional\n",
      "            The data type of the array; default: float.  For binary input data,\n",
      "            the data must be in exactly this format. Most builtin numeric types are \n",
      "            supported and extension types may be supported.\n",
      "        \n",
      "            .. versionadded:: 1.18.0\n",
      "                Complex dtypes.\n",
      "        \n",
      "        count : int, optional\n",
      "            Read this number of `dtype` elements from the data.  If this is\n",
      "            negative (the default), the count will be determined from the\n",
      "            length of the data.\n",
      "        sep : str, optional\n",
      "            The string separating numbers in the data; extra whitespace between\n",
      "            elements is also ignored.\n",
      "        \n",
      "            .. deprecated:: 1.14\n",
      "                Passing ``sep=''``, the default, is deprecated since it will\n",
      "                trigger the deprecated binary mode of this function. This mode\n",
      "                interprets `string` as binary bytes, rather than ASCII text with\n",
      "                decimal numbers, an operation which is better spelt\n",
      "                ``frombuffer(string, dtype, count)``. If `string` contains unicode\n",
      "                text, the binary mode of `fromstring` will first encode it into\n",
      "                bytes using either utf-8 (python 3) or the default encoding\n",
      "                (python 2), neither of which produce sane results.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        arr : ndarray\n",
      "            The constructed array.\n",
      "        \n",
      "        Raises\n",
      "        ------\n",
      "        ValueError\n",
      "            If the string is not the correct size to satisfy the requested\n",
      "            `dtype` and `count`.\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        frombuffer, fromfile, fromiter\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> np.fromstring('1 2', dtype=int, sep=' ')\n",
      "        array([1, 2])\n",
      "        >>> np.fromstring('1, 2', dtype=int, sep=',')\n",
      "        array([1, 2])\n",
      "    \n",
      "    score_cbow_pair(model, word, l1)\n",
      "        Score the trained CBOW model on a pair of words.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        model : :class:`~gensim.models.word2vec.Word2Vec`\n",
      "            The trained model.\n",
      "        word : :class:`~gensim.models.keyedvectors.Vocab`\n",
      "            Vocabulary representation of the first word.\n",
      "        l1 : list of float\n",
      "            Vector representation of the second word.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        float\n",
      "            Logarithm of the sum of exponentiations of input words.\n",
      "    \n",
      "    score_sentence_cbow(...)\n",
      "        score_sentence_cbow(model, sentence, _work, _neu1)\n",
      "        Obtain likelihood score for a single sentence in a fitted CBOW representation.\n",
      "        \n",
      "            Notes\n",
      "            -----\n",
      "            This scoring function is only implemented for hierarchical softmax (`model.hs == 1`).\n",
      "            The model should have been trained using the skip-gram model (`model.cbow` == 1`).\n",
      "        \n",
      "            Parameters\n",
      "            ----------\n",
      "            model : :class:`~gensim.models.word2vec.Word2Vec`\n",
      "                The trained model. It **MUST** have been trained using hierarchical softmax and the CBOW algorithm.\n",
      "            sentence : list of str\n",
      "                The words comprising the sentence to be scored.\n",
      "            _work : np.ndarray\n",
      "                Private working memory for each worker.\n",
      "            _neu1 : np.ndarray\n",
      "                Private working memory for each worker.\n",
      "        \n",
      "            Returns\n",
      "            -------\n",
      "            float\n",
      "                The probability assigned to this sentence by the Skip-Gram model.\n",
      "    \n",
      "    score_sentence_sg(...)\n",
      "        score_sentence_sg(model, sentence, _work)\n",
      "        Obtain likelihood score for a single sentence in a fitted skip-gram representation.\n",
      "        \n",
      "            Notes\n",
      "            -----\n",
      "            This scoring function is only implemented for hierarchical softmax (`model.hs == 1`).\n",
      "            The model should have been trained using the skip-gram model (`model.sg` == 1`).\n",
      "        \n",
      "            Parameters\n",
      "            ----------\n",
      "            model : :class:`~gensim.models.word2vec.Word2Vec`\n",
      "                The trained model. It **MUST** have been trained using hierarchical softmax and the skip-gram algorithm.\n",
      "            sentence : list of str\n",
      "                The words comprising the sentence to be scored.\n",
      "            _work : np.ndarray\n",
      "                Private working memory for each worker.\n",
      "        \n",
      "            Returns\n",
      "            -------\n",
      "            float\n",
      "                The probability assigned to this sentence by the Skip-Gram model.\n",
      "    \n",
      "    score_sg_pair(model, word, word2)\n",
      "        Score the trained Skip-gram model on a pair of words.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        model : :class:`~gensim.models.word2vec.Word2Vec`\n",
      "            The trained model.\n",
      "        word : :class:`~gensim.models.keyedvectors.Vocab`\n",
      "            Vocabulary representation of the first word.\n",
      "        word2 : :class:`~gensim.models.keyedvectors.Vocab`\n",
      "            Vocabulary representation of the second word.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        float\n",
      "            Logarithm of the sum of exponentiations of input words.\n",
      "    \n",
      "    train_batch_cbow(...)\n",
      "        train_batch_cbow(model, sentences, alpha, _work, _neu1, compute_loss)\n",
      "        Update CBOW model by training on a batch of sentences.\n",
      "        \n",
      "            Called internally from :meth:`~gensim.models.word2vec.Word2Vec.train`.\n",
      "        \n",
      "            Parameters\n",
      "            ----------\n",
      "            model : :class:`~gensim.models.word2vec.Word2Vec`\n",
      "                The Word2Vec model instance to train.\n",
      "            sentences : iterable of list of str\n",
      "                The corpus used to train the model.\n",
      "            alpha : float\n",
      "                The learning rate.\n",
      "            _work : np.ndarray\n",
      "                Private working memory for each worker.\n",
      "            _neu1 : np.ndarray\n",
      "                Private working memory for each worker.\n",
      "            compute_loss : bool\n",
      "                Whether or not the training loss should be computed in this batch.\n",
      "        \n",
      "            Returns\n",
      "            -------\n",
      "            int\n",
      "                Number of words in the vocabulary actually used for training (They already existed in the vocabulary\n",
      "                and were not discarded by negative sampling).\n",
      "    \n",
      "    train_batch_sg(...)\n",
      "        train_batch_sg(model, sentences, alpha, _work, compute_loss)\n",
      "        Update skip-gram model by training on a batch of sentences.\n",
      "        \n",
      "            Called internally from :meth:`~gensim.models.word2vec.Word2Vec.train`.\n",
      "        \n",
      "            Parameters\n",
      "            ----------\n",
      "            model : :class:`~gensim.models.word2Vec.Word2Vec`\n",
      "                The Word2Vec model instance to train.\n",
      "            sentences : iterable of list of str\n",
      "                The corpus used to train the model.\n",
      "            alpha : float\n",
      "                The learning rate\n",
      "            _work : np.ndarray\n",
      "                Private working memory for each worker.\n",
      "            compute_loss : bool\n",
      "                Whether or not the training loss should be computed in this batch.\n",
      "        \n",
      "            Returns\n",
      "            -------\n",
      "            int\n",
      "                Number of words in the vocabulary actually used for training (They already existed in the vocabulary\n",
      "                and were not discarded by negative sampling).\n",
      "    \n",
      "    train_cbow_pair(model, word, input_word_indices, l1, alpha, learn_vectors=True, learn_hidden=True, compute_loss=False, context_vectors=None, context_locks=None, is_ft=False)\n",
      "        Train the passed model instance on a word and its context, using the CBOW algorithm.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        model : :class:`~gensim.models.word2vec.Word2Vec`\n",
      "            The model to be trained.\n",
      "        word : str\n",
      "            The label (predicted) word.\n",
      "        input_word_indices : list of int\n",
      "            The vocabulary indices of the words in the context.\n",
      "        l1 : list of float\n",
      "            Vector representation of the label word.\n",
      "        alpha : float\n",
      "            Learning rate.\n",
      "        learn_vectors : bool, optional\n",
      "            Whether the vectors should be updated.\n",
      "        learn_hidden : bool, optional\n",
      "            Whether the weights of the hidden layer should be updated.\n",
      "        compute_loss : bool, optional\n",
      "            Whether or not the training loss should be computed.\n",
      "        context_vectors : list of list of float, optional\n",
      "            Vector representations of the words in the context. If None, these will be retrieved from the model.\n",
      "        context_locks : list of float, optional\n",
      "            The lock factors for each word in the context.\n",
      "        is_ft : bool, optional\n",
      "            If True, weights will be computed using `model.wv.syn0_vocab` and `model.wv.syn0_ngrams`\n",
      "            instead of `model.wv.syn0`.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        numpy.ndarray\n",
      "            Error vector to be back-propagated.\n",
      "    \n",
      "    train_epoch_cbow(...)\n",
      "        train_epoch_cbow(model, corpus_file, offset, _cython_vocab, _cur_epoch, _expected_examples, _expected_words, _work, _neu1, compute_loss)\n",
      "        Train CBOW model for one epoch by training on an input stream. This function is used only in multistream mode.\n",
      "        \n",
      "            Called internally from :meth:`~gensim.models.word2vec.Word2Vec.train`.\n",
      "        \n",
      "            Parameters\n",
      "            ----------\n",
      "            model : :class:`~gensim.models.word2vec.Word2Vec`\n",
      "                The Word2Vec model instance to train.\n",
      "            corpus_file : str\n",
      "                Path to corpus file.\n",
      "            _cur_epoch : int\n",
      "                Current epoch number. Used for calculating and decaying learning rate.\n",
      "            _work : np.ndarray\n",
      "                Private working memory for each worker.\n",
      "            _neu1 : np.ndarray\n",
      "                Private working memory for each worker.\n",
      "            compute_loss : bool\n",
      "                Whether or not the training loss should be computed in this batch.\n",
      "        \n",
      "            Returns\n",
      "            -------\n",
      "            int\n",
      "                Number of words in the vocabulary actually used for training (They already existed in the vocabulary\n",
      "                and were not discarded by negative sampling).\n",
      "    \n",
      "    train_epoch_sg(...)\n",
      "        train_epoch_sg(model, corpus_file, offset, _cython_vocab, _cur_epoch, _expected_examples, _expected_words, _work, _neu1, compute_loss)\n",
      "        Train Skipgram model for one epoch by training on an input stream. This function is used only in multistream mode.\n",
      "        \n",
      "            Called internally from :meth:`~gensim.models.word2vec.Word2Vec.train`.\n",
      "        \n",
      "            Parameters\n",
      "            ----------\n",
      "            model : :class:`~gensim.models.word2vec.Word2Vec`\n",
      "                The Word2Vec model instance to train.\n",
      "            corpus_file : str\n",
      "                Path to corpus file.\n",
      "            _cur_epoch : int\n",
      "                Current epoch number. Used for calculating and decaying learning rate.\n",
      "            _work : np.ndarray\n",
      "                Private working memory for each worker.\n",
      "            _neu1 : np.ndarray\n",
      "                Private working memory for each worker.\n",
      "            compute_loss : bool\n",
      "                Whether or not the training loss should be computed in this batch.\n",
      "        \n",
      "            Returns\n",
      "            -------\n",
      "            int\n",
      "                Number of words in the vocabulary actually used for training (They already existed in the vocabulary\n",
      "                and were not discarded by negative sampling).\n",
      "    \n",
      "    train_sg_pair(model, word, context_index, alpha, learn_vectors=True, learn_hidden=True, context_vectors=None, context_locks=None, compute_loss=False, is_ft=False)\n",
      "        Train the passed model instance on a word and its context, using the Skip-gram algorithm.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        model : :class:`~gensim.models.word2vec.Word2Vec`\n",
      "            The model to be trained.\n",
      "        word : str\n",
      "            The label (predicted) word.\n",
      "        context_index : list of int\n",
      "            The vocabulary indices of the words in the context.\n",
      "        alpha : float\n",
      "            Learning rate.\n",
      "        learn_vectors : bool, optional\n",
      "            Whether the vectors should be updated.\n",
      "        learn_hidden : bool, optional\n",
      "            Whether the weights of the hidden layer should be updated.\n",
      "        context_vectors : list of list of float, optional\n",
      "            Vector representations of the words in the context. If None, these will be retrieved from the model.\n",
      "        context_locks : list of float, optional\n",
      "            The lock factors for each word in the context.\n",
      "        compute_loss : bool, optional\n",
      "            Whether or not the training loss should be computed.\n",
      "        is_ft : bool, optional\n",
      "            If True, weights will be computed using `model.wv.syn0_vocab` and `model.wv.syn0_ngrams`\n",
      "            instead of `model.wv.syn0`.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        numpy.ndarray\n",
      "            Error vector to be back-propagated.\n",
      "    \n",
      "    zeros(...)\n",
      "        zeros(shape, dtype=float, order='C')\n",
      "        \n",
      "        Return a new array of given shape and type, filled with zeros.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        shape : int or tuple of ints\n",
      "            Shape of the new array, e.g., ``(2, 3)`` or ``2``.\n",
      "        dtype : data-type, optional\n",
      "            The desired data-type for the array, e.g., `numpy.int8`.  Default is\n",
      "            `numpy.float64`.\n",
      "        order : {'C', 'F'}, optional, default: 'C'\n",
      "            Whether to store multi-dimensional data in row-major\n",
      "            (C-style) or column-major (Fortran-style) order in\n",
      "            memory.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        out : ndarray\n",
      "            Array of zeros with the given shape, dtype, and order.\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        zeros_like : Return an array of zeros with shape and type of input.\n",
      "        empty : Return a new uninitialized array.\n",
      "        ones : Return a new array setting values to one.\n",
      "        full : Return a new array of given shape filled with value.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> np.zeros(5)\n",
      "        array([ 0.,  0.,  0.,  0.,  0.])\n",
      "        \n",
      "        >>> np.zeros((5,), dtype=int)\n",
      "        array([0, 0, 0, 0, 0])\n",
      "        \n",
      "        >>> np.zeros((2, 1))\n",
      "        array([[ 0.],\n",
      "               [ 0.]])\n",
      "        \n",
      "        >>> s = (2,2)\n",
      "        >>> np.zeros(s)\n",
      "        array([[ 0.,  0.],\n",
      "               [ 0.,  0.]])\n",
      "        \n",
      "        >>> np.zeros((2,), dtype=[('x', 'i4'), ('y', 'i4')]) # custom dtype\n",
      "        array([(0, 0), (0, 0)],\n",
      "              dtype=[('x', '<i4'), ('y', '<i4')])\n",
      "\n",
      "DATA\n",
      "    CORPUSFILE_VERSION = 1\n",
      "    FAST_VERSION = 1\n",
      "    MAX_WORDS_IN_BATCH = 10000\n",
      "    division = _Feature((2, 2, 0, 'alpha', 2), (3, 0, 0, 'alpha', 0), 8192...\n",
      "    exp = <ufunc 'exp'>\n",
      "    expit = <ufunc 'expit'>\n",
      "    log = <ufunc 'log'>\n",
      "    logaddexp = <ufunc 'logaddexp'>\n",
      "    logger = <Logger gensim.models.word2vec (WARNING)>\n",
      "    sqrt = <ufunc 'sqrt'>\n",
      "    string_types = (<class 'str'>,)\n",
      "\n",
      "FILE\n",
      "    /opt/anaconda3/lib/python3.7/site-packages/gensim/models/word2vec.py\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(word2vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def cosdis(v1, v2):\n",
    "    # which characters are common to the two words?\n",
    "    common = v1[1].intersection(v2[1])\n",
    "    # by definition of cosine distance we have\n",
    "    return sum(v1[0][ch]*v2[0][ch] for ch in common)/v1[2]/v2[2]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
