{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "f=open('goldmedals.txt', 'r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections as cls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Paavo Nurmi', 9),\n",
       " ('Carl Lewis', 9),\n",
       " ('Usain Bolt', 9),\n",
       " ('Ray Ewry', 8),\n",
       " ('Allyson Felix', 6)]"
      ]
     },
     "execution_count": 217,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "medal = cls.namedtuple('medal', ['year', 'athelete', 'nation', 'event'])\n",
    "medals = [medal(*line.split(\"\\t\")) for line in f]\n",
    "c = cls.Counter(medal.athelete for medal in medals)\n",
    "c.most_common(5)\n",
    "\n",
    "# atheletes with most number of gold medals in different events\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# bruteforce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Paavo Nurmi',\n",
       "  {'10000m',\n",
       "   '1500m',\n",
       "   '3000m team',\n",
       "   '5000m',\n",
       "   'cross country individual',\n",
       "   'cross country team'}),\n",
       " ('Ville Ritola',\n",
       "  {'10000m',\n",
       "   '3000m steeplechase',\n",
       "   '3000m team',\n",
       "   '5000m',\n",
       "   'cross country team'}),\n",
       " ('Alvin Kraenzlein', {'110m hurdles', '200m hurdles', '60m', 'long jump'}),\n",
       " ('Hannes Kolehmainen',\n",
       "  {'10000m', '5000m', 'cross country individual', 'marathon'}),\n",
       " ('Jesse Owens', {'100m', '200m', '4x100m relay', 'long jump'}),\n",
       " ('Fanny Blankers-Koen', {'100m', '200m', '4x100m relay', '80m hurdles'}),\n",
       " ('Betty Cuthbert', {'100m', '200m', '400m', '4x100m relay'}),\n",
       " ('Carl Lewis', {'100m', '200m', '4x100m relay', 'long jump'}),\n",
       " ('Ray Ewry',\n",
       "  {'high jump, standing', 'long jump, standing', 'triple jump, standing'}),\n",
       " ('Archie Hahn', {'100m', '200m', '60m'}),\n",
       " ('James Lightbody', {'1500m', '3000m steeplechase', '800m'}),\n",
       " ('Harry Hillman', {'200m hurdles', '400m', '400m hurdles'}),\n",
       " ('Melvin Sheppard', {'1500m', '4x400m relay', '800m'}),\n",
       " ('Emil Zatopek', {'10000m', '5000m', 'marathon'}),\n",
       " ('Harrison Dillard', {'100m', '110m hurdles', '4x100m relay'}),\n",
       " ('Robert Joseph Morrow', {'100m', '200m', '4x100m relay'}),\n",
       " ('Wilma Rudolph', {'100m', '200m', '4x100m relay'}),\n",
       " ('Irena Szewinska', {'200m', '400m', '4x100m relay'}),\n",
       " ('Renate Stecher', {'100m', '200m', '4x100m relay'}),\n",
       " ('Valerie Ann Brisco-Hooks', {'200m', '400m', '4x400m relay'}),\n",
       " ('Florence Griffith-Joyner', {'100m', '200m', '4x100m relay'}),\n",
       " ('Michael Johnson', {'200m', '400m', '4x400m relay'}),\n",
       " ('Tirunesh Dibaba', {'10000m', '10miles walk (16093m)', '5000m'}),\n",
       " ('Usain Bolt', {'100m', '200m', '4x100m relay'}),\n",
       " ('Allyson Felix', {'200m', '4x100m relay', '4x400m relay'}),\n",
       " ('Thomas Burke', {'100m', '400m'}),\n",
       " ('Edwin Flack', {'1500m', '800m'}),\n",
       " ('Robert Garrett', {'discus throw', 'shot put'}),\n",
       " ('Ellery Clark', {'high jump', 'long jump'}),\n",
       " ('Charles Bennett', {'1500m', '5000m team'}),\n",
       " ('Walter B. John Tewksbury', {'200m', '400m hurdles'}),\n",
       " ('John Rimmer', {'4000m steeplechase', '5000m team'}),\n",
       " ('Alfred Tysoe', {'5000m team', '800m'}),\n",
       " ('Irving Baxter', {'high jump', 'pole vault'}),\n",
       " ('Meyer Prinstein', {'long jump', 'triple jump'}),\n",
       " ('Martin Sheridan', {'discus throw', 'discus throw ancient style'}),\n",
       " ('Ralph Rose', {'shot put', 'shot put, both hands'}),\n",
       " ('George Edward Larner', {'10miles walk (16093m)', '3500m walk'}),\n",
       " ('Eric Lemming', {'javelin throw', 'javelin throw freestyle'}),\n",
       " ('Ralph Craig', {'100m', '200m'}),\n",
       " ('Charles Decker Reidpath', {'400m', '4x400m relay'}),\n",
       " ('James Edwin Meredith', {'4x400m relay', '800m'}),\n",
       " ('Jim Thorpe', {'decathlon', 'pentathlon'}),\n",
       " ('Armas Rudolf Taipale', {'discus throw', 'discus throw, both hands'}),\n",
       " ('Patrick Mcdonald', {'56lb weight throw (25.4kg)', 'shot put'}),\n",
       " ('Ugo Frigerio', {'10000m walk', '3000m walk'}),\n",
       " ('Charles Paddock', {'100m', '4x100m relay'}),\n",
       " ('Albert Hill', {'1500m', '800m'}),\n",
       " ('Jackson Scholz', {'200m', '4x100m relay'}),\n",
       " ('Harold Marion Osborn', {'decathlon', 'high jump'}),\n",
       " ('Lemuel Clarence Houser', {'discus throw', 'shot put'}),\n",
       " ('Percy Williams', {'100m', '200m'}),\n",
       " ('Elizabeth Robinson', {'100m', '4x100m relay'}),\n",
       " ('Raymond James Barbuti', {'400m', '4x400m relay'}),\n",
       " ('Eddie Tolan', {'100m', '200m'}),\n",
       " ('William Arthur Carr', {'400m', '4x400m relay'}),\n",
       " ('Mildred Didrikson', {'80m hurdles', 'javelin throw'}),\n",
       " ('Helen Herring Stephens', {'100m', '4x100m relay'}),\n",
       " ('Melvin Emery Patton', {'200m', '4x100m relay'}),\n",
       " ('Arthur Wint', {'400m', '4x400m relay'}),\n",
       " ('Roy Braxton Cochran', {'400m hurdles', '4x400m relay'}),\n",
       " ('Malvin Groston Whitfield', {'4x400m relay', '800m'}),\n",
       " ('Micheline Ostermeyer', {'discus throw', 'shot put'}),\n",
       " ('Lindy Remigino', {'100m', '4x100m relay'}),\n",
       " ('Marjorie Jackson', {'100m', '200m'}),\n",
       " ('Andrew William Stanfield', {'200m', '4x100m relay'}),\n",
       " ('Vincent George Rhoden', {'400m', '4x400m relay'}),\n",
       " ('Shirley Strickland-De La Hunty', {'4x100m relay', '80m hurdles'}),\n",
       " ('Vladimir Kuts', {'10000m', '5000m'}),\n",
       " ('Charles Lamont Jenkins', {'400m', '4x400m relay'}),\n",
       " ('Glenn Ashby Davis', {'400m hurdles', '4x400m relay'}),\n",
       " ('Thomas William Courtney', {'4x400m relay', '800m'}),\n",
       " ('Armin Hary', {'100m', '4x100m relay'}),\n",
       " ('Otis Crandall Davis', {'400m', '4x400m relay'}),\n",
       " ('Peter Snell', {'1500m', '800m'}),\n",
       " ('Iryna Press', {'80m hurdles', 'pentathlon'}),\n",
       " ('Tamara Press', {'discus throw', 'shot put'}),\n",
       " ('Robert Hayes', {'100m', '4x100m relay'}),\n",
       " ('Wyomia Tyus', {'100m', '4x100m relay'}),\n",
       " ('Henry Carr', {'200m', '4x400m relay'}),\n",
       " ('Michael Denny Larrabee', {'400m', '4x400m relay'}),\n",
       " ('James Ray Hines', {'100m', '4x100m relay'}),\n",
       " ('Kipchoge Keino', {'1500m', '3000m steeplechase'}),\n",
       " ('Lee Edward Evans', {'400m', '4x400m relay'}),\n",
       " ('Vincent Edward Matthews', {'400m', '4x400m relay'}),\n",
       " ('Ingrid Becker-Mickler', {'4x100m relay', 'pentathlon'}),\n",
       " ('Lasse Viren', {'10000m', '5000m'}),\n",
       " ('Valery Borzov', {'100m', '200m'}),\n",
       " ('Monika Zehrt', {'400m', '4x400m relay'}),\n",
       " ('Annegret Richter-Irrgang', {'100m', '4x100m relay'}),\n",
       " ('Heidemarie Rosendahl', {'4x100m relay', 'long jump'}),\n",
       " ('Tatiana Kazankina', {'1500m', '800m'}),\n",
       " ('Bärbel Eckert-Wöckel', {'200m', '4x100m relay'}),\n",
       " ('Alberto Juantorena', {'400m', '800m'}),\n",
       " ('Miruts Yifter', {'10000m', '5000m'}),\n",
       " ('Viktor Markin', {'400m', '4x400m relay'}),\n",
       " ('Evelyn Ashford', {'100m', '4x100m relay'}),\n",
       " ('Alonzo C. Babers', {'400m', '4x400m relay'}),\n",
       " ('Chandra Danette Cheeseborough', {'4x100m relay', '4x400m relay'}),\n",
       " ('Steve Lewis', {'400m', '4x400m relay'}),\n",
       " ('Olga Bryzgina', {'400m', '4x400m relay'}),\n",
       " ('Jackie Joyner-Kersee', {'heptathlon', 'long jump'}),\n",
       " ('Gail Devers', {'100m', '4x100m relay'}),\n",
       " ('Michael Marsh', {'200m', '4x100m relay'}),\n",
       " ('Gwen Torrence', {'200m', '4x100m relay'}),\n",
       " ('Quincy Watts', {'400m', '4x400m relay'}),\n",
       " ('Marie-Jose Perec', {'200m', '400m'}),\n",
       " ('Donovan Bailey', {'100m', '4x100m relay'}),\n",
       " ('Svetlana Masterkova', {'1500m', '800m'}),\n",
       " ('Robert Korzeniowski', {'20km walk', '50km walk'}),\n",
       " ('Maurice Greene', {'100m', '4x100m relay'}),\n",
       " ('Pauline Elaine Davis', {'200m', '4x100m relay'}),\n",
       " ('Angelo Taylor', {'400m hurdles', '4x400m relay'}),\n",
       " ('Kenenisa Bekele', {'10000m', '5000m'}),\n",
       " ('Hicham El guerrouj', {'1500m', '5000m'}),\n",
       " ('Kelly Holmes', {'1500m', '800m'}),\n",
       " ('Veronica Campbell-Brown', {'200m', '4x100m relay'}),\n",
       " ('Jeremy Wariner', {'400m', '4x400m relay'}),\n",
       " ('Meseret Defar', {'5000m', '5000m team'}),\n",
       " ('LaShawn Merritt', {'400m', '4x400m relay'}),\n",
       " ('Sanya Richards-Ross', {'400m', '4x400m relay'}),\n",
       " ('Mo Farah', {'10miles walk (16093m)', '5000m team'}),\n",
       " ('Elaine Thompson', {'100m', '200m'}),\n",
       " ('Tianna Bartoletta', {'4x100m relay', 'long jump'}),\n",
       " ('Mohamed Farah', {'10miles walk (16093m)', '5000m team'}),\n",
       " ('Thomas Curtis', {'110m hurdles'}),\n",
       " ('Spyridon Louis', {'marathon'}),\n",
       " ('William Welles Hoyt', {'pole vault'}),\n",
       " ('James Connolly', {'triple jump'}),\n",
       " ('Francis Jarvis', {'100m'}),\n",
       " ('George Orton', {'3000m steeplechase'}),\n",
       " ('Maxwell W.Maxey Long', {'400m'}),\n",
       " ('Sidney Robinson', {'5000m team'}),\n",
       " ('Stanley Rowley', {'5000m team'}),\n",
       " ('Rudolf Bauer', {'discus throw'}),\n",
       " ('John Jesus Flanagan', {'hammer throw'}),\n",
       " ('Michel Theato', {'marathon'}),\n",
       " ('Richard Sheldon', {'shot put'}),\n",
       " ('Frederick Schule', {'110m hurdles'}),\n",
       " ('David Curtis Munson', {'4miles team'}),\n",
       " ('Arthur Newton', {'4miles team'}),\n",
       " ('Paul Harry Pilgrim', {'4miles team'}),\n",
       " ('George Underwood', {'4miles team'}),\n",
       " ('Howard Valentine', {'4miles team'}),\n",
       " ('Etienne Desmarteau', {'56lb weight throw (25.4kg)'}),\n",
       " ('Thomas Francis Kiely', {'decathlon'}),\n",
       " ('Samuel Jones', {'high jump'}),\n",
       " ('Thomas Hicks', {'marathon'}),\n",
       " ('Charles Dvorak', {'pole vault'}),\n",
       " ('Max Emmerich', {'triathlon (long jump / shot put / 100 yards)'}),\n",
       " ('Reginald Walker', {'100m'}),\n",
       " ('Forrest Smithson', {'110m hurdles'}),\n",
       " ('Robert Kerr', {'200m'}),\n",
       " ('Arthur Russell', {'3200m steeplechase'}),\n",
       " ('William Coales', {'3miles team (4828m)'}),\n",
       " ('Joseph Edmund Deakin', {'3miles team (4828m)'}),\n",
       " ('Arthur James Robertson', {'3miles team (4828m)'}),\n",
       " ('Wyndham Halswelle', {'400m'}),\n",
       " ('Charles Joseph Bacon', {'400m hurdles'}),\n",
       " ('John Nathaniel Cartmell', {'4x400m relay'}),\n",
       " ('William Frank Hamilton', {'4x400m relay'}),\n",
       " ('John Baxter Taylor', {'4x400m relay'}),\n",
       " ('Emil Robert Voigt', {'5 miles (8047m)'}),\n",
       " ('Harry Franklin Porter', {'high jump'}),\n",
       " ('Francis C. Irons', {'long jump'}),\n",
       " ('John Joseph Hayes', {'marathon'}),\n",
       " ('Edward Tiffin Cooke', {'pole vault'}),\n",
       " ('Alfred Carleten Gilbert', {'pole vault'}),\n",
       " ('Timoty J. Ahearne', {'triple jump'}),\n",
       " ('George Goulding', {'10000m walk'}),\n",
       " ('Frederick Kelly', {'110m hurdles'}),\n",
       " ('Arnold Jackson', {'1500m'}),\n",
       " ('Tell Schirnding Berna', {'3000m team'}),\n",
       " ('George V. Bonhag', {'3000m team'}),\n",
       " ('Abel Kiviat', {'3000m team'}),\n",
       " ('Henry Louis Scott', {'3000m team'}),\n",
       " ('Norman Taber', {'3000m team'}),\n",
       " ('William Applegarth', {'4x100m relay'}),\n",
       " (\"Victor Henry Augustus D'arcy\", {'4x100m relay'}),\n",
       " ('David Henry Jacobs', {'4x100m relay'}),\n",
       " ('Henry Maitland Mackintosh', {'4x100m relay'}),\n",
       " ('Edward F. Lindberg', {'4x400m relay'}),\n",
       " ('Hjalmar Andersson', {'cross country team'}),\n",
       " ('John Eke', {'cross country team'}),\n",
       " ('Josef Ternström', {'cross country team'}),\n",
       " ('Hugo K. Wieslander', {'decathlon'}),\n",
       " ('Matthew J. Mcgrath', {'hammer throw'}),\n",
       " ('Alma Wilfred Richards', {'high jump'}),\n",
       " ('Platt Adams', {'high jump, standing'}),\n",
       " ('Juho Julius Saaristo', {'javelin throw, both hands'}),\n",
       " ('Albert Lovejoy Gutterson', {'long jump'}),\n",
       " ('Konstantinos Tsiclitiras', {'long jump, standing'}),\n",
       " ('Kennedy Kane Mcarthur', {'marathon'}),\n",
       " ('Ferdinand Reinhardt Bie', {'pentathlon'}),\n",
       " ('Harry Stoddard Babcock', {'pole vault'}),\n",
       " ('Gustav Lindblom', {'triple jump'}),\n",
       " ('Earl Thomson', {'110m hurdles'}),\n",
       " ('Allen Woodring', {'200m'}),\n",
       " ('Percy Hodge', {'3000m steeplechase'}),\n",
       " ('Horace Hallock Brown', {'3000m team'}),\n",
       " ('Ivan Dresser', {'3000m team'}),\n",
       " ('Arlie Alfred Schardt', {'3000m team'}),\n",
       " (\"Bevil Gordon D'Urban Rudd\", {'400m'}),\n",
       " ('Frank Loomis', {'400m hurdles'}),\n",
       " ('Morris Kirksey', {'4x100m relay'}),\n",
       " ('Loren Murchison', {'4x100m relay'}),\n",
       " ('John Creyghton Ainsworth-Davis', {'4x400m relay'}),\n",
       " ('Guy Montagu Butler', {'4x400m relay'}),\n",
       " ('Cecil Richmond Griffiths', {'4x400m relay'}),\n",
       " ('Robert Alexander Lindsay', {'4x400m relay'}),\n",
       " ('Joseph Guillemot', {'5000m'}),\n",
       " ('Frederik Teudor Koskenniemi', {'cross country team'}),\n",
       " ('Heikki Liimatainen', {'cross country team'}),\n",
       " ('Helge Andreas Lövland', {'decathlon'}),\n",
       " ('Elmer Konstantin Niklander', {'discus throw'}),\n",
       " ('Patrick Ryan', {'hammer throw'}),\n",
       " ('Richmond Landon', {'high jump'}),\n",
       " ('Jonni Myyrä', {'javelin throw'}),\n",
       " ('R. William E. Petersson', {'long jump'}),\n",
       " ('Eero Reino Lehtonen', {'pentathlon'}),\n",
       " ('Frank Foss', {'pole vault'}),\n",
       " ('Frans Wilhelmi Pörhölä', {'shot put'}),\n",
       " ('Vilho Immanuel Tuulos', {'triple jump'}),\n",
       " ('Harold Abrahams', {'100m'}),\n",
       " ('Daniel Kinsey', {'110m hurdles'}),\n",
       " ('Elias Katz', {'3000m team'}),\n",
       " ('Frej Liewendhal', {'3000m team'}),\n",
       " ('Eino Seppälä', {'3000m team'}),\n",
       " ('S. Talja', {'3000m team'}),\n",
       " ('Eric Henry Liddell', {'400m'}),\n",
       " ('Frederick Morgan Taylor', {'400m hurdles'}),\n",
       " ('Louis Alfred Clarke', {'4x100m relay'}),\n",
       " ('Francis Hussey', {'4x100m relay'}),\n",
       " ('Jeremiah Alfred Leconey', {'4x100m relay'}),\n",
       " ('Commodore Cochrane', {'4x400m relay'}),\n",
       " ('Alan Helffrich', {'4x400m relay'}),\n",
       " ('J. Oliver Macdonald', {'4x400m relay'}),\n",
       " ('William Stephenson', {'4x400m relay'}),\n",
       " ('Douglas Gordon Lowe', {'800m'}),\n",
       " ('Eero Berg', {'cross country team'}),\n",
       " ('Eino Rastas', {'cross country team'}),\n",
       " ('Väinö Jeremias Sipilä', {'cross country team'}),\n",
       " ('Frederick Delmont Tootell', {'hammer throw'}),\n",
       " ('William De Hart Hubbard', {'long jump'}),\n",
       " ('Albin Oskar Stenroos', {'marathon'}),\n",
       " ('Lee Barnes', {'pole vault'}),\n",
       " ('Anthony William Winter', {'triple jump'}),\n",
       " ('Sidney Atkinson', {'110m hurdles'}),\n",
       " ('Harri Edvin Larva', {'1500m'}),\n",
       " ('Toivo Aarne Loukola', {'3000m steeplechase'}),\n",
       " ('David George Burghley', {'400m hurdles'}),\n",
       " ('Charles Edward Borah', {'4x100m relay'}),\n",
       " ('James F. Quinn', {'4x100m relay'}),\n",
       " ('Henry Argue Russell', {'4x100m relay'}),\n",
       " ('Frank Clifford Wykoff', {'4x100m relay'}),\n",
       " ('Myrtle Alice Cook', {'4x100m relay'}),\n",
       " ('Fanny Rosenfeld', {'4x100m relay'}),\n",
       " ('Ethel M. Smith', {'4x100m relay'}),\n",
       " ('Jean Thompson', {'4x100m relay'}),\n",
       " ('Frederick Pitt Alderman', {'4x400m relay'}),\n",
       " ('George Hetzel Baird', {'4x400m relay'}),\n",
       " ('Emerson Lane Spencer', {'4x400m relay'}),\n",
       " ('Karoline \"Lina\" Radke', {'800m'}),\n",
       " ('Paavo Ilmari Yrjölä', {'decathlon'}),\n",
       " ('Halina Konopacka', {'discus throw'}),\n",
       " (\"Patrick O'callaghan\", {'hammer throw'}),\n",
       " ('Robert Wade King', {'high jump'}),\n",
       " ('Ethel Catherwood', {'high jump'}),\n",
       " ('Erik H. Lundqvist', {'javelin throw'}),\n",
       " ('Edward Barton Hamm', {'long jump'}),\n",
       " ('Boughera Mohamed El Ouafi', {'marathon'}),\n",
       " ('Sabin William Carr', {'pole vault'}),\n",
       " ('John H. Kuck', {'shot put'}),\n",
       " ('Mikio Oda', {'triple jump'}),\n",
       " ('Janusz Kusocinski', {'10000m'}),\n",
       " ('Stanislawa Walasiewicz', {'100m'}),\n",
       " ('George J. Jr. Saling', {'110m hurdles'}),\n",
       " ('Luigi Beccali', {'1500m'}),\n",
       " ('Volmari Iso-Hollo', {'3000m steeplechase'}),\n",
       " ('Robert Morton Newburgh Tisdall', {'400m hurdles'}),\n",
       " ('Hector Monroe Dyer', {'4x100m relay'}),\n",
       " ('Robert Allan Kiesel', {'4x100m relay'}),\n",
       " ('Martin Emmett Toppino', {'4x100m relay'}),\n",
       " ('Mary Louise Carew', {'4x100m relay'}),\n",
       " ('Evelyn Furtsch', {'4x100m relay'}),\n",
       " ('Annette Joan Rogers', {'4x100m relay'}),\n",
       " ('Wilhelmina Von bremen', {'4x100m relay'}),\n",
       " ('Edgar Allen Ablowich', {'4x400m relay'}),\n",
       " ('Ivan William Fuqua', {'4x400m relay'}),\n",
       " ('Karl De Witt Warner', {'4x400m relay'}),\n",
       " ('Lauri Aleksanteri Lehtinen', {'5000m'}),\n",
       " ('Thomas William Green', {'50km walk'}),\n",
       " ('Thomas Hampson', {'800m'}),\n",
       " ('James Aloysius Bernard Bausch', {'decathlon'}),\n",
       " ('John Franklin Anderson', {'discus throw'}),\n",
       " ('Lillian Copeland', {'discus throw'}),\n",
       " ('Duncan Mcnaughton', {'high jump'}),\n",
       " ('Jean Shiley', {'high jump'}),\n",
       " ('Matti Henrik Jarvinen', {'javelin throw'}),\n",
       " ('Edward Lansing Gordon', {'long jump'}),\n",
       " ('Juan Carlos Zabala', {'marathon'}),\n",
       " ('William Waring Miller', {'pole vault'}),\n",
       " ('Leo Joseph Sexton', {'shot put'}),\n",
       " ('Chuhei Nambu', {'triple jump'}),\n",
       " ('Ilmari Salminen', {'10000m'}),\n",
       " ('Forrest Grady Towns', {'110m hurdles'}),\n",
       " ('John Lovelock', {'1500m'}),\n",
       " ('Archibald Franklin Williams', {'400m'}),\n",
       " ('Glenn Foster Hardin', {'400m hurdles'}),\n",
       " ('Foy Draper', {'4x100m relay'}),\n",
       " ('Ralph Metcalfe', {'4x100m relay'}),\n",
       " ('Harriett Claiborne Bland', {'4x100m relay'}),\n",
       " ('Arthur Godfrey Kilner Brown', {'4x400m relay'}),\n",
       " ('Godfrey Lionel Rampling', {'4x400m relay'}),\n",
       " ('William Roberts', {'4x400m relay'}),\n",
       " ('Frederick Ferdinand Wolff', {'4x400m relay'}),\n",
       " ('Gunnar Höckert', {'5000m'}),\n",
       " ('Hector Harold Whitlock', {'50km walk'}),\n",
       " ('John Youie Woodruff', {'800m'}),\n",
       " ('Trebisonda Valla', {'80m hurdles'}),\n",
       " ('Glenn Edward Morris', {'decathlon'}),\n",
       " ('William Kenneth Carpenter', {'discus throw'}),\n",
       " ('Gisela Mauermayer', {'discus throw'}),\n",
       " ('Karl Hein', {'hammer throw'}),\n",
       " ('Cornelius Cooper Johnson', {'high jump'}),\n",
       " ('Ibolya Csak', {'high jump'}),\n",
       " ('Gerhard (Gerd) Stöck', {'javelin throw'}),\n",
       " ('Mathilde (Tilly) Fleischer', {'javelin throw'}),\n",
       " ('Kitei Son', {'marathon'}),\n",
       " ('Earle Elmer Meadows', {'pole vault'}),\n",
       " ('Hans Woellke', {'shot put'}),\n",
       " ('Naoto Tajima', {'triple jump'}),\n",
       " ('John Mikaelsson', {'10000m walk'}),\n",
       " ('William Franklin III Porter', {'110m hurdles'}),\n",
       " ('Henry Eriksson', {'1500m'}),\n",
       " ('Tore Sjöstrand', {'3000m steeplechase'}),\n",
       " ('Harold Norwood Ewell', {'4x100m relay'}),\n",
       " ('Lorenzo Christopher Wright', {'4x100m relay'}),\n",
       " ('Xenia Stad-De jong', {'4x100m relay'}),\n",
       " ('Gerda Johanna Marie Van der kade-Koudijs', {'4x100m relay'}),\n",
       " ('Jeannette Josephina Maria Witziers-Timmer', {'4x100m relay'}),\n",
       " ('Clifford Frederick Bourland', {'4x400m relay'}),\n",
       " ('Arthur Harold Harnden', {'4x400m relay'}),\n",
       " ('Gaston Etienne Reiff', {'5000m'}),\n",
       " ('John Artur Ljunggren', {'50km walk'}),\n",
       " ('Robert Mathias', {'decathlon'}),\n",
       " ('Adolfo Consolini', {'discus throw'}),\n",
       " ('Imre Nemeth', {'hammer throw'}),\n",
       " ('John Winter', {'high jump'}),\n",
       " ('Alice Marie Coachman', {'high jump'}),\n",
       " ('Kaj Tapio Rautavaara', {'javelin throw'}),\n",
       " ('Hermine (Herma) Bauma', {'javelin throw'}),\n",
       " ('William Samuel Steele', {'long jump'}),\n",
       " ('Olga Gyarmati', {'long jump'}),\n",
       " ('Delfo Cabrera', {'marathon'}),\n",
       " ('Owen Guinn Smith', {'pole vault'}),\n",
       " ('Wilbur Marvin Thompson', {'shot put'}),\n",
       " ('Arne Ahman', {'triple jump'}),\n",
       " ('Joseph Barthel', {'1500m'}),\n",
       " ('Horace III Ashenfelter', {'3000m steeplechase'}),\n",
       " ('Charles Hewes Jr. Moore', {'400m hurdles'}),\n",
       " ('Finis Dean Smith', {'4x100m relay'}),\n",
       " ('Mae Heriwentha Faggs', {'4x100m relay'}),\n",
       " ('Catherine Hardy', {'4x100m relay'}),\n",
       " ('Barbara Pearl Jones', {'4x100m relay'}),\n",
       " ('Janet Theresa Moreau', {'4x100m relay'}),\n",
       " ('Leslie Laing', {'4x400m relay'}),\n",
       " ('Herbert Mckenley', {'4x400m relay'}),\n",
       " ('Giuseppe Dordoni', {'50km walk'}),\n",
       " ('Simeon Garland Iness', {'discus throw'}),\n",
       " ('Nina Romashkova-Ponomareva', {'discus throw'}),\n",
       " ('Jozsef Csermak', {'hammer throw'}),\n",
       " ('Walter Francis Davis', {'high jump'}),\n",
       " ('Esther Cornelia Brand', {'high jump'}),\n",
       " ('Cyrus C. Jr. Young', {'javelin throw'}),\n",
       " ('Dana Ingrova-Zatopkova', {'javelin throw'}),\n",
       " ('Jerome Cousins Biffle', {'long jump'}),\n",
       " ('Yvette Winefred Williams', {'long jump'}),\n",
       " ('Robert Richards', {'pole vault'}),\n",
       " ('William Patrick Jr. O', {'shot put'}),\n",
       " ('Galina Zybina', {'shot put'}),\n",
       " ('Adhemar Ferreira Da Silva', {'triple jump'}),\n",
       " ('Lee Calhoun', {'110m hurdles'}),\n",
       " ('Ronald Michael Delany', {'1500m'}),\n",
       " ('Leonid Spirin', {'20km walk'}),\n",
       " ('Christopher William Brasher', {'3000m steeplechase'}),\n",
       " ('Walter Thane Baker', {'4x100m relay'}),\n",
       " ('Leamon King', {'4x100m relay'}),\n",
       " ('Ira James Murchison', {'4x100m relay'}),\n",
       " ('Norma Wilson Crooker-Fleming', {'4x100m relay'}),\n",
       " ('Fluer Northey Mellor-Wenham', {'4x100m relay'}),\n",
       " ('Louis Woodard III Jones', {'4x400m relay'}),\n",
       " ('Jesse William Mashburn', {'4x400m relay'}),\n",
       " ('Norman Richard Read', {'50km walk'}),\n",
       " ('Milton Gray Campbell', {'decathlon'}),\n",
       " ('Alfred Oerter', {'discus throw'}),\n",
       " ('Olga Fikotova-Connolly', {'discus throw'}),\n",
       " ('Harold Vincent Connolly', {'hammer throw'}),\n",
       " ('Charles Everett Dumas', {'high jump'}),\n",
       " ('Mildred Louise Mcdaniel', {'high jump'}),\n",
       " ('Egil Danielsen', {'javelin throw'}),\n",
       " ('Inese Yaunzeme', {'javelin throw'}),\n",
       " ('Gregory Curtis Bell', {'long jump'}),\n",
       " ('Elzbieta Dunska-Krzesinska', {'long jump'}),\n",
       " ('Alain Mimoun', {'marathon'}),\n",
       " ('Tamara Tyshkevich', {'shot put'}),\n",
       " ('Pyotr Bolotnikov', {'10000m'}),\n",
       " ('Herbert Elliott', {'1500m'}),\n",
       " ('Livio Berruti', {'200m'}),\n",
       " ('Vladimir Golubnichy', {'20km walk'}),\n",
       " ('Zdzislaw Krzyszkowiak', {'3000m steeplechase'}),\n",
       " ('Bernd Cullmann', {'4x100m relay'}),\n",
       " ('Martin Lauer', {'4x100m relay'}),\n",
       " ('Walter Mahlendorf', {'4x100m relay'}),\n",
       " ('Martha B. Hudson', {'4x100m relay'}),\n",
       " ('Lucinda Williams', {'4x100m relay'}),\n",
       " ('John Lloyd Yerman', {'4x400m relay'}),\n",
       " ('Earl Verdelle Young', {'4x400m relay'}),\n",
       " ('Murray Gordon Halberg', {'5000m'}),\n",
       " ('Donald James Thompson', {'50km walk'}),\n",
       " ('Lyudmila Lisenko-Shevtsova', {'800m'}),\n",
       " ('Rafer Lewis Johnson', {'decathlon'}),\n",
       " ('Vasili Rudenkov', {'hammer throw'}),\n",
       " ('Robert Shavlakadze', {'high jump'}),\n",
       " ('Iolanda Balas', {'high jump'}),\n",
       " ('Viktor Tsybulenko', {'javelin throw'}),\n",
       " ('Elvira Ozolina', {'javelin throw'}),\n",
       " ('Ralph Harold Boston', {'long jump'}),\n",
       " ('Vera Kolashnikova-Krepkina', {'long jump'}),\n",
       " ('Abebe Bikila', {'marathon'}),\n",
       " ('Donald George Bragg', {'pole vault'}),\n",
       " ('William Henry Nieder', {'shot put'}),\n",
       " ('Jozef Schmidt', {'triple jump'}),\n",
       " ('William Mills', {'10000m'}),\n",
       " ('Hayes Jones', {'110m hurdles'}),\n",
       " ('Edith Marie Mcguire', {'200m'}),\n",
       " ('Kenneth Joseph Matthews', {'20km walk'}),\n",
       " ('Gaston Roelants', {'3000m steeplechase'}),\n",
       " ('Warren (Rex) Jay Cawley', {'400m hurdles'}),\n",
       " ('Gerald Howard Ashworth', {'4x100m relay'}),\n",
       " ('Otis Paul Drayton', {'4x100m relay'}),\n",
       " ('Richard Vaughn Stebbins', {'4x100m relay'}),\n",
       " ('Halina Herrmann-Gorecka-Richter', {'4x100m relay'}),\n",
       " ('Ewa Klobukowska', {'4x100m relay'}),\n",
       " ('Teresa Barbara Wieczorek-Cieply', {'4x100m relay'}),\n",
       " ('Ollan Conley Cassell', {'4x400m relay'}),\n",
       " ('Ulis C. Williams', {'4x400m relay'}),\n",
       " ('Robert Keyser Schul', {'5000m'}),\n",
       " ('Abdon Pamich', {'50km walk'}),\n",
       " ('Ann Elizabeth Packer', {'800m'}),\n",
       " ('Karin Richert-Balzer', {'80m hurdles'}),\n",
       " ('Willi Holdorf', {'decathlon'}),\n",
       " ('Romuald Klim', {'hammer throw'}),\n",
       " ('Valeri Brumel', {'high jump'}),\n",
       " ('Pauli Lauri Nevala', {'javelin throw'}),\n",
       " ('Mihaela Penes', {'javelin throw'}),\n",
       " ('Lynn Davies', {'long jump'}),\n",
       " ('Mary Denise Bignal-Rand', {'long jump'}),\n",
       " ('Frederick Morgan Hansen', {'pole vault'}),\n",
       " ('Dallas Crutcher III Long', {'shot put'}),\n",
       " ('Nabiba Naftali Temu', {'10000m'}),\n",
       " ('Willie Davenport', {'110m hurdles'}),\n",
       " ('Thomas C. Smith', {'200m'}),\n",
       " ('Amos Kipwabok Biwott', {'3000m steeplechase'}),\n",
       " ('Colette Besson', {'400m'}),\n",
       " ('David Peter Hemery', {'400m hurdles'}),\n",
       " ('Charles Edward Greene', {'4x100m relay'}),\n",
       " ('Melvin Jr. Pender', {'4x100m relay'}),\n",
       " ('Ronald Ray Smith', {'4x100m relay'}),\n",
       " ('Margaret Johnson Bailes', {'4x100m relay'}),\n",
       " ('Barbara Ann Ferrell', {'4x100m relay'}),\n",
       " ('Mildrette Netter', {'4x100m relay'}),\n",
       " ('Ronald John III Freeman', {'4x400m relay'}),\n",
       " ('George Lawrence James', {'4x400m relay'}),\n",
       " ('Mohamed Gammoudi', {'5000m'}),\n",
       " ('Christoph Höhne', {'50km walk'}),\n",
       " ('Ralph Doubell', {'800m'}),\n",
       " ('Madeline Manning-Jackson', {'800m'}),\n",
       " ('Maureen Caird', {'80m hurdles'}),\n",
       " ('William Anthony Toomey', {'decathlon'}),\n",
       " ('Lia Manoliu', {'discus throw'}),\n",
       " ('Gyula Zsivotzky', {'hammer throw'}),\n",
       " ('Richard Douglas Fosbury', {'high jump'}),\n",
       " ('Miloslava Rezkova-Hubner', {'high jump'}),\n",
       " ('Yanis Lusis', {'javelin throw'}),\n",
       " ('Angela Nemeth-Ranky', {'javelin throw'}),\n",
       " ('Bob Beamon', {'long jump'}),\n",
       " ('Viorica Viscopoleanu', {'long jump'}),\n",
       " ('Mamo Wolde', {'marathon'}),\n",
       " ('Robert Lloyd Seagren', {'pole vault'}),\n",
       " ('James Randel Matson', {'shot put'}),\n",
       " ('Margitta Helmbold-Gummel', {'shot put'}),\n",
       " ('Viktor Saneev', {'triple jump'}),\n",
       " ('Annelie Ehrhardt', {'100m hurdles'}),\n",
       " ('Rodney Jr. Milburn', {'110m hurdles'}),\n",
       " ('Pekka Vasala', {'1500m'}),\n",
       " ('Lyudmila Bragina', {'1500m'}),\n",
       " ('Peter Frenkel', {'20km walk'}),\n",
       " ('John Akii-Bua', {'400m hurdles'}),\n",
       " ('Lawrence J. Black', {'4x100m relay'}),\n",
       " ('Edward James Hart', {'4x100m relay'}),\n",
       " ('Robert Taylor', {'4x100m relay'}),\n",
       " ('Gerald A. Tinker', {'4x100m relay'}),\n",
       " ('Christiane Krause', {'4x100m relay'}),\n",
       " ('Charles Asati', {'4x400m relay'}),\n",
       " ('Hezekiah Munyoro Nyamau', {'4x400m relay'}),\n",
       " ('Robert Ouko', {'4x400m relay'}),\n",
       " ('Julius Sang', {'4x400m relay'}),\n",
       " ('Dagmar Käsling', {'4x400m relay'}),\n",
       " ('Rita Kühne', {'4x400m relay'}),\n",
       " ('Helga Seidler', {'4x400m relay'}),\n",
       " ('Bernd Kannenberg', {'50km walk'}),\n",
       " ('David James Wottle', {'800m'}),\n",
       " ('Hildegard Falck-Janze', {'800m'}),\n",
       " ('Nikolai Avilov', {'decathlon'}),\n",
       " ('Ludvik Danek', {'discus throw'}),\n",
       " ('Faina Melnik', {'discus throw'}),\n",
       " ('Anatoli Bondarchuk', {'hammer throw'}),\n",
       " ('Yuri Tarmak', {'high jump'}),\n",
       " ('Ulrike Meyfarth', {'high jump'}),\n",
       " ('Klaus Wolfermann', {'javelin throw'}),\n",
       " ('Ruth Fuchs', {'javelin throw'}),\n",
       " ('Randel Luvelle Williams', {'long jump'}),\n",
       " ('Frank Charles Shorter', {'marathon'}),\n",
       " ('Mary Peters', {'pentathlon'}),\n",
       " ('Wolfgang Nordwig', {'pole vault'}),\n",
       " ('Wladyslaw Komar', {'shot put'}),\n",
       " ('Nadezhda Chizhova', {'shot put'}),\n",
       " ('Hasely Crawford', {'100m'}),\n",
       " ('Johanna Schaller-Klier', {'100m hurdles'}),\n",
       " ('Guy Drut', {'110m hurdles'}),\n",
       " ('John Walker', {'1500m'}),\n",
       " ('Donald Quarrie', {'200m'}),\n",
       " ('Daniel Bautista rocha', {'20km walk'}),\n",
       " ('Anders Garderud', {'3000m steeplechase'}),\n",
       " ('Edwin Moses', {'400m hurdles'}),\n",
       " ('Harvey Edward Glance', {'4x100m relay'}),\n",
       " ('Millard Frank Jr. Hampton', {'4x100m relay'}),\n",
       " ('John Wesley Jones', {'4x100m relay'}),\n",
       " ('Steven Earl Riddick', {'4x100m relay'}),\n",
       " ('Carla Bodendorf', {'4x100m relay'}),\n",
       " ('Marlies Oelsner-Göhr', {'4x100m relay'}),\n",
       " ('Benjamin Gene Brown', {'4x400m relay'}),\n",
       " ('Herman Ronald Frazier', {'4x400m relay'}),\n",
       " ('Frederick Vaughn Newhouse', {'4x400m relay'}),\n",
       " ('Maxwell Lander Parks', {'4x400m relay'}),\n",
       " ('Christina Brehmer-Lathan', {'4x400m relay'}),\n",
       " ('Doris Maletzki', {'4x400m relay'}),\n",
       " ('Brigitte Rohde', {'4x400m relay'}),\n",
       " ('Ellen Stropahl-Streidt', {'4x400m relay'}),\n",
       " ('William Bruce Jenner', {'decathlon'}),\n",
       " ('Malcolm Maurice Wilkins', {'discus throw'}),\n",
       " ('Evelin Schlaak-Jahl', {'discus throw'}),\n",
       " ('Yuri Sedykh', {'hammer throw'}),\n",
       " ('Jacek Wszola', {'high jump'}),\n",
       " ('Rosemarie Witschas-Ackermann', {'high jump'}),\n",
       " ('Miklos Nemeth', {'javelin throw'}),\n",
       " ('Arnie Robinson', {'long jump'}),\n",
       " ('Angela Voigt', {'long jump'}),\n",
       " ('Waldemar Cierpinski', {'marathon'}),\n",
       " ('Siegrun Siegl', {'pentathlon'}),\n",
       " ('Tadeusz Slusarski', {'pole vault'}),\n",
       " ('Udo Beyer', {'shot put'}),\n",
       " ('Ivanka Mikailova Christova-Todorova', {'shot put'}),\n",
       " ('Allan Wells', {'100m'}),\n",
       " ('Liudmila Kondratieva', {'100m'}),\n",
       " ('Vera Komisova', {'100m hurdles'}),\n",
       " ('Thomas Munkelt', {'110m hurdles'}),\n",
       " ('Sebastian Coe', {'1500m'}),\n",
       " ('Pietro Mennea', {'200m'}),\n",
       " ('Maurizio Damilano', {'20km walk'}),\n",
       " ('Bronislaw Malinowski', {'3000m steeplechase'}),\n",
       " ('Marita Koch', {'400m'}),\n",
       " ('Volker Beck', {'400m hurdles'}),\n",
       " ('Aleksandr Aksinin', {'4x100m relay'}),\n",
       " ('Vladimir Muravyov', {'4x100m relay'}),\n",
       " ('Andrei Prokofiev', {'4x100m relay'}),\n",
       " ('Nikolai Sidorov', {'4x100m relay'}),\n",
       " ('Ingrid Auerswald-Lange', {'4x100m relay'}),\n",
       " ('Romy Müller', {'4x100m relay'}),\n",
       " ('Nikolai Chernetski', {'4x400m relay'}),\n",
       " ('Mikhail Linge', {'4x400m relay'}),\n",
       " ('Remigius Valiulis', {'4x400m relay'}),\n",
       " ('Tatiana Goishchik', {'4x400m relay'}),\n",
       " ('Irina Nazarova', {'4x400m relay'}),\n",
       " ('Tatiana Prorochenko', {'4x400m relay'}),\n",
       " ('Nina Ziuskova', {'4x400m relay'}),\n",
       " ('Hartwig Gauder', {'50km walk'}),\n",
       " ('Steve Ovett', {'800m'}),\n",
       " ('Nadezhda Olizarenko', {'800m'}),\n",
       " ('Daley Thompson', {'decathlon'}),\n",
       " ('Viktor Rashchupkin', {'discus throw'}),\n",
       " ('Gerd Wessig', {'high jump'}),\n",
       " ('Sara Simeoni', {'high jump'}),\n",
       " ('Dainis Kula', {'javelin throw'}),\n",
       " ('Maria Colon', {'javelin throw'}),\n",
       " ('Lutz Dombrowski', {'long jump'}),\n",
       " ('Tatiana Kolpakova', {'long jump'}),\n",
       " ('Nadezhda Tkachenko', {'pentathlon'}),\n",
       " ('Wladyslaw Kozakiewicz', {'pole vault'}),\n",
       " ('Vladimir Kiselev', {'shot put'}),\n",
       " ('Ilona Schoknecht-Slupianek', {'shot put'}),\n",
       " ('Yaak Uudmae', {'triple jump'}),\n",
       " ('Alberto Cova', {'10000m'}),\n",
       " ('Benita Fitzgerald-Brown', {'100m hurdles'}),\n",
       " ('Roger Kingdom', {'110m hurdles'}),\n",
       " ('Gabriella Dorio', {'1500m'}),\n",
       " ('Ernesto Canto gudino', {'20km walk'}),\n",
       " ('Maricica Puica', {'3000m'}),\n",
       " ('Julius Korir', {'3000m steeplechase'}),\n",
       " ('Nawal El moutawakel', {'400m hurdles'}),\n",
       " ('Ronald James Brown', {'4x100m relay'}),\n",
       " ('Samuel Louis Graddy', {'4x100m relay'}),\n",
       " ('Calvin Smith', {'4x100m relay'}),\n",
       " ('Jeanette Bolden', {'4x100m relay'}),\n",
       " ('Alice Regina Brown', {'4x100m relay'}),\n",
       " ('Raymond Armstead', {'4x400m relay'}),\n",
       " ('Antonio Ricardo Mckay', {'4x400m relay'}),\n",
       " ('Sunder L. Nix', {'4x400m relay'}),\n",
       " ('Sherryl Frances Howard', {'4x400m relay'}),\n",
       " ('Lillie Mae Leatherwood-King', {'4x400m relay'}),\n",
       " ('Said Aouita', {'5000m'}),\n",
       " ('Raul Gonzalez', {'50km walk'}),\n",
       " ('Joaquim Cruz', {'800m'}),\n",
       " ('Doina Besliu-Melinte', {'800m'}),\n",
       " ('Rolf Danneberg', {'discus throw'}),\n",
       " ('Ria G. Stalman', {'discus throw'}),\n",
       " ('Juha Tiainen', {'hammer throw'}),\n",
       " ('Glyniss Leanne Nunn-Saunders-Cearns', {'heptathlon'}),\n",
       " ('Dietmar Mögenburg', {'high jump'}),\n",
       " ('Arto Harkonen', {'javelin throw'}),\n",
       " ('Teresa Sanderson', {'javelin throw'}),\n",
       " ('Anisoara Cusmir-Stanciu', {'long jump'}),\n",
       " ('Carlos Lopes', {'marathon'}),\n",
       " ('Joan Benoit', {'marathon'}),\n",
       " ('Pierre Quinon', {'pole vault'}),\n",
       " ('Alessandro Andrei', {'shot put'}),\n",
       " ('Claudia Losch', {'shot put'}),\n",
       " ('Alfredrick Alphonzo Joyner', {'triple jump'}),\n",
       " ('Moulay Brahim Boutayeb', {'10000m'}),\n",
       " ('Olga Bondarenko', {'10000m'}),\n",
       " ('Yordanka Donkova', {'100m hurdles'}),\n",
       " ('Peter Rono', {'1500m'}),\n",
       " ('Paula Ivan', {'1500m'}),\n",
       " ('Joseph Nathaniel Jr. Deloach', {'200m'}),\n",
       " ('Jozef Pribilinec', {'20km walk'}),\n",
       " ('Tatyana Samolenko-Dorovskikh', {'3000m'}),\n",
       " ('Julius Kariuki', {'3000m steeplechase'}),\n",
       " ('Andre Lamar Phillips', {'400m hurdles'}),\n",
       " ('Deborah Flintoff-King', {'400m hurdles'}),\n",
       " ('Viktor Bryzgin', {'4x100m relay'}),\n",
       " ('Vladimir Krylov', {'4x100m relay'}),\n",
       " ('Vitaly Savin', {'4x100m relay'}),\n",
       " ('Sheila Echols', {'4x100m relay'}),\n",
       " ('Daniel Everett', {'4x400m relay'}),\n",
       " ('Harry Butch . Reynolds', {'4x400m relay'}),\n",
       " ('Kevin Bernard Robinzine', {'4x400m relay'}),\n",
       " ('Tatiana Ledovskaya', {'4x400m relay'}),\n",
       " ('Olga Nazarova', {'4x400m relay'}),\n",
       " ('Mariya Pinigina', {'4x400m relay'}),\n",
       " ('John Ngugi', {'5000m'}),\n",
       " ('Vyacheslav Ivanenko', {'50km walk'}),\n",
       " ('Paul Ereng', {'800m'}),\n",
       " ('Sigrun Wodars', {'800m'}),\n",
       " ('Christian Schenk', {'decathlon'}),\n",
       " ('Jürgen Schult', {'discus throw'}),\n",
       " ('Martina Hellmann', {'discus throw'}),\n",
       " ('Sergei Litvinov', {'hammer throw'}),\n",
       " ('Gennadi Avdeenko', {'high jump'}),\n",
       " ('Dorothy Louise Ritter', {'high jump'}),\n",
       " ('Tapio Korjus', {'javelin throw'}),\n",
       " ('Petra Felke-Meier', {'javelin throw'}),\n",
       " ('Gelindo Bordin', {'marathon'}),\n",
       " ('Rosa Mota', {'marathon'}),\n",
       " ('Sergey Bubka', {'pole vault'}),\n",
       " ('Ulf Timmermann', {'shot put'}),\n",
       " ('Natalya Lisovskaya', {'shot put'}),\n",
       " ('Kristo Gantchev Markov', {'triple jump'}),\n",
       " ('Khalid Skah', {'10000m'}),\n",
       " ('Derartu Tulu', {'10000m'}),\n",
       " ('Yueling Chen', {'10000m walk'}),\n",
       " ('Linford Christie', {'100m'}),\n",
       " ('Paraskevi Voula Patoulidou', {'100m hurdles'}),\n",
       " ('Mark Mckoy', {'110m hurdles'}),\n",
       " ('Fermin Cacho ruiz', {'1500m'}),\n",
       " ('Hassiba Boulmerka', {'1500m'}),\n",
       " ('Daniel Plaza montero', {'20km walk'}),\n",
       " ('Elena Romanova', {'3000m'}),\n",
       " ('Mathew Birir', {'3000m steeplechase'}),\n",
       " ('Kevin Curtis Young', {'400m hurdles'}),\n",
       " ('Sally Janet Jane Gunnell', {'400m hurdles'}),\n",
       " ('Leroy Burrell', {'4x100m relay'}),\n",
       " ('James Jett', {'4x100m relay'}),\n",
       " ('Dennis Mitchell', {'4x100m relay'}),\n",
       " ('Michelle Finn', {'4x100m relay'}),\n",
       " ('Carlette D. Guidry-White', {'4x100m relay'}),\n",
       " ('Esther Jones', {'4x100m relay'}),\n",
       " ('Darnell Hall', {'4x400m relay'}),\n",
       " ('Charles L. Jenkins', {'4x400m relay'}),\n",
       " ('Andrew Valmon', {'4x400m relay'}),\n",
       " ('Marina Chmonina', {'4x400m relay'}),\n",
       " ('Lioudmila Djigalova', {'4x400m relay'}),\n",
       " ('Lilia Nurutdinova', {'4x400m relay'}),\n",
       " ('Yelena Rouzina', {'4x400m relay'}),\n",
       " ('Dieter Baumann', {'5000m'}),\n",
       " ('Andrey Perlov', {'50km walk'}),\n",
       " ('William Tanui', {'800m'}),\n",
       " ('Ellen Van langen', {'800m'}),\n",
       " ('Robert Zmelik', {'decathlon'}),\n",
       " ('Romas Ubartas', {'discus throw'}),\n",
       " ('Maritza Marten', {'discus throw'}),\n",
       " ('Andrey Abduvaliev', {'hammer throw'}),\n",
       " ('Javier Sotomayor', {'high jump'}),\n",
       " ('Heike Henkel', {'high jump'}),\n",
       " ('Jan Zelezny', {'javelin throw'}),\n",
       " ('Silke Renk', {'javelin throw'}),\n",
       " ('Heike Drechsler', {'long jump'}),\n",
       " ('Young-Cho Hwang', {'marathon'}),\n",
       " ('Valentina Yegorova', {'marathon'}),\n",
       " ('Maksim Tarasov', {'pole vault'}),\n",
       " ('Michael D. Stulce', {'shot put'}),\n",
       " ('Svetlana Krivelyova', {'shot put'}),\n",
       " ('Mike Conley', {'triple jump'}),\n",
       " ('Haile Gebrselassie', {'10000m'}),\n",
       " ('Fernanda Ribeiro', {'10000m'}),\n",
       " ('Yelena Nikolayeva', {'10000m walk'}),\n",
       " ('Ludmila Engquist', {'100m hurdles'}),\n",
       " ('Allen Johnson', {'110m hurdles'}),\n",
       " ('Nourredine Morceli', {'1500m'}),\n",
       " ('Jefferson Perez', {'20km walk'}),\n",
       " ('Joseph Keter', {'3000m steeplechase'}),\n",
       " ('Derrick Adkins', {'400m hurdles'}),\n",
       " ('Deon Marie Hemmings', {'400m hurdles'}),\n",
       " ('Carlton Chambers', {'4x100m relay'}),\n",
       " ('Robert Esmie', {'4x100m relay'}),\n",
       " ('Glenroy Gilbert', {'4x100m relay'}),\n",
       " ('Bruny Surin', {'4x100m relay'}),\n",
       " ('Chryste Gaines', {'4x100m relay'}),\n",
       " ('Inger Miller', {'4x100m relay'}),\n",
       " ('Alvin Harrison', {'4x400m relay'}),\n",
       " ('Anthuan Maybank', {'4x400m relay'}),\n",
       " ('Derek Mills', {'4x400m relay'}),\n",
       " ('Jason Rouser', {'4x400m relay'}),\n",
       " ('Lamont Smith', {'4x400m relay'}),\n",
       " ('Kim Graham', {'4x400m relay'}),\n",
       " ('Maicel Malone', {'4x400m relay'}),\n",
       " ('Jearl Miles', {'4x400m relay'}),\n",
       " ('Rochelle Stevens', {'4x400m relay'}),\n",
       " ('Linetta Wilson', {'4x400m relay'}),\n",
       " ('Venuste Niyongabo', {'5000m'}),\n",
       " ('Junxia Wang', {'5000m'}),\n",
       " ('Vebjorn Rodal', {'800m'}),\n",
       " (\"Dan O'brien\", {'decathlon'}),\n",
       " ('Lars Riedel', {'discus throw'}),\n",
       " ('Ilke Wyludda', {'discus throw'}),\n",
       " ('Balazs Kiss', {'hammer throw'}),\n",
       " ('Ghada Shouaa', {'heptathlon'}),\n",
       " ('Charles Austin', {'high jump'}),\n",
       " ('Stefka Kostadinova', {'high jump'}),\n",
       " ('Heli Orvokki Rantanen', {'javelin throw'}),\n",
       " ('Chioma Ajunwa', {'long jump'}),\n",
       " ('Josia Thugwane', {'marathon'}),\n",
       " ('Fatuma Roba', {'marathon'}),\n",
       " ('Jean Galfione', {'pole vault'}),\n",
       " ('Eric Randolph Barnes', {'shot put'}),\n",
       " ('Astrid Kumbernuss', {'shot put'}),\n",
       " ('Kenny Harrison', {'triple jump'}),\n",
       " ('Inessa Kravets', {'triple jump'}),\n",
       " ('Olga Shishigina', {'100m hurdles'}),\n",
       " ('Anier Garcia', {'110m hurdles'}),\n",
       " ('Noah Kiprono Ngenyi', {'1500m'}),\n",
       " ('Nouria Merah-Benida', {'1500m'}),\n",
       " ('Konstantinos Kenteris', {'200m'}),\n",
       " ('Liping Wang', {'20km race walk'}),\n",
       " ('Reuben Kosgei', {'3000m steeplechase'}),\n",
       " ('Cathy Freeman', {'400m'}),\n",
       " ('Irina Privalova', {'400m hurdles'}),\n",
       " ('Kenneth Brokenburr', {'4x100m relay'}),\n",
       " ('Jon Drummond', {'4x100m relay'}),\n",
       " ('Brian Lewis', {'4x100m relay'}),\n",
       " ('Tim Montgomery', {'4x100m relay'}),\n",
       " ('Bernard Williams iii', {'4x100m relay'}),\n",
       " ('Debbie Ferguson-Mckenzie', {'4x100m relay'}),\n",
       " ('Sevatheda Fynes', {'4x100m relay'}),\n",
       " ('Eldice Lewis', {'4x100m relay'}),\n",
       " ('Chandra Sturrup', {'4x100m relay'}),\n",
       " ('Andrea Anderson', {'4x400m relay'}),\n",
       " ('LaTasha Colander', {'4x400m relay'}),\n",
       " ('Monique Hennagan', {'4x400m relay'}),\n",
       " ('Millon Wolde', {'5000m'}),\n",
       " ('Gabriela Szabo', {'5000m'}),\n",
       " ('Nils Schumann', {'800m'}),\n",
       " ('Maria Mutola', {'800m'}),\n",
       " ('Erki Nool', {'decathlon'}),\n",
       " ('Virgilijus Alekna', {'discus throw'}),\n",
       " ('Ellina Zvereva', {'discus throw'}),\n",
       " ('Szymon Ziolkowski', {'hammer throw'}),\n",
       " ('Kamila Skolimowska', {'hammer throw'}),\n",
       " ('Denise Lewis', {'heptathlon'}),\n",
       " ('Sergey Kliugin', {'high jump'}),\n",
       " ('Yelena Yelesina', {'high jump'}),\n",
       " ('Trine Hattestad', {'javelin throw'}),\n",
       " ('Ivan Pedroso', {'long jump'}),\n",
       " ('Gezahegne Abera', {'marathon'}),\n",
       " ('Naoko Takahashi', {'marathon'}),\n",
       " ('Nick Hysong', {'pole vault'}),\n",
       " ('Stacy Dragila', {'pole vault'}),\n",
       " ('Arsi Harju', {'shot put'}),\n",
       " ('Yanina Karolchik', {'shot put'}),\n",
       " ('Jonathan David Edwards', {'triple jump'}),\n",
       " ('Tereza Marinova', {'triple jump'}),\n",
       " ('Huina Xing', {'10000m'}),\n",
       " ('Justin Gatlin', {'100m'}),\n",
       " ('Yuliya Nesterenko', {'100m'}),\n",
       " ('Joanna Hayes', {'100m hurdles'}),\n",
       " ('Xiang Liu', {'110m hurdles'}),\n",
       " ('Shawn Crawford', {'200m'}),\n",
       " ('Athanasia Tsoumeleka', {'20km race walk'}),\n",
       " ('Ivano Brugnetti', {'20km walk'}),\n",
       " ('Ezekiel Kemboi', {'3000m steeplechase'}),\n",
       " ('Tonique Williams-Darling', {'400m'}),\n",
       " ('Felix Sanchez', {'400m hurdles'}),\n",
       " ('Fani Chalkia', {'400m hurdles'}),\n",
       " ('Darren Campbell', {'4x100m relay'}),\n",
       " ('Marlon Devonish', {'4x100m relay'}),\n",
       " ('Jason Gardener', {'4x100m relay'}),\n",
       " ('Mark Lewis-Francis', {'4x100m relay'}),\n",
       " ('Aleen Bailey', {'4x100m relay'}),\n",
       " ('Tanya Lawrence', {'4x100m relay'}),\n",
       " ('Beverly Mcdonald', {'4x100m relay'}),\n",
       " ('Sherone Simpson', {'4x100m relay'}),\n",
       " ('Derrick Brew', {'4x400m relay'}),\n",
       " ('Otis Harris', {'4x400m relay'}),\n",
       " ('Darold Williamson', {'4x400m relay'}),\n",
       " ('Kelly Willie', {'4x400m relay'}),\n",
       " ('Crystal Cox', {'4x400m relay'}),\n",
       " ('Monique Henderson', {'4x400m relay'}),\n",
       " ('Sanya Richards', {'4x400m relay'}),\n",
       " ('Moushaumi Robinson', {'4x400m relay'}),\n",
       " ('Deedee Trotter', {'4x400m relay'}),\n",
       " ('Yuriy Borzakovskiy', {'800m'}),\n",
       " ('Roman Sebrle', {'decathlon'}),\n",
       " ('Natalya Sadova', {'discus throw'}),\n",
       " ('Koji Murofushi', {'hammer throw'}),\n",
       " ('Olga Kuzenkova', {'hammer throw'}),\n",
       " ('Carolina Kluft', {'heptathlon'}),\n",
       " ('Stefan Holm', {'high jump'}),\n",
       " ('Elena Slesarenko', {'high jump'}),\n",
       " ('Andreas Thorkildsen', {'javelin throw'}),\n",
       " ('Osleidys Menendez', {'javelin throw'}),\n",
       " ('Dwight Phillips', {'long jump'}),\n",
       " ('Tatyana Lebedeva', {'long jump'}),\n",
       " ('Stefano Baldini', {'marathon'}),\n",
       " ('Mizuki Noguchi', {'marathon'}),\n",
       " ('Timothy Mack', {'pole vault'}),\n",
       " ('Elena Isinbaeva', {'pole vault'}),\n",
       " ('Yuriy Bilonog', {'shot put'}),\n",
       " ('Yumileidi Cumba', {'shot put'}),\n",
       " ('Christian Olsson', {'triple jump'}),\n",
       " ('Francoise Mbango etone', {'triple jump'}),\n",
       " ('Shelly-Ann Fraser', {'100m'}),\n",
       " ('Dawn Harper', {'100m hurdles'}),\n",
       " ('Dayron Robles', {'110m hurdles'}),\n",
       " ('Asbel Kipruto Kiprop', {'1500m'}),\n",
       " ('Nancy jebet Langat', {'1500m'}),\n",
       " ('Olga Kaniskina', {'20km race walk'}),\n",
       " ('Valeriy Borchin', {'20km walk'}),\n",
       " ('Brimin Kiprop Kipruto', {'3000m steeplechase'}),\n",
       " ('Gulnara Galkina-Samitova', {'3000m steeplechase'}),\n",
       " ('Christine Ohuruogu', {'400m'}),\n",
       " ('Melaine Walker', {'400m hurdles'}),\n",
       " ('Nesta Carter', {'4x100m relay'}),\n",
       " ('Michael Frater', {'4x100m relay'}),\n",
       " ('Asafa Powell', {'4x100m relay'}),\n",
       " ('Yuliya Chermoshanskaya', {'4x100m relay'}),\n",
       " ('Aleksandra Fedoriva', {'4x100m relay'}),\n",
       " ('Yulia Gushchina', {'4x100m relay'}),\n",
       " ('Evgeniya Polyakova', {'4x100m relay'}),\n",
       " ('David Neville', {'4x400m relay'}),\n",
       " ('Mary Wineberg', {'4x400m relay'}),\n",
       " ('Alex Schwazer', {'50km walk'}),\n",
       " ('Wilfred Bungei', {'800m'}),\n",
       " ('Pamela Jelimo', {'800m'}),\n",
       " ('Bryan Clay', {'decathlon'}),\n",
       " ('Gerd Kanter', {'discus throw'}),\n",
       " ('Stephanie Brown trafton', {'discus throw'}),\n",
       " ('Primoz Kozmus', {'hammer throw'}),\n",
       " ('Aksana Miankova', {'hammer throw'}),\n",
       " ('Nataliia Dobrynska', {'heptathlon'}),\n",
       " ('Andrey Silnov', {'high jump'}),\n",
       " ('Tia Hellebaut', {'high jump'}),\n",
       " ('Barbora Spotakova', {'javelin throw'}),\n",
       " ('Irving Jahir Saladino aranda', {'long jump'}),\n",
       " ('Maurren Higa Maggi', {'long jump'}),\n",
       " ('Samuel Kamau Wansiru', {'marathon'}),\n",
       " ('Constantina Tomescu', {'marathon'}),\n",
       " ('Steve Hooker', {'pole vault'}),\n",
       " ('Tomasz Majewski', {'shot put'}),\n",
       " ('Valerie Vili', {'shot put'}),\n",
       " ('Nelson Evora', {'triple jump'}),\n",
       " ('Shelly-Ann Fraser-Pryce', {'100m'}),\n",
       " ('Yohan Blake', {'4x100m relay'}),\n",
       " ('Carmelita Jeter', {'4x100m relay'}),\n",
       " ('Natalya Antyukh', {'400m hurdles'}),\n",
       " ('DeeDee Trotter', {'4x400m relay'}),\n",
       " ('Yuliya Zaripova', {'3000m steeplechase'}),\n",
       " ('Ivan Ukhov', {'high jump'}),\n",
       " ('Anna Chicherova', {'high jump'}),\n",
       " ('Tatyana Lysenko', {'hammer throw'}),\n",
       " ('Sergey Kirdyapkin', {'50km walk'}),\n",
       " ('Elena Lashmanova', {'20km walk'}),\n",
       " ('Mariya Savinova', {'800m'}),\n",
       " ('Valerie Adams', {'shot put'}),\n",
       " ('David Lekuta Rudisha', {'800m'}),\n",
       " ('Olga Rypakova', {'triple jump'}),\n",
       " ('Robert Harting', {'discus throw'}),\n",
       " ('Krisztian Pars', {'hammer throw'}),\n",
       " ('Kirani James', {'400m'}),\n",
       " ('Jessica Ennis', {'heptathlon'}),\n",
       " ('Kemar Bailey-Cole', {'4x100m relay'}),\n",
       " ('Tiki Gelana', {'marathon'}),\n",
       " ('Alistair Brownlee', {'triathlon (long jump / shot put / 100 yards)'}),\n",
       " ('Greg Rutherford', {'long jump'}),\n",
       " ('Renaud Lavillenie', {'pole vault'}),\n",
       " ('Sally Pearson', {'100m hurdles'}),\n",
       " ('Taoufik Makhloufi', {'1500m'}),\n",
       " ('Ding Chen', {'20km walk'}),\n",
       " ('Sandra Perkovic', {'discus throw'}),\n",
       " ('Chris Brown', {'4x400m relay'}),\n",
       " ('Michael Mathieu', {'4x400m relay'}),\n",
       " ('Ramon Miller', {'4x400m relay'}),\n",
       " ('Demetrius Pinder', {'4x400m relay'}),\n",
       " ('Lauryn Williams', {'4x100m relay'}),\n",
       " ('Jennifer Suhr', {'pole vault'}),\n",
       " ('Ashton Eaton', {'decathlon'}),\n",
       " ('Keshia Baker', {'4x400m relay'}),\n",
       " ('Diamond Dixon', {'4x400m relay'}),\n",
       " ('Brittney Reese', {'long jump'}),\n",
       " ('Keshorn Walcott', {'javelin throw'}),\n",
       " ('Stephen Kiprotich', {'marathon'}),\n",
       " ('Alptekin Asli Cakir', {'1500m'}),\n",
       " ('Nicola Spirig', {'triathlon (long jump / shot put / 100 yards)'}),\n",
       " ('Christian Taylor', {'triple jump'}),\n",
       " ('Bianca Knight', {'4x100m relay'}),\n",
       " ('Aries Merritt', {'110m hurdles'}),\n",
       " ('Jeneba Tarmoh', {'4x100m relay'}),\n",
       " ('Tianna Madison', {'4x100m relay'}),\n",
       " ('Francena McCorory', {'4x400m relay'}),\n",
       " ('Tori Bowie', {'4x100m relay'}),\n",
       " ('Vivian Jepkemoi Cheruiyot', {'5000m team'}),\n",
       " ('Almaz Ayana', {'10miles walk (16093m)'}),\n",
       " ('Lashawn Merritt', {'4x400m relay'}),\n",
       " ('Morolake Akinosun', {'4x100m relay'}),\n",
       " ('Nickel Ashmeade', {'4x100m relay'}),\n",
       " ('Ruth Beitia', {'high jump'}),\n",
       " ('Michelle Carter', {'shot put'}),\n",
       " ('Matthew Centrowitz', {'1500m'}),\n",
       " ('Kerron Clement', {'400m hurdles'}),\n",
       " ('Kyle Clemons', {'4x400m relay'}),\n",
       " ('Ryan Crouser', {'shot put'}),\n",
       " ('Thiago Braz Da Silva', {'pole vault'}),\n",
       " ('Derek Drouin', {'high jump'}),\n",
       " ('Taylor Ellis-Watson', {'4x400m relay'}),\n",
       " ('Phyllis Francis', {'4x400m relay'}),\n",
       " ('English Gardner', {'4x100m relay'}),\n",
       " ('Arman Hall', {'4x400m relay'}),\n",
       " ('Christoph Harting', {'discus throw'}),\n",
       " ('Natasha Hastings', {'4x400m relay'}),\n",
       " ('Jeff Henderson', {'long jump'}),\n",
       " ('Caterine Ibarguen', {'triple jump'}),\n",
       " ('Ruth Jebet', {'3000m steeplechase'}),\n",
       " ('Gwen Jorgensen', {'triathlon (long jump / shot put / 100 yards)'}),\n",
       " ('Eliud Kipchoge', {'marathon'}),\n",
       " ('Conseslus Kipruto', {'3000m steeplechase'}),\n",
       " ('Faith Chepngetich Kipyegon', {'1500m'}),\n",
       " ('Sara Kolak', {'javelin throw'}),\n",
       " ('Hong Liu', {'20km walk'}),\n",
       " ('Francena Mccorory', {'4x400m relay'}),\n",
       " ('Omar Mcleod', {'110m hurdles'}),\n",
       " ('Tony Mcquay', {'4x400m relay'}),\n",
       " ('Shaunae Miller', {'400m'}),\n",
       " ('Jevaughn Minzie', {'4x100m relay'}),\n",
       " ('Dalilah Muhammad', {'400m hurdles'}),\n",
       " ('Dilshod Nazarov', {'hammer throw'}),\n",
       " ('Courtney Okolo', {'4x400m relay'}),\n",
       " ('Jenna Prandini', {'4x100m relay'}),\n",
       " ('Gil Roberts', {'4x400m relay'}),\n",
       " ('Thomas Rohler', {'javelin throw'}),\n",
       " ('Brianna Rollins', {'100m hurdles'}),\n",
       " ('Caster Semenya', {'800m'}),\n",
       " ('Ekaterini Stefanidi', {'pole vault'}),\n",
       " ('Jemima Jelagat Sumgong', {'marathon'}),\n",
       " ('Nafissatou Thiam', {'heptathlon'}),\n",
       " ('Matej Toth', {'50km walk'}),\n",
       " ('NIEKERK Wayde Van', {'400m'}),\n",
       " ('David Verburg', {'4x400m relay'}),\n",
       " ('Zhen Wang', {'20km walk'}),\n",
       " ('Ariana Washington', {'4x100m relay'}),\n",
       " ('Anita Wlodarczyk', {'hammer throw'})]"
      ]
     },
     "execution_count": 229,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dct = cls.defaultdict(list)\n",
    "for medal in medals:\n",
    "    dct[medal.athelete]\n",
    "    dct[medal.athelete].append(medal.event)\n",
    "\n",
    "def clean(event):\n",
    "    return ' '.join(word for word in event.split() if word not in ('men','women'))\n",
    "\n",
    "dct2 = cls.defaultdict(set)\n",
    "for athelete, events in dct.items():\n",
    "    for event in events:\n",
    "        dct2[athelete].add(clean(event))\n",
    "import operator\n",
    "sorted(dct2.items(), key=lambda x: len(x[1]), reverse=True)\n",
    "# for i in range(5):\n",
    "#     athelete=max(dct2.items(), key=lambda x: len(x[1]))\n",
    "#     print(athelete)#, dct2[athelete])\n",
    "#     dct2.pop(athelete[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "k = cls.Counter('abc') - cls.Counter('bc')\n",
    "print(sum(k.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class dict in module builtins:\n",
      "\n",
      "class dict(object)\n",
      " |  dict() -> new empty dictionary\n",
      " |  dict(mapping) -> new dictionary initialized from a mapping object's\n",
      " |      (key, value) pairs\n",
      " |  dict(iterable) -> new dictionary initialized as if via:\n",
      " |      d = {}\n",
      " |      for k, v in iterable:\n",
      " |          d[k] = v\n",
      " |  dict(**kwargs) -> new dictionary initialized with the name=value pairs\n",
      " |      in the keyword argument list.  For example:  dict(one=1, two=2)\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __contains__(self, key, /)\n",
      " |      True if the dictionary has the specified key, else False.\n",
      " |  \n",
      " |  __delitem__(self, key, /)\n",
      " |      Delete self[key].\n",
      " |  \n",
      " |  __eq__(self, value, /)\n",
      " |      Return self==value.\n",
      " |  \n",
      " |  __ge__(self, value, /)\n",
      " |      Return self>=value.\n",
      " |  \n",
      " |  __getattribute__(self, name, /)\n",
      " |      Return getattr(self, name).\n",
      " |  \n",
      " |  __getitem__(...)\n",
      " |      x.__getitem__(y) <==> x[y]\n",
      " |  \n",
      " |  __gt__(self, value, /)\n",
      " |      Return self>value.\n",
      " |  \n",
      " |  __init__(self, /, *args, **kwargs)\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |  \n",
      " |  __iter__(self, /)\n",
      " |      Implement iter(self).\n",
      " |  \n",
      " |  __le__(self, value, /)\n",
      " |      Return self<=value.\n",
      " |  \n",
      " |  __len__(self, /)\n",
      " |      Return len(self).\n",
      " |  \n",
      " |  __lt__(self, value, /)\n",
      " |      Return self<value.\n",
      " |  \n",
      " |  __ne__(self, value, /)\n",
      " |      Return self!=value.\n",
      " |  \n",
      " |  __repr__(self, /)\n",
      " |      Return repr(self).\n",
      " |  \n",
      " |  __setitem__(self, key, value, /)\n",
      " |      Set self[key] to value.\n",
      " |  \n",
      " |  __sizeof__(...)\n",
      " |      D.__sizeof__() -> size of D in memory, in bytes\n",
      " |  \n",
      " |  clear(...)\n",
      " |      D.clear() -> None.  Remove all items from D.\n",
      " |  \n",
      " |  copy(...)\n",
      " |      D.copy() -> a shallow copy of D\n",
      " |  \n",
      " |  get(self, key, default=None, /)\n",
      " |      Return the value for key if key is in the dictionary, else default.\n",
      " |  \n",
      " |  items(...)\n",
      " |      D.items() -> a set-like object providing a view on D's items\n",
      " |  \n",
      " |  keys(...)\n",
      " |      D.keys() -> a set-like object providing a view on D's keys\n",
      " |  \n",
      " |  pop(...)\n",
      " |      D.pop(k[,d]) -> v, remove specified key and return the corresponding value.\n",
      " |      If key is not found, d is returned if given, otherwise KeyError is raised\n",
      " |  \n",
      " |  popitem(...)\n",
      " |      D.popitem() -> (k, v), remove and return some (key, value) pair as a\n",
      " |      2-tuple; but raise KeyError if D is empty.\n",
      " |  \n",
      " |  setdefault(self, key, default=None, /)\n",
      " |      Insert key with a value of default if key is not in the dictionary.\n",
      " |      \n",
      " |      Return the value for key if key is in the dictionary, else default.\n",
      " |  \n",
      " |  update(...)\n",
      " |      D.update([E, ]**F) -> None.  Update D from dict/iterable E and F.\n",
      " |      If E is present and has a .keys() method, then does:  for k in E: D[k] = E[k]\n",
      " |      If E is present and lacks a .keys() method, then does:  for k, v in E: D[k] = v\n",
      " |      In either case, this is followed by: for k in F:  D[k] = F[k]\n",
      " |  \n",
      " |  values(...)\n",
      " |      D.values() -> an object providing a view on D's values\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods defined here:\n",
      " |  \n",
      " |  fromkeys(iterable, value=None, /) from builtins.type\n",
      " |      Create a new dictionary with keys from iterable and values set to value.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Static methods defined here:\n",
      " |  \n",
      " |  __new__(*args, **kwargs) from builtins.type\n",
      " |      Create and return a new object.  See help(type) for accurate signature.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes defined here:\n",
      " |  \n",
      " |  __hash__ = None\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Paavo Nurmi', 9),\n",
       " ('Carl Lewis', 9),\n",
       " ('Usain Bolt', 9),\n",
       " ('Ray Ewry', 8),\n",
       " ('Allyson Felix', 6)]"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "medals = [medal(*line.split(\"\\t\")) for line in open('goldmedals.txt', 'r')]\n",
    "c = cls.Counter(medal.athelete for medal in medals)\n",
    "c.most_common(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Challenge # 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class set in module builtins:\n",
      "\n",
      "class set(object)\n",
      " |  set() -> new empty set object\n",
      " |  set(iterable) -> new set object\n",
      " |  \n",
      " |  Build an unordered collection of unique elements.\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __and__(self, value, /)\n",
      " |      Return self&value.\n",
      " |  \n",
      " |  __contains__(...)\n",
      " |      x.__contains__(y) <==> y in x.\n",
      " |  \n",
      " |  __eq__(self, value, /)\n",
      " |      Return self==value.\n",
      " |  \n",
      " |  __ge__(self, value, /)\n",
      " |      Return self>=value.\n",
      " |  \n",
      " |  __getattribute__(self, name, /)\n",
      " |      Return getattr(self, name).\n",
      " |  \n",
      " |  __gt__(self, value, /)\n",
      " |      Return self>value.\n",
      " |  \n",
      " |  __iand__(self, value, /)\n",
      " |      Return self&=value.\n",
      " |  \n",
      " |  __init__(self, /, *args, **kwargs)\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |  \n",
      " |  __ior__(self, value, /)\n",
      " |      Return self|=value.\n",
      " |  \n",
      " |  __isub__(self, value, /)\n",
      " |      Return self-=value.\n",
      " |  \n",
      " |  __iter__(self, /)\n",
      " |      Implement iter(self).\n",
      " |  \n",
      " |  __ixor__(self, value, /)\n",
      " |      Return self^=value.\n",
      " |  \n",
      " |  __le__(self, value, /)\n",
      " |      Return self<=value.\n",
      " |  \n",
      " |  __len__(self, /)\n",
      " |      Return len(self).\n",
      " |  \n",
      " |  __lt__(self, value, /)\n",
      " |      Return self<value.\n",
      " |  \n",
      " |  __ne__(self, value, /)\n",
      " |      Return self!=value.\n",
      " |  \n",
      " |  __or__(self, value, /)\n",
      " |      Return self|value.\n",
      " |  \n",
      " |  __rand__(self, value, /)\n",
      " |      Return value&self.\n",
      " |  \n",
      " |  __reduce__(...)\n",
      " |      Return state information for pickling.\n",
      " |  \n",
      " |  __repr__(self, /)\n",
      " |      Return repr(self).\n",
      " |  \n",
      " |  __ror__(self, value, /)\n",
      " |      Return value|self.\n",
      " |  \n",
      " |  __rsub__(self, value, /)\n",
      " |      Return value-self.\n",
      " |  \n",
      " |  __rxor__(self, value, /)\n",
      " |      Return value^self.\n",
      " |  \n",
      " |  __sizeof__(...)\n",
      " |      S.__sizeof__() -> size of S in memory, in bytes\n",
      " |  \n",
      " |  __sub__(self, value, /)\n",
      " |      Return self-value.\n",
      " |  \n",
      " |  __xor__(self, value, /)\n",
      " |      Return self^value.\n",
      " |  \n",
      " |  add(...)\n",
      " |      Add an element to a set.\n",
      " |      \n",
      " |      This has no effect if the element is already present.\n",
      " |  \n",
      " |  clear(...)\n",
      " |      Remove all elements from this set.\n",
      " |  \n",
      " |  copy(...)\n",
      " |      Return a shallow copy of a set.\n",
      " |  \n",
      " |  difference(...)\n",
      " |      Return the difference of two or more sets as a new set.\n",
      " |      \n",
      " |      (i.e. all elements that are in this set but not the others.)\n",
      " |  \n",
      " |  difference_update(...)\n",
      " |      Remove all elements of another set from this set.\n",
      " |  \n",
      " |  discard(...)\n",
      " |      Remove an element from a set if it is a member.\n",
      " |      \n",
      " |      If the element is not a member, do nothing.\n",
      " |  \n",
      " |  intersection(...)\n",
      " |      Return the intersection of two sets as a new set.\n",
      " |      \n",
      " |      (i.e. all elements that are in both sets.)\n",
      " |  \n",
      " |  intersection_update(...)\n",
      " |      Update a set with the intersection of itself and another.\n",
      " |  \n",
      " |  isdisjoint(...)\n",
      " |      Return True if two sets have a null intersection.\n",
      " |  \n",
      " |  issubset(...)\n",
      " |      Report whether another set contains this set.\n",
      " |  \n",
      " |  issuperset(...)\n",
      " |      Report whether this set contains another set.\n",
      " |  \n",
      " |  pop(...)\n",
      " |      Remove and return an arbitrary set element.\n",
      " |      Raises KeyError if the set is empty.\n",
      " |  \n",
      " |  remove(...)\n",
      " |      Remove an element from a set; it must be a member.\n",
      " |      \n",
      " |      If the element is not a member, raise a KeyError.\n",
      " |  \n",
      " |  symmetric_difference(...)\n",
      " |      Return the symmetric difference of two sets as a new set.\n",
      " |      \n",
      " |      (i.e. all elements that are in exactly one of the sets.)\n",
      " |  \n",
      " |  symmetric_difference_update(...)\n",
      " |      Update a set with the symmetric difference of itself and another.\n",
      " |  \n",
      " |  union(...)\n",
      " |      Return the union of sets as a new set.\n",
      " |      \n",
      " |      (i.e. all elements that are in either set.)\n",
      " |  \n",
      " |  update(...)\n",
      " |      Update a set with the union of itself and others.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Static methods defined here:\n",
      " |  \n",
      " |  __new__(*args, **kwargs) from builtins.type\n",
      " |      Create and return a new object.  See help(type) for accurate signature.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes defined here:\n",
      " |  \n",
      " |  __hash__ = None\n",
      "\n"
     ]
    }
   ],
   "source": [
    "s = set()\n",
    "help(set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = set([medal.event for medal in medals])\n",
    "dct = cls.defaultdict(int)\n",
    "for event in k:\n",
    "    for medal in medals:\n",
    "        dct[medal.athelete] += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting gensim\n",
      "  Using cached gensim-3.8.3-cp37-cp37m-macosx_10_9_x86_64.whl (24.2 MB)\n",
      "Requirement already satisfied: six>=1.5.0 in /opt/anaconda3/lib/python3.7/site-packages (from gensim) (1.14.0)\n",
      "Requirement already satisfied: numpy>=1.11.3 in /opt/anaconda3/lib/python3.7/site-packages (from gensim) (1.18.1)\n",
      "Requirement already satisfied: scipy>=0.18.1 in /opt/anaconda3/lib/python3.7/site-packages (from gensim) (1.4.1)\n",
      "Processing /Users/prajjwaldangal/Library/Caches/pip/wheels/56/b5/6d/86dbe4f29d4688e5163a8b8c6b740494310040286fca4dc648/smart_open-2.1.0-py3-none-any.whl\n",
      "Collecting boto3\n",
      "  Using cached boto3-1.14.48-py2.py3-none-any.whl (129 kB)\n",
      "Requirement already satisfied: boto in /opt/anaconda3/lib/python3.7/site-packages (from smart-open>=1.8.1->gensim) (2.49.0)\n",
      "Requirement already satisfied: requests in /opt/anaconda3/lib/python3.7/site-packages (from smart-open>=1.8.1->gensim) (2.22.0)\n",
      "Collecting s3transfer<0.4.0,>=0.3.0\n",
      "  Using cached s3transfer-0.3.3-py2.py3-none-any.whl (69 kB)\n",
      "Collecting jmespath<1.0.0,>=0.7.1\n",
      "  Using cached jmespath-0.10.0-py2.py3-none-any.whl (24 kB)\n",
      "Collecting botocore<1.18.0,>=1.17.48\n",
      "  Using cached botocore-1.17.48-py2.py3-none-any.whl (6.5 MB)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /opt/anaconda3/lib/python3.7/site-packages (from requests->smart-open>=1.8.1->gensim) (1.25.8)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /opt/anaconda3/lib/python3.7/site-packages (from requests->smart-open>=1.8.1->gensim) (3.0.4)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in /opt/anaconda3/lib/python3.7/site-packages (from requests->smart-open>=1.8.1->gensim) (2.8)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/lib/python3.7/site-packages (from requests->smart-open>=1.8.1->gensim) (2019.11.28)\n",
      "Collecting docutils<0.16,>=0.10\n",
      "  Using cached docutils-0.15.2-py3-none-any.whl (547 kB)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /opt/anaconda3/lib/python3.7/site-packages (from botocore<1.18.0,>=1.17.48->boto3->smart-open>=1.8.1->gensim) (2.8.1)\n",
      "Installing collected packages: docutils, jmespath, botocore, s3transfer, boto3, smart-open, gensim\n",
      "  Attempting uninstall: docutils\n",
      "    Found existing installation: docutils 0.16\n",
      "    Uninstalling docutils-0.16:\n",
      "      Successfully uninstalled docutils-0.16\n",
      "Successfully installed boto3-1.14.48 botocore-1.17.48 docutils-0.15.2 gensim-3.8.3 jmespath-0.10.0 s3transfer-0.3.3 smart-open-2.1.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on module gensim.models.word2vec in gensim.models:\n",
      "\n",
      "NAME\n",
      "    gensim.models.word2vec\n",
      "\n",
      "DESCRIPTION\n",
      "    This module implements the word2vec family of algorithms, using highly optimized C routines,\n",
      "    data streaming and Pythonic interfaces.\n",
      "    \n",
      "    The word2vec algorithms include skip-gram and CBOW models, using either\n",
      "    hierarchical softmax or negative sampling: `Tomas Mikolov et al: Efficient Estimation of Word Representations\n",
      "    in Vector Space <https://arxiv.org/pdf/1301.3781.pdf>`_, `Tomas Mikolov et al: Distributed Representations of Words\n",
      "    and Phrases and their Compositionality <https://arxiv.org/abs/1310.4546>`_.\n",
      "    \n",
      "    Other embeddings\n",
      "    ================\n",
      "    \n",
      "    There are more ways to train word vectors in Gensim than just Word2Vec.\n",
      "    See also :class:`~gensim.models.doc2vec.Doc2Vec`, :class:`~gensim.models.fasttext.FastText` and\n",
      "    wrappers for :class:`~gensim.models.wrappers.VarEmbed` and :class:`~gensim.models.wrappers.WordRank`.\n",
      "    \n",
      "    The training algorithms were originally ported from the C package https://code.google.com/p/word2vec/\n",
      "    and extended with additional functionality and optimizations over the years.\n",
      "    \n",
      "    For a tutorial on Gensim word2vec, with an interactive web app trained on GoogleNews,\n",
      "    visit https://rare-technologies.com/word2vec-tutorial/.\n",
      "    \n",
      "    **Make sure you have a C compiler before installing Gensim, to use the optimized word2vec routines**\n",
      "    (70x speedup compared to plain NumPy implementation, https://rare-technologies.com/parallelizing-word2vec-in-python/).\n",
      "    \n",
      "    Usage examples\n",
      "    ==============\n",
      "    \n",
      "    Initialize a model with e.g.:\n",
      "    \n",
      "    .. sourcecode:: pycon\n",
      "    \n",
      "        >>> from gensim.test.utils import common_texts, get_tmpfile\n",
      "        >>> from gensim.models import Word2Vec\n",
      "        >>>\n",
      "        >>> path = get_tmpfile(\"word2vec.model\")\n",
      "        >>>\n",
      "        >>> model = Word2Vec(common_texts, size=100, window=5, min_count=1, workers=4)\n",
      "        >>> model.save(\"word2vec.model\")\n",
      "    \n",
      "    The training is streamed, meaning `sentences` can be a generator, reading input data\n",
      "    from disk on-the-fly, without loading the entire corpus into RAM.\n",
      "    \n",
      "    It also means you can continue training the model later:\n",
      "    \n",
      "    .. sourcecode:: pycon\n",
      "    \n",
      "        >>> model = Word2Vec.load(\"word2vec.model\")\n",
      "        >>> model.train([[\"hello\", \"world\"]], total_examples=1, epochs=1)\n",
      "        (0, 2)\n",
      "    \n",
      "    The trained word vectors are stored in a :class:`~gensim.models.keyedvectors.KeyedVectors` instance in `model.wv`:\n",
      "    \n",
      "    .. sourcecode:: pycon\n",
      "    \n",
      "        >>> vector = model.wv['computer']  # numpy vector of a word\n",
      "    \n",
      "    The reason for separating the trained vectors into `KeyedVectors` is that if you don't\n",
      "    need the full model state any more (don't need to continue training), the state can discarded,\n",
      "    resulting in a much smaller and faster object that can be mmapped for lightning\n",
      "    fast loading and sharing the vectors in RAM between processes:\n",
      "    \n",
      "    .. sourcecode:: pycon\n",
      "    \n",
      "        >>> from gensim.models import KeyedVectors\n",
      "        >>>\n",
      "        >>> path = get_tmpfile(\"wordvectors.kv\")\n",
      "        >>>\n",
      "        >>> model.wv.save(path)\n",
      "        >>> wv = KeyedVectors.load(\"model.wv\", mmap='r')\n",
      "        >>> vector = wv['computer']  # numpy vector of a word\n",
      "    \n",
      "    Gensim can also load word vectors in the \"word2vec C format\", as a\n",
      "    :class:`~gensim.models.keyedvectors.KeyedVectors` instance:\n",
      "    \n",
      "    .. sourcecode:: pycon\n",
      "    \n",
      "        >>> from gensim.test.utils import datapath\n",
      "        >>>\n",
      "        >>> wv_from_text = KeyedVectors.load_word2vec_format(datapath('word2vec_pre_kv_c'), binary=False)  # C text format\n",
      "        >>> wv_from_bin = KeyedVectors.load_word2vec_format(datapath(\"euclidean_vectors.bin\"), binary=True)  # C bin format\n",
      "    \n",
      "    It is impossible to continue training the vectors loaded from the C format because the hidden weights,\n",
      "    vocabulary frequencies and the binary tree are missing. To continue training, you'll need the\n",
      "    full :class:`~gensim.models.word2vec.Word2Vec` object state, as stored by :meth:`~gensim.models.word2vec.Word2Vec.save`,\n",
      "    not just the :class:`~gensim.models.keyedvectors.KeyedVectors`.\n",
      "    \n",
      "    You can perform various NLP word tasks with a trained model. Some of them\n",
      "    are already built-in - you can see it in :mod:`gensim.models.keyedvectors`.\n",
      "    \n",
      "    If you're finished training a model (i.e. no more updates, only querying),\n",
      "    you can switch to the :class:`~gensim.models.keyedvectors.KeyedVectors` instance:\n",
      "    \n",
      "    .. sourcecode:: pycon\n",
      "    \n",
      "        >>> word_vectors = model.wv\n",
      "        >>> del model\n",
      "    \n",
      "    to trim unneeded model state = use much less RAM and allow fast loading and memory sharing (mmap).\n",
      "    \n",
      "    Note that there is a :mod:`gensim.models.phrases` module which lets you automatically\n",
      "    detect phrases longer than one word. Using phrases, you can learn a word2vec model\n",
      "    where \"words\" are actually multiword expressions, such as `new_york_times` or `financial_crisis`:\n",
      "    \n",
      "    .. sourcecode:: pycon\n",
      "    \n",
      "        >>> from gensim.test.utils import common_texts\n",
      "        >>> from gensim.models import Phrases\n",
      "        >>>\n",
      "        >>> bigram_transformer = Phrases(common_texts)\n",
      "        >>> model = Word2Vec(bigram_transformer[common_texts], min_count=1)\n",
      "\n",
      "CLASSES\n",
      "    builtins.object\n",
      "        BrownCorpus\n",
      "        LineSentence\n",
      "        PathLineSentences\n",
      "        Text8Corpus\n",
      "    gensim.models.base_any2vec.BaseWordEmbeddingsModel(gensim.models.base_any2vec.BaseAny2VecModel)\n",
      "        Word2Vec\n",
      "    gensim.utils.SaveLoad(builtins.object)\n",
      "        Word2VecTrainables\n",
      "        Word2VecVocab\n",
      "    \n",
      "    class BrownCorpus(builtins.object)\n",
      "     |  BrownCorpus(dirname)\n",
      "     |  \n",
      "     |  Iterate over sentences from the `Brown corpus <https://en.wikipedia.org/wiki/Brown_Corpus>`_\n",
      "     |  (part of `NLTK data <https://www.nltk.org/data.html>`_).\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, dirname)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  __iter__(self)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors defined here:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "    \n",
      "    class LineSentence(builtins.object)\n",
      "     |  LineSentence(source, max_sentence_length=10000, limit=None)\n",
      "     |  \n",
      "     |  Iterate over a file that contains sentences: one line = one sentence.\n",
      "     |  Words must be already preprocessed and separated by whitespace.\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, source, max_sentence_length=10000, limit=None)\n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      source : string or a file-like object\n",
      "     |          Path to the file on disk, or an already-open file object (must support `seek(0)`).\n",
      "     |      limit : int or None\n",
      "     |          Clip the file to the first `limit` lines. Do no clipping if `limit is None` (the default).\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      .. sourcecode:: pycon\n",
      "     |      \n",
      "     |          >>> from gensim.test.utils import datapath\n",
      "     |          >>> sentences = LineSentence(datapath('lee_background.cor'))\n",
      "     |          >>> for sentence in sentences:\n",
      "     |          ...     pass\n",
      "     |  \n",
      "     |  __iter__(self)\n",
      "     |      Iterate through the lines in the source.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors defined here:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "    \n",
      "    class PathLineSentences(builtins.object)\n",
      "     |  PathLineSentences(source, max_sentence_length=10000, limit=None)\n",
      "     |  \n",
      "     |  Like :class:`~gensim.models.word2vec.LineSentence`, but process all files in a directory\n",
      "     |  in alphabetical order by filename.\n",
      "     |  \n",
      "     |  The directory must only contain files that can be read by :class:`gensim.models.word2vec.LineSentence`:\n",
      "     |  .bz2, .gz, and text files. Any file not ending with .bz2 or .gz is assumed to be a text file.\n",
      "     |  \n",
      "     |  The format of files (either text, or compressed text files) in the path is one sentence = one line,\n",
      "     |  with words already preprocessed and separated by whitespace.\n",
      "     |  \n",
      "     |  Warnings\n",
      "     |  --------\n",
      "     |  Does **not recurse** into subdirectories.\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, source, max_sentence_length=10000, limit=None)\n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      source : str\n",
      "     |          Path to the directory.\n",
      "     |      limit : int or None\n",
      "     |          Read only the first `limit` lines from each file. Read all if limit is None (the default).\n",
      "     |  \n",
      "     |  __iter__(self)\n",
      "     |      iterate through the files\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors defined here:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "    \n",
      "    class Text8Corpus(builtins.object)\n",
      "     |  Text8Corpus(fname, max_sentence_length=10000)\n",
      "     |  \n",
      "     |  Iterate over sentences from the \"text8\" corpus, unzipped from http://mattmahoney.net/dc/text8.zip.\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, fname, max_sentence_length=10000)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  __iter__(self)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors defined here:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "    \n",
      "    class Word2Vec(gensim.models.base_any2vec.BaseWordEmbeddingsModel)\n",
      "     |  Word2Vec(sentences=None, corpus_file=None, size=100, alpha=0.025, window=5, min_count=5, max_vocab_size=None, sample=0.001, seed=1, workers=3, min_alpha=0.0001, sg=0, hs=0, negative=5, ns_exponent=0.75, cbow_mean=1, hashfxn=<built-in function hash>, iter=5, null_word=0, trim_rule=None, sorted_vocab=1, batch_words=10000, compute_loss=False, callbacks=(), max_final_vocab=None)\n",
      "     |  \n",
      "     |  Train, use and evaluate neural networks described in https://code.google.com/p/word2vec/.\n",
      "     |  \n",
      "     |  Once you're finished training a model (=no more updates, only querying)\n",
      "     |  store and use only the :class:`~gensim.models.keyedvectors.KeyedVectors` instance in `self.wv` to reduce memory.\n",
      "     |  \n",
      "     |  The model can be stored/loaded via its :meth:`~gensim.models.word2vec.Word2Vec.save` and\n",
      "     |  :meth:`~gensim.models.word2vec.Word2Vec.load` methods.\n",
      "     |  \n",
      "     |  The trained word vectors can also be stored/loaded from a format compatible with the\n",
      "     |  original word2vec implementation via `self.wv.save_word2vec_format`\n",
      "     |  and :meth:`gensim.models.keyedvectors.KeyedVectors.load_word2vec_format`.\n",
      "     |  \n",
      "     |  Some important attributes are the following:\n",
      "     |  \n",
      "     |  Attributes\n",
      "     |  ----------\n",
      "     |  wv : :class:`~gensim.models.keyedvectors.Word2VecKeyedVectors`\n",
      "     |      This object essentially contains the mapping between words and embeddings. After training, it can be used\n",
      "     |      directly to query those embeddings in various ways. See the module level docstring for examples.\n",
      "     |  \n",
      "     |  vocabulary : :class:`~gensim.models.word2vec.Word2VecVocab`\n",
      "     |      This object represents the vocabulary (sometimes called Dictionary in gensim) of the model.\n",
      "     |      Besides keeping track of all unique words, this object provides extra functionality, such as\n",
      "     |      constructing a huffman tree (frequent words are closer to the root), or discarding extremely rare words.\n",
      "     |  \n",
      "     |  trainables : :class:`~gensim.models.word2vec.Word2VecTrainables`\n",
      "     |      This object represents the inner shallow neural network used to train the embeddings. The semantics of the\n",
      "     |      network differ slightly in the two available training modes (CBOW or SG) but you can think of it as a NN with\n",
      "     |      a single projection and hidden layer which we train on the corpus. The weights are then used as our embeddings\n",
      "     |      (which means that the size of the hidden layer is equal to the number of features `self.size`).\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      Word2Vec\n",
      "     |      gensim.models.base_any2vec.BaseWordEmbeddingsModel\n",
      "     |      gensim.models.base_any2vec.BaseAny2VecModel\n",
      "     |      gensim.utils.SaveLoad\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __contains__(self, word)\n",
      "     |      Deprecated. Use `self.wv.__contains__` instead.\n",
      "     |      Refer to the documentation for :meth:`~gensim.models.keyedvectors.Word2VecKeyedVectors.__contains__`.\n",
      "     |  \n",
      "     |  __getitem__(self, words)\n",
      "     |      Deprecated. Use `self.wv.__getitem__` instead.\n",
      "     |      Refer to the documentation for :meth:`~gensim.models.keyedvectors.Word2VecKeyedVectors.__getitem__`.\n",
      "     |  \n",
      "     |  __init__(self, sentences=None, corpus_file=None, size=100, alpha=0.025, window=5, min_count=5, max_vocab_size=None, sample=0.001, seed=1, workers=3, min_alpha=0.0001, sg=0, hs=0, negative=5, ns_exponent=0.75, cbow_mean=1, hashfxn=<built-in function hash>, iter=5, null_word=0, trim_rule=None, sorted_vocab=1, batch_words=10000, compute_loss=False, callbacks=(), max_final_vocab=None)\n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      sentences : iterable of iterables, optional\n",
      "     |          The `sentences` iterable can be simply a list of lists of tokens, but for larger corpora,\n",
      "     |          consider an iterable that streams the sentences directly from disk/network.\n",
      "     |          See :class:`~gensim.models.word2vec.BrownCorpus`, :class:`~gensim.models.word2vec.Text8Corpus`\n",
      "     |          or :class:`~gensim.models.word2vec.LineSentence` in :mod:`~gensim.models.word2vec` module for such examples.\n",
      "     |          See also the `tutorial on data streaming in Python\n",
      "     |          <https://rare-technologies.com/data-streaming-in-python-generators-iterators-iterables/>`_.\n",
      "     |          If you don't supply `sentences`, the model is left uninitialized -- use if you plan to initialize it\n",
      "     |          in some other way.\n",
      "     |      corpus_file : str, optional\n",
      "     |          Path to a corpus file in :class:`~gensim.models.word2vec.LineSentence` format.\n",
      "     |          You may use this argument instead of `sentences` to get performance boost. Only one of `sentences` or\n",
      "     |          `corpus_file` arguments need to be passed (or none of them, in that case, the model is left uninitialized).\n",
      "     |      size : int, optional\n",
      "     |          Dimensionality of the word vectors.\n",
      "     |      window : int, optional\n",
      "     |          Maximum distance between the current and predicted word within a sentence.\n",
      "     |      min_count : int, optional\n",
      "     |          Ignores all words with total frequency lower than this.\n",
      "     |      workers : int, optional\n",
      "     |          Use these many worker threads to train the model (=faster training with multicore machines).\n",
      "     |      sg : {0, 1}, optional\n",
      "     |          Training algorithm: 1 for skip-gram; otherwise CBOW.\n",
      "     |      hs : {0, 1}, optional\n",
      "     |          If 1, hierarchical softmax will be used for model training.\n",
      "     |          If 0, and `negative` is non-zero, negative sampling will be used.\n",
      "     |      negative : int, optional\n",
      "     |          If > 0, negative sampling will be used, the int for negative specifies how many \"noise words\"\n",
      "     |          should be drawn (usually between 5-20).\n",
      "     |          If set to 0, no negative sampling is used.\n",
      "     |      ns_exponent : float, optional\n",
      "     |          The exponent used to shape the negative sampling distribution. A value of 1.0 samples exactly in proportion\n",
      "     |          to the frequencies, 0.0 samples all words equally, while a negative value samples low-frequency words more\n",
      "     |          than high-frequency words. The popular default value of 0.75 was chosen by the original Word2Vec paper.\n",
      "     |          More recently, in https://arxiv.org/abs/1804.04212, Caselles-Dupré, Lesaint, & Royo-Letelier suggest that\n",
      "     |          other values may perform better for recommendation applications.\n",
      "     |      cbow_mean : {0, 1}, optional\n",
      "     |          If 0, use the sum of the context word vectors. If 1, use the mean, only applies when cbow is used.\n",
      "     |      alpha : float, optional\n",
      "     |          The initial learning rate.\n",
      "     |      min_alpha : float, optional\n",
      "     |          Learning rate will linearly drop to `min_alpha` as training progresses.\n",
      "     |      seed : int, optional\n",
      "     |          Seed for the random number generator. Initial vectors for each word are seeded with a hash of\n",
      "     |          the concatenation of word + `str(seed)`. Note that for a fully deterministically-reproducible run,\n",
      "     |          you must also limit the model to a single worker thread (`workers=1`), to eliminate ordering jitter\n",
      "     |          from OS thread scheduling. (In Python 3, reproducibility between interpreter launches also requires\n",
      "     |          use of the `PYTHONHASHSEED` environment variable to control hash randomization).\n",
      "     |      max_vocab_size : int, optional\n",
      "     |          Limits the RAM during vocabulary building; if there are more unique\n",
      "     |          words than this, then prune the infrequent ones. Every 10 million word types need about 1GB of RAM.\n",
      "     |          Set to `None` for no limit.\n",
      "     |      max_final_vocab : int, optional\n",
      "     |          Limits the vocab to a target vocab size by automatically picking a matching min_count. If the specified\n",
      "     |          min_count is more than the calculated min_count, the specified min_count will be used.\n",
      "     |          Set to `None` if not required.\n",
      "     |      sample : float, optional\n",
      "     |          The threshold for configuring which higher-frequency words are randomly downsampled,\n",
      "     |          useful range is (0, 1e-5).\n",
      "     |      hashfxn : function, optional\n",
      "     |          Hash function to use to randomly initialize weights, for increased training reproducibility.\n",
      "     |      iter : int, optional\n",
      "     |          Number of iterations (epochs) over the corpus.\n",
      "     |      trim_rule : function, optional\n",
      "     |          Vocabulary trimming rule, specifies whether certain words should remain in the vocabulary,\n",
      "     |          be trimmed away, or handled using the default (discard if word count < min_count).\n",
      "     |          Can be None (min_count will be used, look to :func:`~gensim.utils.keep_vocab_item`),\n",
      "     |          or a callable that accepts parameters (word, count, min_count) and returns either\n",
      "     |          :attr:`gensim.utils.RULE_DISCARD`, :attr:`gensim.utils.RULE_KEEP` or :attr:`gensim.utils.RULE_DEFAULT`.\n",
      "     |          The rule, if given, is only used to prune vocabulary during build_vocab() and is not stored as part of the\n",
      "     |          model.\n",
      "     |      \n",
      "     |          The input parameters are of the following types:\n",
      "     |              * `word` (str) - the word we are examining\n",
      "     |              * `count` (int) - the word's frequency count in the corpus\n",
      "     |              * `min_count` (int) - the minimum count threshold.\n",
      "     |      sorted_vocab : {0, 1}, optional\n",
      "     |          If 1, sort the vocabulary by descending frequency before assigning word indexes.\n",
      "     |          See :meth:`~gensim.models.word2vec.Word2VecVocab.sort_vocab()`.\n",
      "     |      batch_words : int, optional\n",
      "     |          Target size (in words) for batches of examples passed to worker threads (and\n",
      "     |          thus cython routines).(Larger batches will be passed if individual\n",
      "     |          texts are longer than 10000 words, but the standard cython code truncates to that maximum.)\n",
      "     |      compute_loss: bool, optional\n",
      "     |          If True, computes and stores loss value which can be retrieved using\n",
      "     |          :meth:`~gensim.models.word2vec.Word2Vec.get_latest_training_loss`.\n",
      "     |      callbacks : iterable of :class:`~gensim.models.callbacks.CallbackAny2Vec`, optional\n",
      "     |          Sequence of callbacks to be executed at specific stages during training.\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      Initialize and train a :class:`~gensim.models.word2vec.Word2Vec` model\n",
      "     |      \n",
      "     |      .. sourcecode:: pycon\n",
      "     |      \n",
      "     |          >>> from gensim.models import Word2Vec\n",
      "     |          >>> sentences = [[\"cat\", \"say\", \"meow\"], [\"dog\", \"say\", \"woof\"]]\n",
      "     |          >>> model = Word2Vec(sentences, min_count=1)\n",
      "     |  \n",
      "     |  __str__(self)\n",
      "     |      Human readable representation of the model's state.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      str\n",
      "     |          Human readable representation of the model's state, including the vocabulary size, vector size\n",
      "     |          and learning rate.\n",
      "     |  \n",
      "     |  accuracy(self, questions, restrict_vocab=30000, most_similar=None, case_insensitive=True)\n",
      "     |      Deprecated. Use `self.wv.accuracy` instead.\n",
      "     |      See :meth:`~gensim.models.word2vec.Word2VecKeyedVectors.accuracy`.\n",
      "     |  \n",
      "     |  clear_sims(self)\n",
      "     |      Remove all L2-normalized word vectors from the model, to free up memory.\n",
      "     |      \n",
      "     |      You can recompute them later again using the :meth:`~gensim.models.word2vec.Word2Vec.init_sims` method.\n",
      "     |  \n",
      "     |  delete_temporary_training_data(self, replace_word_vectors_with_normalized=False)\n",
      "     |      Discard parameters that are used in training and scoring, to save memory.\n",
      "     |      \n",
      "     |      Warnings\n",
      "     |      --------\n",
      "     |      Use only if you're sure you're done training a model.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      replace_word_vectors_with_normalized : bool, optional\n",
      "     |          If True, forget the original (not normalized) word vectors and only keep\n",
      "     |          the L2-normalized word vectors, to save even more memory.\n",
      "     |  \n",
      "     |  get_latest_training_loss(self)\n",
      "     |      Get current value of the training loss.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      float\n",
      "     |          Current training loss.\n",
      "     |  \n",
      "     |  init_sims(self, replace=False)\n",
      "     |      Deprecated. Use `self.wv.init_sims` instead.\n",
      "     |      See :meth:`~gensim.models.keyedvectors.Word2VecKeyedVectors.init_sims`.\n",
      "     |  \n",
      "     |  intersect_word2vec_format(self, fname, lockf=0.0, binary=False, encoding='utf8', unicode_errors='strict')\n",
      "     |      Merge in an input-hidden weight matrix loaded from the original C word2vec-tool format,\n",
      "     |      where it intersects with the current vocabulary.\n",
      "     |      \n",
      "     |      No words are added to the existing vocabulary, but intersecting words adopt the file's weights, and\n",
      "     |      non-intersecting words are left alone.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      fname : str\n",
      "     |          The file path to load the vectors from.\n",
      "     |      lockf : float, optional\n",
      "     |          Lock-factor value to be set for any imported word-vectors; the\n",
      "     |          default value of 0.0 prevents further updating of the vector during subsequent\n",
      "     |          training. Use 1.0 to allow further training updates of merged vectors.\n",
      "     |      binary : bool, optional\n",
      "     |          If True, `fname` is in the binary word2vec C format.\n",
      "     |      encoding : str, optional\n",
      "     |          Encoding of `text` for `unicode` function (python2 only).\n",
      "     |      unicode_errors : str, optional\n",
      "     |          Error handling behaviour, used as parameter for `unicode` function (python2 only).\n",
      "     |  \n",
      "     |  predict_output_word(self, context_words_list, topn=10)\n",
      "     |      Get the probability distribution of the center word given context words.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      context_words_list : list of str\n",
      "     |          List of context words.\n",
      "     |      topn : int, optional\n",
      "     |          Return `topn` words and their probabilities.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      list of (str, float)\n",
      "     |          `topn` length list of tuples of (word, probability).\n",
      "     |  \n",
      "     |  reset_from(self, other_model)\n",
      "     |      Borrow shareable pre-built structures from `other_model` and reset hidden layer weights.\n",
      "     |      \n",
      "     |      Structures copied are:\n",
      "     |          * Vocabulary\n",
      "     |          * Index to word mapping\n",
      "     |          * Cumulative frequency table (used for negative sampling)\n",
      "     |          * Cached corpus length\n",
      "     |      \n",
      "     |      Useful when testing multiple models on the same corpus in parallel.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      other_model : :class:`~gensim.models.word2vec.Word2Vec`\n",
      "     |          Another model to copy the internal structures from.\n",
      "     |  \n",
      "     |  save(self, *args, **kwargs)\n",
      "     |      Save the model.\n",
      "     |      This saved model can be loaded again using :func:`~gensim.models.word2vec.Word2Vec.load`, which supports\n",
      "     |      online training and getting vectors for vocabulary words.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      fname : str\n",
      "     |          Path to the file.\n",
      "     |  \n",
      "     |  save_word2vec_format(self, fname, fvocab=None, binary=False)\n",
      "     |      Deprecated. Use `model.wv.save_word2vec_format` instead.\n",
      "     |      See :meth:`gensim.models.KeyedVectors.save_word2vec_format`.\n",
      "     |  \n",
      "     |  score(self, sentences, total_sentences=1000000, chunksize=100, queue_factor=2, report_delay=1)\n",
      "     |      Score the log probability for a sequence of sentences.\n",
      "     |      This does not change the fitted model in any way (see :meth:`~gensim.models.word2vec.Word2Vec.train` for that).\n",
      "     |      \n",
      "     |      Gensim has currently only implemented score for the hierarchical softmax scheme,\n",
      "     |      so you need to have run word2vec with `hs=1` and `negative=0` for this to work.\n",
      "     |      \n",
      "     |      Note that you should specify `total_sentences`; you'll run into problems if you ask to\n",
      "     |      score more than this number of sentences but it is inefficient to set the value too high.\n",
      "     |      \n",
      "     |      See the `article by Matt Taddy: \"Document Classification by Inversion of Distributed Language Representations\"\n",
      "     |      <https://arxiv.org/pdf/1504.07295.pdf>`_ and the\n",
      "     |      `gensim demo <https://github.com/piskvorky/gensim/blob/develop/docs/notebooks/deepir.ipynb>`_ for examples of\n",
      "     |      how to use such scores in document classification.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      sentences : iterable of list of str\n",
      "     |          The `sentences` iterable can be simply a list of lists of tokens, but for larger corpora,\n",
      "     |          consider an iterable that streams the sentences directly from disk/network.\n",
      "     |          See :class:`~gensim.models.word2vec.BrownCorpus`, :class:`~gensim.models.word2vec.Text8Corpus`\n",
      "     |          or :class:`~gensim.models.word2vec.LineSentence` in :mod:`~gensim.models.word2vec` module for such examples.\n",
      "     |      total_sentences : int, optional\n",
      "     |          Count of sentences.\n",
      "     |      chunksize : int, optional\n",
      "     |          Chunksize of jobs\n",
      "     |      queue_factor : int, optional\n",
      "     |          Multiplier for size of queue (number of workers * queue_factor).\n",
      "     |      report_delay : float, optional\n",
      "     |          Seconds to wait before reporting progress.\n",
      "     |  \n",
      "     |  train(self, sentences=None, corpus_file=None, total_examples=None, total_words=None, epochs=None, start_alpha=None, end_alpha=None, word_count=0, queue_factor=2, report_delay=1.0, compute_loss=False, callbacks=())\n",
      "     |      Update the model's neural weights from a sequence of sentences.\n",
      "     |      \n",
      "     |      Notes\n",
      "     |      -----\n",
      "     |      To support linear learning-rate decay from (initial) `alpha` to `min_alpha`, and accurate\n",
      "     |      progress-percentage logging, either `total_examples` (count of sentences) or `total_words` (count of\n",
      "     |      raw words in sentences) **MUST** be provided. If `sentences` is the same corpus\n",
      "     |      that was provided to :meth:`~gensim.models.word2vec.Word2Vec.build_vocab` earlier,\n",
      "     |      you can simply use `total_examples=self.corpus_count`.\n",
      "     |      \n",
      "     |      Warnings\n",
      "     |      --------\n",
      "     |      To avoid common mistakes around the model's ability to do multiple training passes itself, an\n",
      "     |      explicit `epochs` argument **MUST** be provided. In the common and recommended case\n",
      "     |      where :meth:`~gensim.models.word2vec.Word2Vec.train` is only called once, you can set `epochs=self.iter`.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      sentences : iterable of list of str\n",
      "     |          The `sentences` iterable can be simply a list of lists of tokens, but for larger corpora,\n",
      "     |          consider an iterable that streams the sentences directly from disk/network.\n",
      "     |          See :class:`~gensim.models.word2vec.BrownCorpus`, :class:`~gensim.models.word2vec.Text8Corpus`\n",
      "     |          or :class:`~gensim.models.word2vec.LineSentence` in :mod:`~gensim.models.word2vec` module for such examples.\n",
      "     |          See also the `tutorial on data streaming in Python\n",
      "     |          <https://rare-technologies.com/data-streaming-in-python-generators-iterators-iterables/>`_.\n",
      "     |      corpus_file : str, optional\n",
      "     |          Path to a corpus file in :class:`~gensim.models.word2vec.LineSentence` format.\n",
      "     |          You may use this argument instead of `sentences` to get performance boost. Only one of `sentences` or\n",
      "     |          `corpus_file` arguments need to be passed (not both of them).\n",
      "     |      total_examples : int\n",
      "     |          Count of sentences.\n",
      "     |      total_words : int\n",
      "     |          Count of raw words in sentences.\n",
      "     |      epochs : int\n",
      "     |          Number of iterations (epochs) over the corpus.\n",
      "     |      start_alpha : float, optional\n",
      "     |          Initial learning rate. If supplied, replaces the starting `alpha` from the constructor,\n",
      "     |          for this one call to`train()`.\n",
      "     |          Use only if making multiple calls to `train()`, when you want to manage the alpha learning-rate yourself\n",
      "     |          (not recommended).\n",
      "     |      end_alpha : float, optional\n",
      "     |          Final learning rate. Drops linearly from `start_alpha`.\n",
      "     |          If supplied, this replaces the final `min_alpha` from the constructor, for this one call to `train()`.\n",
      "     |          Use only if making multiple calls to `train()`, when you want to manage the alpha learning-rate yourself\n",
      "     |          (not recommended).\n",
      "     |      word_count : int, optional\n",
      "     |          Count of words already trained. Set this to 0 for the usual\n",
      "     |          case of training on all words in sentences.\n",
      "     |      queue_factor : int, optional\n",
      "     |          Multiplier for size of queue (number of workers * queue_factor).\n",
      "     |      report_delay : float, optional\n",
      "     |          Seconds to wait before reporting progress.\n",
      "     |      compute_loss: bool, optional\n",
      "     |          If True, computes and stores loss value which can be retrieved using\n",
      "     |          :meth:`~gensim.models.word2vec.Word2Vec.get_latest_training_loss`.\n",
      "     |      callbacks : iterable of :class:`~gensim.models.callbacks.CallbackAny2Vec`, optional\n",
      "     |          Sequence of callbacks to be executed at specific stages during training.\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      .. sourcecode:: pycon\n",
      "     |      \n",
      "     |          >>> from gensim.models import Word2Vec\n",
      "     |          >>> sentences = [[\"cat\", \"say\", \"meow\"], [\"dog\", \"say\", \"woof\"]]\n",
      "     |          >>>\n",
      "     |          >>> model = Word2Vec(min_count=1)\n",
      "     |          >>> model.build_vocab(sentences)  # prepare the model vocabulary\n",
      "     |          >>> model.train(sentences, total_examples=model.corpus_count, epochs=model.iter)  # train word vectors\n",
      "     |          (1, 30)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods defined here:\n",
      "     |  \n",
      "     |  load(*args, **kwargs) from builtins.type\n",
      "     |      Load a previously saved :class:`~gensim.models.word2vec.Word2Vec` model.\n",
      "     |      \n",
      "     |      See Also\n",
      "     |      --------\n",
      "     |      :meth:`~gensim.models.word2vec.Word2Vec.save`\n",
      "     |          Save model.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      fname : str\n",
      "     |          Path to the saved file.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      :class:`~gensim.models.word2vec.Word2Vec`\n",
      "     |          Loaded model.\n",
      "     |  \n",
      "     |  load_word2vec_format(fname, fvocab=None, binary=False, encoding='utf8', unicode_errors='strict', limit=None, datatype=<class 'numpy.float32'>) from builtins.type\n",
      "     |      Deprecated. Use :meth:`gensim.models.KeyedVectors.load_word2vec_format` instead.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Static methods defined here:\n",
      "     |  \n",
      "     |  log_accuracy(section)\n",
      "     |      Deprecated. Use `self.wv.log_accuracy` instead.\n",
      "     |      See :meth:`~gensim.models.word2vec.Word2VecKeyedVectors.log_accuracy`.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from gensim.models.base_any2vec.BaseWordEmbeddingsModel:\n",
      "     |  \n",
      "     |  build_vocab(self, sentences=None, corpus_file=None, update=False, progress_per=10000, keep_raw_vocab=False, trim_rule=None, **kwargs)\n",
      "     |      Build vocabulary from a sequence of sentences (can be a once-only generator stream).\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      sentences : iterable of list of str\n",
      "     |          Can be simply a list of lists of tokens, but for larger corpora,\n",
      "     |          consider an iterable that streams the sentences directly from disk/network.\n",
      "     |          See :class:`~gensim.models.word2vec.BrownCorpus`, :class:`~gensim.models.word2vec.Text8Corpus`\n",
      "     |          or :class:`~gensim.models.word2vec.LineSentence` module for such examples.\n",
      "     |      corpus_file : str, optional\n",
      "     |          Path to a corpus file in :class:`~gensim.models.word2vec.LineSentence` format.\n",
      "     |          You may use this argument instead of `sentences` to get performance boost. Only one of `sentences` or\n",
      "     |          `corpus_file` arguments need to be passed (not both of them).\n",
      "     |      update : bool\n",
      "     |          If true, the new words in `sentences` will be added to model's vocab.\n",
      "     |      progress_per : int, optional\n",
      "     |          Indicates how many words to process before showing/updating the progress.\n",
      "     |      keep_raw_vocab : bool, optional\n",
      "     |          If False, the raw vocabulary will be deleted after the scaling is done to free up RAM.\n",
      "     |      trim_rule : function, optional\n",
      "     |          Vocabulary trimming rule, specifies whether certain words should remain in the vocabulary,\n",
      "     |          be trimmed away, or handled using the default (discard if word count < min_count).\n",
      "     |          Can be None (min_count will be used, look to :func:`~gensim.utils.keep_vocab_item`),\n",
      "     |          or a callable that accepts parameters (word, count, min_count) and returns either\n",
      "     |          :attr:`gensim.utils.RULE_DISCARD`, :attr:`gensim.utils.RULE_KEEP` or :attr:`gensim.utils.RULE_DEFAULT`.\n",
      "     |          The rule, if given, is only used to prune vocabulary during current method call and is not stored as part\n",
      "     |          of the model.\n",
      "     |      \n",
      "     |          The input parameters are of the following types:\n",
      "     |              * `word` (str) - the word we are examining\n",
      "     |              * `count` (int) - the word's frequency count in the corpus\n",
      "     |              * `min_count` (int) - the minimum count threshold.\n",
      "     |      \n",
      "     |      **kwargs : object\n",
      "     |          Key word arguments propagated to `self.vocabulary.prepare_vocab`\n",
      "     |  \n",
      "     |  build_vocab_from_freq(self, word_freq, keep_raw_vocab=False, corpus_count=None, trim_rule=None, update=False)\n",
      "     |      Build vocabulary from a dictionary of word frequencies.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      word_freq : dict of (str, int)\n",
      "     |          A mapping from a word in the vocabulary to its frequency count.\n",
      "     |      keep_raw_vocab : bool, optional\n",
      "     |          If False, delete the raw vocabulary after the scaling is done to free up RAM.\n",
      "     |      corpus_count : int, optional\n",
      "     |          Even if no corpus is provided, this argument can set corpus_count explicitly.\n",
      "     |      trim_rule : function, optional\n",
      "     |          Vocabulary trimming rule, specifies whether certain words should remain in the vocabulary,\n",
      "     |          be trimmed away, or handled using the default (discard if word count < min_count).\n",
      "     |          Can be None (min_count will be used, look to :func:`~gensim.utils.keep_vocab_item`),\n",
      "     |          or a callable that accepts parameters (word, count, min_count) and returns either\n",
      "     |          :attr:`gensim.utils.RULE_DISCARD`, :attr:`gensim.utils.RULE_KEEP` or :attr:`gensim.utils.RULE_DEFAULT`.\n",
      "     |          The rule, if given, is only used to prune vocabulary during current method call and is not stored as part\n",
      "     |          of the model.\n",
      "     |      \n",
      "     |          The input parameters are of the following types:\n",
      "     |              * `word` (str) - the word we are examining\n",
      "     |              * `count` (int) - the word's frequency count in the corpus\n",
      "     |              * `min_count` (int) - the minimum count threshold.\n",
      "     |      \n",
      "     |      update : bool, optional\n",
      "     |          If true, the new provided words in `word_freq` dict will be added to model's vocab.\n",
      "     |  \n",
      "     |  doesnt_match(self, words)\n",
      "     |      Deprecated, use self.wv.doesnt_match() instead.\n",
      "     |      \n",
      "     |      Refer to the documentation for :meth:`~gensim.models.keyedvectors.WordEmbeddingsKeyedVectors.doesnt_match`.\n",
      "     |  \n",
      "     |  estimate_memory(self, vocab_size=None, report=None)\n",
      "     |      Estimate required memory for a model using current settings and provided vocabulary size.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      vocab_size : int, optional\n",
      "     |          Number of unique tokens in the vocabulary\n",
      "     |      report : dict of (str, int), optional\n",
      "     |          A dictionary from string representations of the model's memory consuming members to their size in bytes.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      dict of (str, int)\n",
      "     |          A dictionary from string representations of the model's memory consuming members to their size in bytes.\n",
      "     |  \n",
      "     |  evaluate_word_pairs(self, pairs, delimiter='\\t', restrict_vocab=300000, case_insensitive=True, dummy4unknown=False)\n",
      "     |      Deprecated, use self.wv.evaluate_word_pairs() instead.\n",
      "     |      \n",
      "     |      Refer to the documentation for\n",
      "     |      :meth:`~gensim.models.keyedvectors.WordEmbeddingsKeyedVectors.evaluate_word_pairs`.\n",
      "     |  \n",
      "     |  most_similar(self, positive=None, negative=None, topn=10, restrict_vocab=None, indexer=None)\n",
      "     |      Deprecated, use self.wv.most_similar() instead.\n",
      "     |      \n",
      "     |      Refer to the documentation for :meth:`~gensim.models.keyedvectors.WordEmbeddingsKeyedVectors.most_similar`.\n",
      "     |  \n",
      "     |  most_similar_cosmul(self, positive=None, negative=None, topn=10)\n",
      "     |      Deprecated, use self.wv.most_similar_cosmul() instead.\n",
      "     |      \n",
      "     |      Refer to the documentation for\n",
      "     |      :meth:`~gensim.models.keyedvectors.WordEmbeddingsKeyedVectors.most_similar_cosmul`.\n",
      "     |  \n",
      "     |  n_similarity(self, ws1, ws2)\n",
      "     |      Deprecated, use self.wv.n_similarity() instead.\n",
      "     |      \n",
      "     |      Refer to the documentation for :meth:`~gensim.models.keyedvectors.WordEmbeddingsKeyedVectors.n_similarity`.\n",
      "     |  \n",
      "     |  similar_by_vector(self, vector, topn=10, restrict_vocab=None)\n",
      "     |      Deprecated, use self.wv.similar_by_vector() instead.\n",
      "     |      \n",
      "     |      Refer to the documentation for :meth:`~gensim.models.keyedvectors.WordEmbeddingsKeyedVectors.similar_by_vector`.\n",
      "     |  \n",
      "     |  similar_by_word(self, word, topn=10, restrict_vocab=None)\n",
      "     |      Deprecated, use self.wv.similar_by_word() instead.\n",
      "     |      \n",
      "     |      Refer to the documentation for :meth:`~gensim.models.keyedvectors.WordEmbeddingsKeyedVectors.similar_by_word`.\n",
      "     |  \n",
      "     |  similarity(self, w1, w2)\n",
      "     |      Deprecated, use self.wv.similarity() instead.\n",
      "     |      \n",
      "     |      Refer to the documentation for :meth:`~gensim.models.keyedvectors.WordEmbeddingsKeyedVectors.similarity`.\n",
      "     |  \n",
      "     |  wmdistance(self, document1, document2)\n",
      "     |      Deprecated, use self.wv.wmdistance() instead.\n",
      "     |      \n",
      "     |      Refer to the documentation for :meth:`~gensim.models.keyedvectors.WordEmbeddingsKeyedVectors.wmdistance`.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from gensim.models.base_any2vec.BaseWordEmbeddingsModel:\n",
      "     |  \n",
      "     |  cum_table\n",
      "     |  \n",
      "     |  hashfxn\n",
      "     |  \n",
      "     |  iter\n",
      "     |  \n",
      "     |  layer1_size\n",
      "     |  \n",
      "     |  min_count\n",
      "     |  \n",
      "     |  sample\n",
      "     |  \n",
      "     |  syn0_lockf\n",
      "     |  \n",
      "     |  syn1\n",
      "     |  \n",
      "     |  syn1neg\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from gensim.utils.SaveLoad:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "    \n",
      "    class Word2VecTrainables(gensim.utils.SaveLoad)\n",
      "     |  Word2VecTrainables(vector_size=100, seed=1, hashfxn=<built-in function hash>)\n",
      "     |  \n",
      "     |  Represents the inner shallow neural network used to train :class:`~gensim.models.word2vec.Word2Vec`.\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      Word2VecTrainables\n",
      "     |      gensim.utils.SaveLoad\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, vector_size=100, seed=1, hashfxn=<built-in function hash>)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  prepare_weights(self, hs, negative, wv, update=False, vocabulary=None)\n",
      "     |      Build tables and model weights based on final vocabulary settings.\n",
      "     |  \n",
      "     |  reset_weights(self, hs, negative, wv)\n",
      "     |      Reset all projection weights to an initial (untrained) state, but keep the existing vocabulary.\n",
      "     |  \n",
      "     |  seeded_vector(self, seed_string, vector_size)\n",
      "     |      Get a random vector (but deterministic by seed_string).\n",
      "     |  \n",
      "     |  update_weights(self, hs, negative, wv)\n",
      "     |      Copy all the existing weights, and reset the weights for the newly added vocabulary.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from gensim.utils.SaveLoad:\n",
      "     |  \n",
      "     |  save(self, fname_or_handle, separately=None, sep_limit=10485760, ignore=frozenset(), pickle_protocol=2)\n",
      "     |      Save the object to a file.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      fname_or_handle : str or file-like\n",
      "     |          Path to output file or already opened file-like object. If the object is a file handle,\n",
      "     |          no special array handling will be performed, all attributes will be saved to the same file.\n",
      "     |      separately : list of str or None, optional\n",
      "     |          If None, automatically detect large numpy/scipy.sparse arrays in the object being stored, and store\n",
      "     |          them into separate files. This prevent memory errors for large objects, and also allows\n",
      "     |          `memory-mapping <https://en.wikipedia.org/wiki/Mmap>`_ the large arrays for efficient\n",
      "     |          loading and sharing the large arrays in RAM between multiple processes.\n",
      "     |      \n",
      "     |          If list of str: store these attributes into separate files. The automated size check\n",
      "     |          is not performed in this case.\n",
      "     |      sep_limit : int, optional\n",
      "     |          Don't store arrays smaller than this separately. In bytes.\n",
      "     |      ignore : frozenset of str, optional\n",
      "     |          Attributes that shouldn't be stored at all.\n",
      "     |      pickle_protocol : int, optional\n",
      "     |          Protocol number for pickle.\n",
      "     |      \n",
      "     |      See Also\n",
      "     |      --------\n",
      "     |      :meth:`~gensim.utils.SaveLoad.load`\n",
      "     |          Load object from file.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods inherited from gensim.utils.SaveLoad:\n",
      "     |  \n",
      "     |  load(fname, mmap=None) from builtins.type\n",
      "     |      Load an object previously saved using :meth:`~gensim.utils.SaveLoad.save` from a file.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      fname : str\n",
      "     |          Path to file that contains needed object.\n",
      "     |      mmap : str, optional\n",
      "     |          Memory-map option.  If the object was saved with large arrays stored separately, you can load these arrays\n",
      "     |          via mmap (shared memory) using `mmap='r'.\n",
      "     |          If the file being loaded is compressed (either '.gz' or '.bz2'), then `mmap=None` **must be** set.\n",
      "     |      \n",
      "     |      See Also\n",
      "     |      --------\n",
      "     |      :meth:`~gensim.utils.SaveLoad.save`\n",
      "     |          Save object to file.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      object\n",
      "     |          Object loaded from `fname`.\n",
      "     |      \n",
      "     |      Raises\n",
      "     |      ------\n",
      "     |      AttributeError\n",
      "     |          When called on an object instance instead of class (this is a class method).\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from gensim.utils.SaveLoad:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "    \n",
      "    class Word2VecVocab(gensim.utils.SaveLoad)\n",
      "     |  Word2VecVocab(max_vocab_size=None, min_count=5, sample=0.001, sorted_vocab=True, null_word=0, max_final_vocab=None, ns_exponent=0.75)\n",
      "     |  \n",
      "     |  Vocabulary used by :class:`~gensim.models.word2vec.Word2Vec`.\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      Word2VecVocab\n",
      "     |      gensim.utils.SaveLoad\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, max_vocab_size=None, min_count=5, sample=0.001, sorted_vocab=True, null_word=0, max_final_vocab=None, ns_exponent=0.75)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  add_null_word(self, wv)\n",
      "     |  \n",
      "     |  create_binary_tree(self, wv)\n",
      "     |      Create a `binary Huffman tree <https://en.wikipedia.org/wiki/Huffman_coding>`_ using stored vocabulary\n",
      "     |      word counts. Frequent words will have shorter binary codes.\n",
      "     |      Called internally from :meth:`~gensim.models.word2vec.Word2VecVocab.build_vocab`.\n",
      "     |  \n",
      "     |  make_cum_table(self, wv, domain=2147483647)\n",
      "     |      Create a cumulative-distribution table using stored vocabulary word counts for\n",
      "     |      drawing random words in the negative-sampling training routines.\n",
      "     |      \n",
      "     |      To draw a word index, choose a random integer up to the maximum value in the table (cum_table[-1]),\n",
      "     |      then finding that integer's sorted insertion point (as if by `bisect_left` or `ndarray.searchsorted()`).\n",
      "     |      That insertion point is the drawn index, coming up in proportion equal to the increment at that slot.\n",
      "     |      \n",
      "     |      Called internally from :meth:`~gensim.models.word2vec.Word2VecVocab.build_vocab`.\n",
      "     |  \n",
      "     |  prepare_vocab(self, hs, negative, wv, update=False, keep_raw_vocab=False, trim_rule=None, min_count=None, sample=None, dry_run=False)\n",
      "     |      Apply vocabulary settings for `min_count` (discarding less-frequent words)\n",
      "     |      and `sample` (controlling the downsampling of more-frequent words).\n",
      "     |      \n",
      "     |      Calling with `dry_run=True` will only simulate the provided settings and\n",
      "     |      report the size of the retained vocabulary, effective corpus length, and\n",
      "     |      estimated memory requirements. Results are both printed via logging and\n",
      "     |      returned as a dict.\n",
      "     |      \n",
      "     |      Delete the raw vocabulary after the scaling is done to free up RAM,\n",
      "     |      unless `keep_raw_vocab` is set.\n",
      "     |  \n",
      "     |  scan_vocab(self, sentences=None, corpus_file=None, progress_per=10000, workers=None, trim_rule=None)\n",
      "     |  \n",
      "     |  sort_vocab(self, wv)\n",
      "     |      Sort the vocabulary so the most frequent words have the lowest indexes.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from gensim.utils.SaveLoad:\n",
      "     |  \n",
      "     |  save(self, fname_or_handle, separately=None, sep_limit=10485760, ignore=frozenset(), pickle_protocol=2)\n",
      "     |      Save the object to a file.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      fname_or_handle : str or file-like\n",
      "     |          Path to output file or already opened file-like object. If the object is a file handle,\n",
      "     |          no special array handling will be performed, all attributes will be saved to the same file.\n",
      "     |      separately : list of str or None, optional\n",
      "     |          If None, automatically detect large numpy/scipy.sparse arrays in the object being stored, and store\n",
      "     |          them into separate files. This prevent memory errors for large objects, and also allows\n",
      "     |          `memory-mapping <https://en.wikipedia.org/wiki/Mmap>`_ the large arrays for efficient\n",
      "     |          loading and sharing the large arrays in RAM between multiple processes.\n",
      "     |      \n",
      "     |          If list of str: store these attributes into separate files. The automated size check\n",
      "     |          is not performed in this case.\n",
      "     |      sep_limit : int, optional\n",
      "     |          Don't store arrays smaller than this separately. In bytes.\n",
      "     |      ignore : frozenset of str, optional\n",
      "     |          Attributes that shouldn't be stored at all.\n",
      "     |      pickle_protocol : int, optional\n",
      "     |          Protocol number for pickle.\n",
      "     |      \n",
      "     |      See Also\n",
      "     |      --------\n",
      "     |      :meth:`~gensim.utils.SaveLoad.load`\n",
      "     |          Load object from file.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods inherited from gensim.utils.SaveLoad:\n",
      "     |  \n",
      "     |  load(fname, mmap=None) from builtins.type\n",
      "     |      Load an object previously saved using :meth:`~gensim.utils.SaveLoad.save` from a file.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      fname : str\n",
      "     |          Path to file that contains needed object.\n",
      "     |      mmap : str, optional\n",
      "     |          Memory-map option.  If the object was saved with large arrays stored separately, you can load these arrays\n",
      "     |          via mmap (shared memory) using `mmap='r'.\n",
      "     |          If the file being loaded is compressed (either '.gz' or '.bz2'), then `mmap=None` **must be** set.\n",
      "     |      \n",
      "     |      See Also\n",
      "     |      --------\n",
      "     |      :meth:`~gensim.utils.SaveLoad.save`\n",
      "     |          Save object to file.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      object\n",
      "     |          Object loaded from `fname`.\n",
      "     |      \n",
      "     |      Raises\n",
      "     |      ------\n",
      "     |      AttributeError\n",
      "     |          When called on an object instance instead of class (this is a class method).\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from gensim.utils.SaveLoad:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "\n",
      "FUNCTIONS\n",
      "    array(...)\n",
      "        array(object, dtype=None, copy=True, order='K', subok=False, ndmin=0)\n",
      "        \n",
      "        Create an array.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        object : array_like\n",
      "            An array, any object exposing the array interface, an object whose\n",
      "            __array__ method returns an array, or any (nested) sequence.\n",
      "        dtype : data-type, optional\n",
      "            The desired data-type for the array.  If not given, then the type will\n",
      "            be determined as the minimum type required to hold the objects in the\n",
      "            sequence.\n",
      "        copy : bool, optional\n",
      "            If true (default), then the object is copied.  Otherwise, a copy will\n",
      "            only be made if __array__ returns a copy, if obj is a nested sequence,\n",
      "            or if a copy is needed to satisfy any of the other requirements\n",
      "            (`dtype`, `order`, etc.).\n",
      "        order : {'K', 'A', 'C', 'F'}, optional\n",
      "            Specify the memory layout of the array. If object is not an array, the\n",
      "            newly created array will be in C order (row major) unless 'F' is\n",
      "            specified, in which case it will be in Fortran order (column major).\n",
      "            If object is an array the following holds.\n",
      "        \n",
      "            ===== ========= ===================================================\n",
      "            order  no copy                     copy=True\n",
      "            ===== ========= ===================================================\n",
      "            'K'   unchanged F & C order preserved, otherwise most similar order\n",
      "            'A'   unchanged F order if input is F and not C, otherwise C order\n",
      "            'C'   C order   C order\n",
      "            'F'   F order   F order\n",
      "            ===== ========= ===================================================\n",
      "        \n",
      "            When ``copy=False`` and a copy is made for other reasons, the result is\n",
      "            the same as if ``copy=True``, with some exceptions for `A`, see the\n",
      "            Notes section. The default order is 'K'.\n",
      "        subok : bool, optional\n",
      "            If True, then sub-classes will be passed-through, otherwise\n",
      "            the returned array will be forced to be a base-class array (default).\n",
      "        ndmin : int, optional\n",
      "            Specifies the minimum number of dimensions that the resulting\n",
      "            array should have.  Ones will be pre-pended to the shape as\n",
      "            needed to meet this requirement.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        out : ndarray\n",
      "            An array object satisfying the specified requirements.\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        empty_like : Return an empty array with shape and type of input.\n",
      "        ones_like : Return an array of ones with shape and type of input.\n",
      "        zeros_like : Return an array of zeros with shape and type of input.\n",
      "        full_like : Return a new array with shape of input filled with value.\n",
      "        empty : Return a new uninitialized array.\n",
      "        ones : Return a new array setting values to one.\n",
      "        zeros : Return a new array setting values to zero.\n",
      "        full : Return a new array of given shape filled with value.\n",
      "        \n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        When order is 'A' and `object` is an array in neither 'C' nor 'F' order,\n",
      "        and a copy is forced by a change in dtype, then the order of the result is\n",
      "        not necessarily 'C' as expected. This is likely a bug.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> np.array([1, 2, 3])\n",
      "        array([1, 2, 3])\n",
      "        \n",
      "        Upcasting:\n",
      "        \n",
      "        >>> np.array([1, 2, 3.0])\n",
      "        array([ 1.,  2.,  3.])\n",
      "        \n",
      "        More than one dimension:\n",
      "        \n",
      "        >>> np.array([[1, 2], [3, 4]])\n",
      "        array([[1, 2],\n",
      "               [3, 4]])\n",
      "        \n",
      "        Minimum dimensions 2:\n",
      "        \n",
      "        >>> np.array([1, 2, 3], ndmin=2)\n",
      "        array([[1, 2, 3]])\n",
      "        \n",
      "        Type provided:\n",
      "        \n",
      "        >>> np.array([1, 2, 3], dtype=complex)\n",
      "        array([ 1.+0.j,  2.+0.j,  3.+0.j])\n",
      "        \n",
      "        Data-type consisting of more than one element:\n",
      "        \n",
      "        >>> x = np.array([(1,2),(3,4)],dtype=[('a','<i4'),('b','<i4')])\n",
      "        >>> x['a']\n",
      "        array([1, 3])\n",
      "        \n",
      "        Creating an array from sub-classes:\n",
      "        \n",
      "        >>> np.array(np.mat('1 2; 3 4'))\n",
      "        array([[1, 2],\n",
      "               [3, 4]])\n",
      "        \n",
      "        >>> np.array(np.mat('1 2; 3 4'), subok=True)\n",
      "        matrix([[1, 2],\n",
      "                [3, 4]])\n",
      "    \n",
      "    default_timer = perf_counter(...)\n",
      "        perf_counter() -> float\n",
      "        \n",
      "        Performance counter for benchmarking.\n",
      "    \n",
      "    empty(...)\n",
      "        empty(shape, dtype=float, order='C')\n",
      "        \n",
      "        Return a new array of given shape and type, without initializing entries.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        shape : int or tuple of int\n",
      "            Shape of the empty array, e.g., ``(2, 3)`` or ``2``.\n",
      "        dtype : data-type, optional\n",
      "            Desired output data-type for the array, e.g, `numpy.int8`. Default is\n",
      "            `numpy.float64`.\n",
      "        order : {'C', 'F'}, optional, default: 'C'\n",
      "            Whether to store multi-dimensional data in row-major\n",
      "            (C-style) or column-major (Fortran-style) order in\n",
      "            memory.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        out : ndarray\n",
      "            Array of uninitialized (arbitrary) data of the given shape, dtype, and\n",
      "            order.  Object arrays will be initialized to None.\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        empty_like : Return an empty array with shape and type of input.\n",
      "        ones : Return a new array setting values to one.\n",
      "        zeros : Return a new array setting values to zero.\n",
      "        full : Return a new array of given shape filled with value.\n",
      "        \n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        `empty`, unlike `zeros`, does not set the array values to zero,\n",
      "        and may therefore be marginally faster.  On the other hand, it requires\n",
      "        the user to manually set all the values in the array, and should be\n",
      "        used with caution.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> np.empty([2, 2])\n",
      "        array([[ -9.74499359e+001,   6.69583040e-309],\n",
      "               [  2.13182611e-314,   3.06959433e-309]])         #uninitialized\n",
      "        \n",
      "        >>> np.empty([2, 2], dtype=int)\n",
      "        array([[-1073741821, -1067949133],\n",
      "               [  496041986,    19249760]])                     #uninitialized\n",
      "    \n",
      "    fromstring(...)\n",
      "        fromstring(string, dtype=float, count=-1, sep='')\n",
      "        \n",
      "        A new 1-D array initialized from text data in a string.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        string : str\n",
      "            A string containing the data.\n",
      "        dtype : data-type, optional\n",
      "            The data type of the array; default: float.  For binary input data,\n",
      "            the data must be in exactly this format. Most builtin numeric types are \n",
      "            supported and extension types may be supported.\n",
      "        \n",
      "            .. versionadded:: 1.18.0\n",
      "                Complex dtypes.\n",
      "        \n",
      "        count : int, optional\n",
      "            Read this number of `dtype` elements from the data.  If this is\n",
      "            negative (the default), the count will be determined from the\n",
      "            length of the data.\n",
      "        sep : str, optional\n",
      "            The string separating numbers in the data; extra whitespace between\n",
      "            elements is also ignored.\n",
      "        \n",
      "            .. deprecated:: 1.14\n",
      "                Passing ``sep=''``, the default, is deprecated since it will\n",
      "                trigger the deprecated binary mode of this function. This mode\n",
      "                interprets `string` as binary bytes, rather than ASCII text with\n",
      "                decimal numbers, an operation which is better spelt\n",
      "                ``frombuffer(string, dtype, count)``. If `string` contains unicode\n",
      "                text, the binary mode of `fromstring` will first encode it into\n",
      "                bytes using either utf-8 (python 3) or the default encoding\n",
      "                (python 2), neither of which produce sane results.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        arr : ndarray\n",
      "            The constructed array.\n",
      "        \n",
      "        Raises\n",
      "        ------\n",
      "        ValueError\n",
      "            If the string is not the correct size to satisfy the requested\n",
      "            `dtype` and `count`.\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        frombuffer, fromfile, fromiter\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> np.fromstring('1 2', dtype=int, sep=' ')\n",
      "        array([1, 2])\n",
      "        >>> np.fromstring('1, 2', dtype=int, sep=',')\n",
      "        array([1, 2])\n",
      "    \n",
      "    score_cbow_pair(model, word, l1)\n",
      "        Score the trained CBOW model on a pair of words.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        model : :class:`~gensim.models.word2vec.Word2Vec`\n",
      "            The trained model.\n",
      "        word : :class:`~gensim.models.keyedvectors.Vocab`\n",
      "            Vocabulary representation of the first word.\n",
      "        l1 : list of float\n",
      "            Vector representation of the second word.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        float\n",
      "            Logarithm of the sum of exponentiations of input words.\n",
      "    \n",
      "    score_sentence_cbow(...)\n",
      "        score_sentence_cbow(model, sentence, _work, _neu1)\n",
      "        Obtain likelihood score for a single sentence in a fitted CBOW representation.\n",
      "        \n",
      "            Notes\n",
      "            -----\n",
      "            This scoring function is only implemented for hierarchical softmax (`model.hs == 1`).\n",
      "            The model should have been trained using the skip-gram model (`model.cbow` == 1`).\n",
      "        \n",
      "            Parameters\n",
      "            ----------\n",
      "            model : :class:`~gensim.models.word2vec.Word2Vec`\n",
      "                The trained model. It **MUST** have been trained using hierarchical softmax and the CBOW algorithm.\n",
      "            sentence : list of str\n",
      "                The words comprising the sentence to be scored.\n",
      "            _work : np.ndarray\n",
      "                Private working memory for each worker.\n",
      "            _neu1 : np.ndarray\n",
      "                Private working memory for each worker.\n",
      "        \n",
      "            Returns\n",
      "            -------\n",
      "            float\n",
      "                The probability assigned to this sentence by the Skip-Gram model.\n",
      "    \n",
      "    score_sentence_sg(...)\n",
      "        score_sentence_sg(model, sentence, _work)\n",
      "        Obtain likelihood score for a single sentence in a fitted skip-gram representation.\n",
      "        \n",
      "            Notes\n",
      "            -----\n",
      "            This scoring function is only implemented for hierarchical softmax (`model.hs == 1`).\n",
      "            The model should have been trained using the skip-gram model (`model.sg` == 1`).\n",
      "        \n",
      "            Parameters\n",
      "            ----------\n",
      "            model : :class:`~gensim.models.word2vec.Word2Vec`\n",
      "                The trained model. It **MUST** have been trained using hierarchical softmax and the skip-gram algorithm.\n",
      "            sentence : list of str\n",
      "                The words comprising the sentence to be scored.\n",
      "            _work : np.ndarray\n",
      "                Private working memory for each worker.\n",
      "        \n",
      "            Returns\n",
      "            -------\n",
      "            float\n",
      "                The probability assigned to this sentence by the Skip-Gram model.\n",
      "    \n",
      "    score_sg_pair(model, word, word2)\n",
      "        Score the trained Skip-gram model on a pair of words.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        model : :class:`~gensim.models.word2vec.Word2Vec`\n",
      "            The trained model.\n",
      "        word : :class:`~gensim.models.keyedvectors.Vocab`\n",
      "            Vocabulary representation of the first word.\n",
      "        word2 : :class:`~gensim.models.keyedvectors.Vocab`\n",
      "            Vocabulary representation of the second word.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        float\n",
      "            Logarithm of the sum of exponentiations of input words.\n",
      "    \n",
      "    train_batch_cbow(...)\n",
      "        train_batch_cbow(model, sentences, alpha, _work, _neu1, compute_loss)\n",
      "        Update CBOW model by training on a batch of sentences.\n",
      "        \n",
      "            Called internally from :meth:`~gensim.models.word2vec.Word2Vec.train`.\n",
      "        \n",
      "            Parameters\n",
      "            ----------\n",
      "            model : :class:`~gensim.models.word2vec.Word2Vec`\n",
      "                The Word2Vec model instance to train.\n",
      "            sentences : iterable of list of str\n",
      "                The corpus used to train the model.\n",
      "            alpha : float\n",
      "                The learning rate.\n",
      "            _work : np.ndarray\n",
      "                Private working memory for each worker.\n",
      "            _neu1 : np.ndarray\n",
      "                Private working memory for each worker.\n",
      "            compute_loss : bool\n",
      "                Whether or not the training loss should be computed in this batch.\n",
      "        \n",
      "            Returns\n",
      "            -------\n",
      "            int\n",
      "                Number of words in the vocabulary actually used for training (They already existed in the vocabulary\n",
      "                and were not discarded by negative sampling).\n",
      "    \n",
      "    train_batch_sg(...)\n",
      "        train_batch_sg(model, sentences, alpha, _work, compute_loss)\n",
      "        Update skip-gram model by training on a batch of sentences.\n",
      "        \n",
      "            Called internally from :meth:`~gensim.models.word2vec.Word2Vec.train`.\n",
      "        \n",
      "            Parameters\n",
      "            ----------\n",
      "            model : :class:`~gensim.models.word2Vec.Word2Vec`\n",
      "                The Word2Vec model instance to train.\n",
      "            sentences : iterable of list of str\n",
      "                The corpus used to train the model.\n",
      "            alpha : float\n",
      "                The learning rate\n",
      "            _work : np.ndarray\n",
      "                Private working memory for each worker.\n",
      "            compute_loss : bool\n",
      "                Whether or not the training loss should be computed in this batch.\n",
      "        \n",
      "            Returns\n",
      "            -------\n",
      "            int\n",
      "                Number of words in the vocabulary actually used for training (They already existed in the vocabulary\n",
      "                and were not discarded by negative sampling).\n",
      "    \n",
      "    train_cbow_pair(model, word, input_word_indices, l1, alpha, learn_vectors=True, learn_hidden=True, compute_loss=False, context_vectors=None, context_locks=None, is_ft=False)\n",
      "        Train the passed model instance on a word and its context, using the CBOW algorithm.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        model : :class:`~gensim.models.word2vec.Word2Vec`\n",
      "            The model to be trained.\n",
      "        word : str\n",
      "            The label (predicted) word.\n",
      "        input_word_indices : list of int\n",
      "            The vocabulary indices of the words in the context.\n",
      "        l1 : list of float\n",
      "            Vector representation of the label word.\n",
      "        alpha : float\n",
      "            Learning rate.\n",
      "        learn_vectors : bool, optional\n",
      "            Whether the vectors should be updated.\n",
      "        learn_hidden : bool, optional\n",
      "            Whether the weights of the hidden layer should be updated.\n",
      "        compute_loss : bool, optional\n",
      "            Whether or not the training loss should be computed.\n",
      "        context_vectors : list of list of float, optional\n",
      "            Vector representations of the words in the context. If None, these will be retrieved from the model.\n",
      "        context_locks : list of float, optional\n",
      "            The lock factors for each word in the context.\n",
      "        is_ft : bool, optional\n",
      "            If True, weights will be computed using `model.wv.syn0_vocab` and `model.wv.syn0_ngrams`\n",
      "            instead of `model.wv.syn0`.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        numpy.ndarray\n",
      "            Error vector to be back-propagated.\n",
      "    \n",
      "    train_epoch_cbow(...)\n",
      "        train_epoch_cbow(model, corpus_file, offset, _cython_vocab, _cur_epoch, _expected_examples, _expected_words, _work, _neu1, compute_loss)\n",
      "        Train CBOW model for one epoch by training on an input stream. This function is used only in multistream mode.\n",
      "        \n",
      "            Called internally from :meth:`~gensim.models.word2vec.Word2Vec.train`.\n",
      "        \n",
      "            Parameters\n",
      "            ----------\n",
      "            model : :class:`~gensim.models.word2vec.Word2Vec`\n",
      "                The Word2Vec model instance to train.\n",
      "            corpus_file : str\n",
      "                Path to corpus file.\n",
      "            _cur_epoch : int\n",
      "                Current epoch number. Used for calculating and decaying learning rate.\n",
      "            _work : np.ndarray\n",
      "                Private working memory for each worker.\n",
      "            _neu1 : np.ndarray\n",
      "                Private working memory for each worker.\n",
      "            compute_loss : bool\n",
      "                Whether or not the training loss should be computed in this batch.\n",
      "        \n",
      "            Returns\n",
      "            -------\n",
      "            int\n",
      "                Number of words in the vocabulary actually used for training (They already existed in the vocabulary\n",
      "                and were not discarded by negative sampling).\n",
      "    \n",
      "    train_epoch_sg(...)\n",
      "        train_epoch_sg(model, corpus_file, offset, _cython_vocab, _cur_epoch, _expected_examples, _expected_words, _work, _neu1, compute_loss)\n",
      "        Train Skipgram model for one epoch by training on an input stream. This function is used only in multistream mode.\n",
      "        \n",
      "            Called internally from :meth:`~gensim.models.word2vec.Word2Vec.train`.\n",
      "        \n",
      "            Parameters\n",
      "            ----------\n",
      "            model : :class:`~gensim.models.word2vec.Word2Vec`\n",
      "                The Word2Vec model instance to train.\n",
      "            corpus_file : str\n",
      "                Path to corpus file.\n",
      "            _cur_epoch : int\n",
      "                Current epoch number. Used for calculating and decaying learning rate.\n",
      "            _work : np.ndarray\n",
      "                Private working memory for each worker.\n",
      "            _neu1 : np.ndarray\n",
      "                Private working memory for each worker.\n",
      "            compute_loss : bool\n",
      "                Whether or not the training loss should be computed in this batch.\n",
      "        \n",
      "            Returns\n",
      "            -------\n",
      "            int\n",
      "                Number of words in the vocabulary actually used for training (They already existed in the vocabulary\n",
      "                and were not discarded by negative sampling).\n",
      "    \n",
      "    train_sg_pair(model, word, context_index, alpha, learn_vectors=True, learn_hidden=True, context_vectors=None, context_locks=None, compute_loss=False, is_ft=False)\n",
      "        Train the passed model instance on a word and its context, using the Skip-gram algorithm.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        model : :class:`~gensim.models.word2vec.Word2Vec`\n",
      "            The model to be trained.\n",
      "        word : str\n",
      "            The label (predicted) word.\n",
      "        context_index : list of int\n",
      "            The vocabulary indices of the words in the context.\n",
      "        alpha : float\n",
      "            Learning rate.\n",
      "        learn_vectors : bool, optional\n",
      "            Whether the vectors should be updated.\n",
      "        learn_hidden : bool, optional\n",
      "            Whether the weights of the hidden layer should be updated.\n",
      "        context_vectors : list of list of float, optional\n",
      "            Vector representations of the words in the context. If None, these will be retrieved from the model.\n",
      "        context_locks : list of float, optional\n",
      "            The lock factors for each word in the context.\n",
      "        compute_loss : bool, optional\n",
      "            Whether or not the training loss should be computed.\n",
      "        is_ft : bool, optional\n",
      "            If True, weights will be computed using `model.wv.syn0_vocab` and `model.wv.syn0_ngrams`\n",
      "            instead of `model.wv.syn0`.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        numpy.ndarray\n",
      "            Error vector to be back-propagated.\n",
      "    \n",
      "    zeros(...)\n",
      "        zeros(shape, dtype=float, order='C')\n",
      "        \n",
      "        Return a new array of given shape and type, filled with zeros.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        shape : int or tuple of ints\n",
      "            Shape of the new array, e.g., ``(2, 3)`` or ``2``.\n",
      "        dtype : data-type, optional\n",
      "            The desired data-type for the array, e.g., `numpy.int8`.  Default is\n",
      "            `numpy.float64`.\n",
      "        order : {'C', 'F'}, optional, default: 'C'\n",
      "            Whether to store multi-dimensional data in row-major\n",
      "            (C-style) or column-major (Fortran-style) order in\n",
      "            memory.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        out : ndarray\n",
      "            Array of zeros with the given shape, dtype, and order.\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        zeros_like : Return an array of zeros with shape and type of input.\n",
      "        empty : Return a new uninitialized array.\n",
      "        ones : Return a new array setting values to one.\n",
      "        full : Return a new array of given shape filled with value.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> np.zeros(5)\n",
      "        array([ 0.,  0.,  0.,  0.,  0.])\n",
      "        \n",
      "        >>> np.zeros((5,), dtype=int)\n",
      "        array([0, 0, 0, 0, 0])\n",
      "        \n",
      "        >>> np.zeros((2, 1))\n",
      "        array([[ 0.],\n",
      "               [ 0.]])\n",
      "        \n",
      "        >>> s = (2,2)\n",
      "        >>> np.zeros(s)\n",
      "        array([[ 0.,  0.],\n",
      "               [ 0.,  0.]])\n",
      "        \n",
      "        >>> np.zeros((2,), dtype=[('x', 'i4'), ('y', 'i4')]) # custom dtype\n",
      "        array([(0, 0), (0, 0)],\n",
      "              dtype=[('x', '<i4'), ('y', '<i4')])\n",
      "\n",
      "DATA\n",
      "    CORPUSFILE_VERSION = 1\n",
      "    FAST_VERSION = 1\n",
      "    MAX_WORDS_IN_BATCH = 10000\n",
      "    division = _Feature((2, 2, 0, 'alpha', 2), (3, 0, 0, 'alpha', 0), 8192...\n",
      "    exp = <ufunc 'exp'>\n",
      "    expit = <ufunc 'expit'>\n",
      "    log = <ufunc 'log'>\n",
      "    logaddexp = <ufunc 'logaddexp'>\n",
      "    logger = <Logger gensim.models.word2vec (WARNING)>\n",
      "    sqrt = <ufunc 'sqrt'>\n",
      "    string_types = (<class 'str'>,)\n",
      "\n",
      "FILE\n",
      "    /opt/anaconda3/lib/python3.7/site-packages/gensim/models/word2vec.py\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(word2vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def cosdis(v1, v2):\n",
    "    # which characters are common to the two words?\n",
    "    common = v1[1].intersection(v2[1])\n",
    "    # by definition of cosine distance we have\n",
    "    return sum(v1[0][ch]*v2[0][ch] for ch in common)/v1[2]/v2[2]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
